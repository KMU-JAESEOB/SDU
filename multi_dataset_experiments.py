# multi_dataset_experiments.py
"""
ğŸ¯ ë‹¤ì¤‘ ë°ì´í„°ì…‹ SDA-U ì‹¤í—˜ ì‹œìŠ¤í…œ
- SVHN â†” MNIST
- CIFAR-10 â†” STL-10  
- CIFAR-10 â†” CIFAR-100
- Fashion-MNIST â†” MNIST
ë“± ë‹¤ì–‘í•œ ë„ë©”ì¸ ì ì‘ ì‹¤í—˜ ì§€ì›
"""

import os
import sys
import json
import time
import subprocess
from pathlib import Path
from datetime import datetime
import torch
import torchvision

class MultiDatasetExperiments:
    """ë‹¤ì¤‘ ë°ì´í„°ì…‹ SDA-U ì‹¤í—˜ ê´€ë¦¬ì"""
    
    def __init__(self):
        self.results_dir = Path('multi_dataset_results')
        self.results_dir.mkdir(exist_ok=True)
        
        # ğŸ¯ ì§€ì› ë°ì´í„°ì…‹ ì¡°í•© ì •ì˜
        self.dataset_combinations = {
            # ğŸ”¢ ìˆ«ì ì¸ì‹ ë„ë©”ì¸
            'digit_recognition': [
                ('SVHN', 'MNIST', 'SVHNâ†’MNIST: ìì—°í™˜ê²½â†’ì†ê¸€ì”¨'),
                ('MNIST', 'SVHN', 'MNISTâ†’SVHN: ì†ê¸€ì”¨â†’ìì—°í™˜ê²½'),
                ('MNIST', 'FashionMNIST', 'MNISTâ†’Fashion: ìˆ«ìâ†’íŒ¨ì…˜'),
                ('FashionMNIST', 'MNIST', 'Fashionâ†’MNIST: íŒ¨ì…˜â†’ìˆ«ì'),
            ],
            
            # ğŸ–¼ï¸ ìì—° ì´ë¯¸ì§€ ë„ë©”ì¸
            'natural_images': [
                ('CIFAR10', 'STL10', 'CIFAR-10â†’STL-10: ì €í•´ìƒë„â†’ê³ í•´ìƒë„'),
                ('STL10', 'CIFAR10', 'STL-10â†’CIFAR-10: ê³ í•´ìƒë„â†’ì €í•´ìƒë„'),
                ('CIFAR10', 'CIFAR100', 'CIFAR-10â†’CIFAR-100: 10í´ë˜ìŠ¤â†’100í´ë˜ìŠ¤'),
            ],
            
            # ğŸ”„ êµì°¨ ë„ë©”ì¸
            'cross_domain': [
                ('SVHN', 'FashionMNIST', 'SVHNâ†’Fashion: ìˆ«ìâ†’íŒ¨ì…˜'),
                ('CIFAR10', 'MNIST', 'CIFAR-10â†’MNIST: ìì—°ì´ë¯¸ì§€â†’ìˆ«ì'),
            ]
        }
        
        print("ğŸ¯ ë‹¤ì¤‘ ë°ì´í„°ì…‹ SDA-U ì‹¤í—˜ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ!")
        print(f"ğŸ“ ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: {self.results_dir}")
    
    def create_dataset_config(self, source_dataset, target_dataset, experiment_name):
        """íŠ¹ì • ë°ì´í„°ì…‹ ì¡°í•©ì„ ìœ„í•œ config.py ìƒì„±"""
        
        # ğŸ¯ ë°ì´í„°ì…‹ë³„ ìµœì  ì„¤ì •
        dataset_settings = {
            'SVHN': {
                'image_size': 32,
                'channels': 3,
                'num_classes': 10,
                'batch_size': 128,
                'epochs': 20,
                'learning_rate': 1e-3,
                'architecture': 'resnet18'
            },
            'MNIST': {
                'image_size': 28,
                'channels': 1,
                'num_classes': 10,
                'batch_size': 256,
                'epochs': 15,
                'learning_rate': 1e-3,
                'architecture': 'custom_cnn'
            },
            'FashionMNIST': {
                'image_size': 28,
                'channels': 1,
                'num_classes': 10,
                'batch_size': 256,
                'epochs': 15,
                'learning_rate': 1e-3,
                'architecture': 'custom_cnn'
            },
            'CIFAR10': {
                'image_size': 32,
                'channels': 3,
                'num_classes': 10,
                'batch_size': 128,
                'epochs': 25,
                'learning_rate': 1e-3,
                'architecture': 'resnet18'
            },
            'STL10': {
                'image_size': 96,
                'channels': 3,
                'num_classes': 10,
                'batch_size': 64,
                'epochs': 30,
                'learning_rate': 5e-4,
                'architecture': 'resnet18'
            },
            'CIFAR100': {
                'image_size': 32,
                'channels': 3,
                'num_classes': 100,
                'batch_size': 128,
                'epochs': 30,
                'learning_rate': 1e-3,
                'architecture': 'resnet18'
            }
        }
        
        source_settings = dataset_settings.get(source_dataset, dataset_settings['CIFAR10'])
        target_settings = dataset_settings.get(target_dataset, dataset_settings['CIFAR10'])
        
        # ğŸ”§ í•˜ì´ë¸Œë¦¬ë“œ ì„¤ì • (ì†ŒìŠ¤ì™€ íƒ€ê²Ÿì˜ ì¤‘ê°„ê°’)
        hybrid_batch_size = min(source_settings['batch_size'], target_settings['batch_size'])
        hybrid_epochs = max(source_settings['epochs'], target_settings['epochs'])
        hybrid_lr = (source_settings['learning_rate'] + target_settings['learning_rate']) / 2
        
        # ğŸ—ï¸ ì•„í‚¤í…ì²˜ ì„ íƒ (ë” ë³µì¡í•œ ë°ì´í„°ì…‹ ê¸°ì¤€)
        if source_settings['channels'] == 3 or target_settings['channels'] == 3:
            architecture = 'resnet18'
        else:
            architecture = 'custom_cnn'
        
        config_content = f'''# config.py - {experiment_name} ì‹¤í—˜ìš© ì„¤ì •

# ============================================
# ğŸ¯ ë‹¤ì¤‘ ë°ì´í„°ì…‹ ì‹¤í—˜: {experiment_name}
# ============================================
ARCHITECTURE = '{architecture}'
BATCH_SIZE = {hybrid_batch_size}
NUM_EPOCHS = {hybrid_epochs}
LEARNING_RATE = {hybrid_lr}

# ë°ì´í„°ì…‹ ì„¤ì •
SOURCE_DATASET = '{source_dataset}'
TARGET_DATASET = '{target_dataset}'

# SDA-U ì•Œê³ ë¦¬ì¦˜ ì„¤ì • (ë°ì´í„°ì…‹ ìµœì í™”)
TARGET_SUBSET_SIZE = 500     # íƒ€ê²Ÿ ìƒ˜í”Œ ìˆ˜
NUM_UNLEARN_STEPS = 6        # ì–¸ëŸ¬ë‹ ìŠ¤í…
INFLUENCE_SAMPLES = 200      # ì˜í–¥ë„ ê³„ì‚° ìƒ˜í”Œ
ADAPTATION_EPOCHS = {target_settings['epochs'] // 2}  # ì ì‘ í›ˆë ¨ ì—í¬í¬
MAX_UNLEARN_SAMPLES = 100    # ìµœëŒ€ ì–¸ëŸ¬ë‹ ìƒ˜í”Œ

# ğŸ”„ ë‹¤ì¤‘ ë¼ìš´ë“œ SDA-U ì„¤ì •
SDA_U_ROUNDS = 1
PROGRESSIVE_UNLEARNING = True
DYNAMIC_THRESHOLD = True

# ğŸ¯ ì‚¬ì „ í›ˆë ¨ ê°€ì¤‘ì¹˜ ì„¤ì •
USE_PRETRAINED = {'True' if architecture == 'resnet18' else 'False'}
FREEZE_BACKBONE = False

# ğŸ”§ í›ˆë ¨ ì„¤ì •
SCHEDULER_TYPE = 'cosine'
WARMUP_EPOCHS = 2
WEIGHT_DECAY = 1e-4
GRADIENT_CLIP = 1.0

# í•˜ì´ë¸Œë¦¬ë“œ ìŠ¤ì½”ì–´ë§ íŒŒë¼ë¯¸í„°
LAMBDA_U = 0.6
BETA = 0.1

# ì €ì¥ ì„¤ì •
SAVE_MODELS = True
SAVE_RESULTS = True
QUICK_TEST = False

def get_config():
    """ì„¤ì •ì„ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜"""
    import torch
    
    return {{
        'architecture': ARCHITECTURE,
        'batch_size': BATCH_SIZE,
        'num_epochs': NUM_EPOCHS,
        'learning_rate': LEARNING_RATE,
        'target_subset_size': TARGET_SUBSET_SIZE,
        'num_unlearn_steps': NUM_UNLEARN_STEPS,
        'influence_samples': INFLUENCE_SAMPLES,
        'adaptation_epochs': ADAPTATION_EPOCHS,
        'max_unlearn_samples': MAX_UNLEARN_SAMPLES,
        'sda_u_rounds': SDA_U_ROUNDS,
        'progressive_unlearning': PROGRESSIVE_UNLEARNING,
        'dynamic_threshold': DYNAMIC_THRESHOLD,
        'use_pretrained': USE_PRETRAINED,
        'freeze_backbone': FREEZE_BACKBONE,
        'scheduler_type': SCHEDULER_TYPE,
        'warmup_epochs': WARMUP_EPOCHS,
        'weight_decay': WEIGHT_DECAY,
        'gradient_clip': GRADIENT_CLIP,
        'lambda_u': LAMBDA_U,
        'beta': BETA,
        'save_models': SAVE_MODELS,
        'save_results': SAVE_RESULTS,
        'quick_test': QUICK_TEST,
        'source_dataset': SOURCE_DATASET,
        'target_dataset': TARGET_DATASET,
        'gpu_name': torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'
    }}
'''
        
        with open('config.py', 'w', encoding='utf-8') as f:
            f.write(config_content)
    
    def run_single_experiment(self, source_dataset, target_dataset, description):
        """ë‹¨ì¼ ë°ì´í„°ì…‹ ì¡°í•© ì‹¤í—˜ ì‹¤í–‰"""
        
        experiment_name = f"{source_dataset}2{target_dataset}"
        
        print(f"\n{'='*80}")
        print(f"ğŸ§ª ì‹¤í—˜ ì‹œì‘: {experiment_name}")
        print(f"ğŸ“Š {description}")
        print(f"ğŸ“¤ ì†ŒìŠ¤: {source_dataset}")
        print(f"ğŸ“¥ íƒ€ê²Ÿ: {target_dataset}")
        print(f"{'='*80}")
        
        # ğŸ—‚ï¸ ì‹¤í—˜ë³„ ë””ë ‰í† ë¦¬ ìƒì„±
        experiment_dir = self.results_dir / experiment_name
        experiment_dir.mkdir(exist_ok=True)
        
        models_dir = Path('models') / experiment_name
        models_dir.mkdir(parents=True, exist_ok=True)
        
        print(f"ğŸ“ ì‹¤í—˜ ë””ë ‰í† ë¦¬: {experiment_dir}")
        print(f"ğŸ“ ëª¨ë¸ ë””ë ‰í† ë¦¬: {models_dir}")
        
        # config.py ë°±ì—…
        if os.path.exists('config.py.backup'):
            os.remove('config.py.backup')
        if os.path.exists('config.py'):
            os.rename('config.py', 'config.py.backup')
        
        try:
            # ì‹¤í—˜ìš© config.py ìƒì„±
            self.create_dataset_config(source_dataset, target_dataset, experiment_name)
            
            # ì‹¤í—˜ ì‹¤í–‰
            start_time = time.time()
            print("ğŸš€ SDA-U ì‹¤í—˜ ì‹¤í–‰ ì¤‘...")
            
            process = subprocess.Popen(['python', 'main.py'], 
                                     stdout=subprocess.PIPE, 
                                     stderr=subprocess.STDOUT,
                                     text=True, 
                                     bufsize=1)
            
            # ì‹¤ì‹œê°„ ì¶œë ¥ ë° ë¡œê·¸ ì €ì¥
            output_lines = []
            log_file = experiment_dir / f"{experiment_name}_execution_log.txt"
            
            with open(log_file, 'w', encoding='utf-8') as log:
                while True:
                    if process.stdout is None:
                        break
                    output = process.stdout.readline()
                    if output == '' and process.poll() is not None:
                        break
                    if output:
                        print(output.strip())
                        log.write(output)
                        log.flush()
                        output_lines.append(output.strip())
            
            return_code = process.poll()
            end_time = time.time()
            execution_time = end_time - start_time
            
            if return_code == 0:
                print(f"âœ… ì‹¤í—˜ ì™„ë£Œ! (ì‹¤í–‰ì‹œê°„: {execution_time:.1f}ì´ˆ)")
                
                # ğŸ—‚ï¸ ê²°ê³¼ íŒŒì¼ ê´€ë¦¬
                result_file = 'results/sda_u_comprehensive_results.json'
                if os.path.exists(result_file):
                    with open(result_file, 'r', encoding='utf-8') as f:
                        result_data = json.load(f)
                    
                    # ì‹¤í—˜ ë©”íƒ€ë°ì´í„° ì¶”ê°€
                    result_data['experiment_info']['experiment_name'] = experiment_name
                    result_data['experiment_info']['source_dataset'] = source_dataset
                    result_data['experiment_info']['target_dataset'] = target_dataset
                    result_data['experiment_info']['description'] = description
                    result_data['experiment_info']['execution_time_seconds'] = execution_time
                    
                    # ê²°ê³¼ ì €ì¥
                    main_result_file = experiment_dir / f"{experiment_name}_results.json"
                    with open(main_result_file, 'w', encoding='utf-8') as f:
                        json.dump(result_data, f, indent=2, ensure_ascii=False)
                    
                    print(f"ğŸ“‹ ê²°ê³¼ ì €ì¥: {main_result_file}")
                    return True
                else:
                    print("âš ï¸ ê²°ê³¼ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    return False
            else:
                print(f"âŒ ì‹¤í—˜ ì‹¤íŒ¨! (ì¢…ë£Œ ì½”ë“œ: {return_code})")
                return False
                
        except Exception as e:
            print(f"âŒ ì‹¤í—˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return False
        finally:
            # config.py ë³µì›
            if os.path.exists('config.py.backup'):
                if os.path.exists('config.py'):
                    os.remove('config.py')
                os.rename('config.py.backup', 'config.py')
    
    def run_category_experiments(self, category):
        """íŠ¹ì • ì¹´í…Œê³ ë¦¬ì˜ ëª¨ë“  ì‹¤í—˜ ì‹¤í–‰"""
        
        if category not in self.dataset_combinations:
            print(f"âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” ì¹´í…Œê³ ë¦¬: {category}")
            return
        
        experiments = self.dataset_combinations[category]
        total_experiments = len(experiments)
        successful_experiments = 0
        
        print(f"\nğŸ¯ {category.upper()} ì¹´í…Œê³ ë¦¬ ì‹¤í—˜ ì‹œì‘!")
        print(f"ğŸ“Š ì´ {total_experiments}ê°œ ì‹¤í—˜ ì˜ˆì •")
        
        start_time = time.time()
        
        for i, (source, target, description) in enumerate(experiments, 1):
            print(f"\nğŸ”¢ ì§„í–‰ìƒí™©: {i}/{total_experiments}")
            
            if self.run_single_experiment(source, target, description):
                successful_experiments += 1
                print(f"âœ… {i}ë²ˆì§¸ ì‹¤í—˜ ì„±ê³µ!")
            else:
                print(f"âŒ {i}ë²ˆì§¸ ì‹¤í—˜ ì‹¤íŒ¨!")
        
        end_time = time.time()
        total_time = end_time - start_time
        
        print(f"\n{'='*80}")
        print(f"ğŸ‰ {category.upper()} ì¹´í…Œê³ ë¦¬ ì‹¤í—˜ ì™„ë£Œ!")
        print(f"âœ… ì„±ê³µí•œ ì‹¤í—˜: {successful_experiments}/{total_experiments}")
        print(f"â±ï¸ ì´ ì†Œìš”ì‹œê°„: {total_time:.1f}ì´ˆ ({total_time/60:.1f}ë¶„)")
        print(f"{'='*80}")
        
        # ì„±ëŠ¥ ìš”ì•½ í…Œì´ë¸” ìƒì„±
        self.create_performance_table(category)
    
    def create_performance_table(self, category):
        """ì¹´í…Œê³ ë¦¬ë³„ ì„±ëŠ¥ ìš”ì•½ í…Œì´ë¸” ìƒì„±"""
        
        print(f"\nğŸ“Š {category.upper()} ì„±ëŠ¥ ìš”ì•½ í…Œì´ë¸” ìƒì„± ì¤‘...")
        
        performance_data = []
        
        for source, target, description in self.dataset_combinations[category]:
            experiment_name = f"{source}2{target}"
            result_file = self.results_dir / experiment_name / f"{experiment_name}_results.json"
            
            if result_file.exists():
                try:
                    with open(result_file, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                    
                    performance_data.append({
                        'experiment': experiment_name,
                        'source': source,
                        'target': target,
                        'description': description,
                        'final_target_accuracy': data.get('final_results', {}).get('target_accuracy', 0),
                        'best_target_accuracy': data.get('training_progress', {}).get('best_target_accuracy', 0),
                        'execution_time': data.get('experiment_info', {}).get('execution_time_seconds', 0)
                    })
                except Exception as e:
                    print(f"âš ï¸ {experiment_name} ê²°ê³¼ ë¡œë”© ì‹¤íŒ¨: {e}")
        
        # CSV í˜•íƒœë¡œ ì €ì¥
        csv_file = self.results_dir / f"{category}_performance_summary.csv"
        with open(csv_file, 'w', encoding='utf-8') as f:
            f.write("ì‹¤í—˜ëª…,ì†ŒìŠ¤,íƒ€ê²Ÿ,ì„¤ëª…,ìµœì¢…ì •í™•ë„,ìµœê³ ì •í™•ë„,ì‹¤í–‰ì‹œê°„(ì´ˆ)\n")
            for data in performance_data:
                f.write(f"{data['experiment']},{data['source']},{data['target']},"
                       f"{data['description']},{data['final_target_accuracy']:.2f}%,"
                       f"{data['best_target_accuracy']:.2f}%,{data['execution_time']:.1f}\n")
        
        print(f"ğŸ“Š ì„±ëŠ¥ ìš”ì•½ í…Œì´ë¸” ì €ì¥: {csv_file}")
        
        # ì½˜ì†” ì¶œë ¥
        print(f"\nğŸ“ˆ {category.upper()} ì„±ëŠ¥ ìš”ì•½:")
        print("-" * 100)
        print(f"{'ì‹¤í—˜ëª…':<20} {'ì†ŒìŠ¤':<12} {'íƒ€ê²Ÿ':<12} {'ìµœì¢…ì •í™•ë„':<12} {'ìµœê³ ì •í™•ë„':<12} {'ì‹¤í–‰ì‹œê°„':<10}")
        print("-" * 100)
        for data in performance_data:
            print(f"{data['experiment']:<20} {data['source']:<12} {data['target']:<12} "
                  f"{data['final_target_accuracy']:>10.2f}% {data['best_target_accuracy']:>10.2f}% "
                  f"{data['execution_time']:>8.1f}s")
        print("-" * 100)

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    print("ğŸ¯ ë‹¤ì¤‘ ë°ì´í„°ì…‹ SDA-U ì‹¤í—˜ ì‹œìŠ¤í…œ")
    print("=" * 60)
    
    experiments = MultiDatasetExperiments()
    
    print("\nì‹¤í–‰í•  ì‹¤í—˜ ì¹´í…Œê³ ë¦¬ë¥¼ ì„ íƒí•˜ì„¸ìš”:")
    print("1. digit_recognition (ìˆ«ì ì¸ì‹ ë„ë©”ì¸)")
    print("2. natural_images (ìì—° ì´ë¯¸ì§€ ë„ë©”ì¸)")
    print("3. cross_domain (êµì°¨ ë„ë©”ì¸)")
    print("4. ê°œë³„ ì‹¤í—˜ ì„ íƒ")
    print("5. ì „ì²´ ì‹¤í—˜ ì‹¤í–‰")
    
    choice = input("\nì„ íƒ (1-5): ").strip()
    
    if choice == '1':
        experiments.run_category_experiments('digit_recognition')
    elif choice == '2':
        experiments.run_category_experiments('natural_images')
    elif choice == '3':
        experiments.run_category_experiments('cross_domain')
    elif choice == '4':
        # ê°œë³„ ì‹¤í—˜ ì„ íƒ
        print("\nğŸ“‹ ì‚¬ìš© ê°€ëŠ¥í•œ ì‹¤í—˜:")
        all_experiments = []
        for category, exps in experiments.dataset_combinations.items():
            all_experiments.extend(exps)
        
        for i, (source, target, desc) in enumerate(all_experiments, 1):
            print(f"{i}. {source}â†’{target}: {desc}")
        
        try:
            exp_choice = int(input(f"\nì‹¤í—˜ ì„ íƒ (1-{len(all_experiments)}): ")) - 1
            if 0 <= exp_choice < len(all_experiments):
                source, target, desc = all_experiments[exp_choice]
                experiments.run_single_experiment(source, target, desc)
            else:
                print("âŒ ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤.")
        except ValueError:
            print("âŒ ìˆ«ìë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    
    elif choice == '5':
        # ì „ì²´ ì‹¤í—˜ ì‹¤í–‰
        print("\nâš ï¸ ì „ì²´ ì‹¤í—˜ì€ ë§¤ìš° ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        confirm = input("ê³„ì†í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ").lower()
        if confirm == 'y':
            for category in experiments.dataset_combinations.keys():
                experiments.run_category_experiments(category)
        else:
            print("âŒ ì‹¤í—˜ì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    else:
        print("âŒ ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤.")

if __name__ == "__main__":
    main() 