# improved_target_sampling.py
"""
ğŸ¯ íƒ€ê²Ÿ ë„ë©”ì¸ ìƒ˜í”Œë§ ê°œì„  ë°©ì•ˆ
- ê³„ì¸µì  ìƒ˜í”Œë§ (Stratified Sampling)
- í´ë˜ìŠ¤ ê· í˜• ìƒ˜í”Œë§ (Class-balanced Sampling)  
- ë‹¤ì¤‘ ë°°ì¹˜ ìƒ˜í”Œë§ (Multi-batch Sampling)
- ì ì‘ì  ìƒ˜í”Œë§ (Adaptive Sampling)
"""

import torch
import numpy as np
from collections import defaultdict, Counter
import random
from torch.utils.data import DataLoader, Subset

class ImprovedTargetSampler:
    """í–¥ìƒëœ íƒ€ê²Ÿ ë„ë©”ì¸ ìƒ˜í”Œë§ í´ë˜ìŠ¤"""
    
    def __init__(self, target_dataset, num_classes=65):
        self.target_dataset = target_dataset
        self.num_classes = num_classes
        self.class_indices = self._build_class_indices()
        
    def _build_class_indices(self):
        """í´ë˜ìŠ¤ë³„ ìƒ˜í”Œ ì¸ë±ìŠ¤ êµ¬ì¶•"""
        class_indices = defaultdict(list)
        
        for idx in range(len(self.target_dataset)):
            _, label = self.target_dataset[idx]
            class_indices[label].append(idx)
        
        return class_indices
    
    def stratified_sampling(self, total_samples=320, min_per_class=2):
        """ê³„ì¸µì  ìƒ˜í”Œë§: ê° í´ë˜ìŠ¤ì—ì„œ ê· ë“±í•˜ê²Œ ìƒ˜í”Œë§"""
        
        print(f"ğŸ¯ ê³„ì¸µì  ìƒ˜í”Œë§ ì‹œì‘ (ì´ {total_samples}ê°œ, í´ë˜ìŠ¤ë‹¹ ìµœì†Œ {min_per_class}ê°œ)")
        
        # í´ë˜ìŠ¤ë³„ ìƒ˜í”Œ ìˆ˜ ê³„ì‚°
        available_classes = len(self.class_indices)
        base_per_class = max(min_per_class, total_samples // available_classes)
        remaining_samples = total_samples - (base_per_class * available_classes)
        
        selected_indices = []
        class_sample_counts = {}
        
        # ê° í´ë˜ìŠ¤ì—ì„œ ê¸°ë³¸ ìƒ˜í”Œ ìˆ˜ë§Œí¼ ì„ íƒ
        for class_label, indices in self.class_indices.items():
            if len(indices) >= base_per_class:
                sampled = random.sample(indices, base_per_class)
                selected_indices.extend(sampled)
                class_sample_counts[class_label] = base_per_class
            else:
                # í´ë˜ìŠ¤ì— ì¶©ë¶„í•œ ìƒ˜í”Œì´ ì—†ìœ¼ë©´ ëª¨ë“  ìƒ˜í”Œ ì‚¬ìš©
                selected_indices.extend(indices)
                class_sample_counts[class_label] = len(indices)
        
        # ë‚¨ì€ ìƒ˜í”Œì„ í° í´ë˜ìŠ¤ì—ì„œ ì¶”ê°€ ì„ íƒ
        if remaining_samples > 0:
            large_classes = [(label, indices) for label, indices in self.class_indices.items() 
                           if len(indices) > base_per_class]
            
            for i in range(remaining_samples):
                if large_classes:
                    class_label, indices = large_classes[i % len(large_classes)]
                    available_indices = [idx for idx in indices if idx not in selected_indices]
                    if available_indices:
                        selected_indices.append(random.choice(available_indices))
                        class_sample_counts[class_label] += 1
        
        print(f"âœ… ê³„ì¸µì  ìƒ˜í”Œë§ ì™„ë£Œ: {len(selected_indices)}ê°œ ì„ íƒ")
        print(f"ğŸ“Š í´ë˜ìŠ¤ë³„ ë¶„í¬: {dict(class_sample_counts)}")
        
        return selected_indices
    
    def uncertainty_based_sampling(self, model, total_samples=320, device='cuda'):
        """ë¶ˆí™•ì‹¤ì„± ê¸°ë°˜ ìƒ˜í”Œë§: ëª¨ë¸ì´ ê°€ì¥ í™•ì‹ í•˜ì§€ ëª»í•˜ëŠ” ìƒ˜í”Œ ì„ íƒ"""
        
        print(f"ğŸ¤” ë¶ˆí™•ì‹¤ì„± ê¸°ë°˜ ìƒ˜í”Œë§ ì‹œì‘ (ì´ {total_samples}ê°œ)")
        
        model.eval()
        uncertainties = []
        
        # ì „ì²´ ë°ì´í„°ì…‹ì— ëŒ€í•´ ë¶ˆí™•ì‹¤ì„± ê³„ì‚°
        temp_loader = DataLoader(self.target_dataset, batch_size=64, shuffle=False)
        
        with torch.no_grad():
            for batch_idx, (data, _) in enumerate(temp_loader):
                data = data.to(device)
                output = model(data)
                probs = torch.softmax(output, dim=1)
                
                # ì—”íŠ¸ë¡œí”¼ ê¸°ë°˜ ë¶ˆí™•ì‹¤ì„± ê³„ì‚°
                entropy = -torch.sum(probs * torch.log(probs + 1e-8), dim=1)
                uncertainties.extend(entropy.cpu().numpy())
        
        # ë¶ˆí™•ì‹¤ì„±ì´ ë†’ì€ ìˆœì„œë¡œ ì •ë ¬
        sorted_indices = sorted(range(len(uncertainties)), 
                              key=lambda i: uncertainties[i], reverse=True)
        
        selected_indices = sorted_indices[:total_samples]
        
        print(f"âœ… ë¶ˆí™•ì‹¤ì„± ê¸°ë°˜ ìƒ˜í”Œë§ ì™„ë£Œ: {len(selected_indices)}ê°œ ì„ íƒ")
        print(f"ğŸ“Š í‰ê·  ë¶ˆí™•ì‹¤ì„±: {np.mean([uncertainties[i] for i in selected_indices]):.4f}")
        
        return selected_indices
    
    def diverse_sampling(self, total_samples=320, diversity_weight=0.5):
        """ë‹¤ì–‘ì„± ê¸°ë°˜ ìƒ˜í”Œë§: í´ë˜ìŠ¤ ê· í˜• + ë¬´ì‘ìœ„ì„±"""
        
        print(f"ğŸŒˆ ë‹¤ì–‘ì„± ê¸°ë°˜ ìƒ˜í”Œë§ ì‹œì‘ (ì´ {total_samples}ê°œ)")
        
        # í´ë˜ìŠ¤ë³„ ìµœëŒ€ ìƒ˜í”Œ ìˆ˜ ì œí•œ
        max_per_class = max(1, total_samples // self.num_classes)
        selected_indices = []
        
        # ê° í´ë˜ìŠ¤ì—ì„œ ì œí•œëœ ìˆ˜ë§Œí¼ ì„ íƒ
        for class_label, indices in self.class_indices.items():
            sample_count = min(len(indices), max_per_class)
            if sample_count > 0:
                sampled = random.sample(indices, sample_count)
                selected_indices.extend(sampled)
        
        # ë¶€ì¡±í•œ ìƒ˜í”Œì„ ë¬´ì‘ìœ„ë¡œ ì¶”ê°€
        if len(selected_indices) < total_samples:
            all_indices = list(range(len(self.target_dataset)))
            remaining_indices = [idx for idx in all_indices if idx not in selected_indices]
            additional_needed = total_samples - len(selected_indices)
            
            if remaining_indices:
                additional = random.sample(remaining_indices, 
                                         min(additional_needed, len(remaining_indices)))
                selected_indices.extend(additional)
        
        print(f"âœ… ë‹¤ì–‘ì„± ê¸°ë°˜ ìƒ˜í”Œë§ ì™„ë£Œ: {len(selected_indices)}ê°œ ì„ íƒ")
        
        return selected_indices
    
    def multi_batch_sampling(self, num_batches=10, batch_size=32):
        """ë‹¤ì¤‘ ë°°ì¹˜ ìƒ˜í”Œë§: ì—¬ëŸ¬ ë°°ì¹˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ë” ë§ì€ ìƒ˜í”Œ ì»¤ë²„"""
        
        print(f"ğŸ“¦ ë‹¤ì¤‘ ë°°ì¹˜ ìƒ˜í”Œë§ ì‹œì‘ ({num_batches}ê°œ ë°°ì¹˜ Ã— {batch_size}ê°œ)")
        
        # ì…”í”Œëœ DataLoader ìƒì„±
        loader = DataLoader(self.target_dataset, batch_size=batch_size, 
                          shuffle=True, drop_last=False)
        
        selected_batches = []
        batch_count = 0
        
        for batch in loader:
            if batch_count >= num_batches:
                break
            selected_batches.append(batch)
            batch_count += 1
        
        total_samples = sum(len(batch[0]) for batch in selected_batches)
        print(f"âœ… ë‹¤ì¤‘ ë°°ì¹˜ ìƒ˜í”Œë§ ì™„ë£Œ: {total_samples}ê°œ ì„ íƒ")
        
        return selected_batches
    
    def adaptive_sampling(self, model, initial_samples=64, max_samples=512, 
                         threshold=0.1, device='cuda'):
        """ì ì‘ì  ìƒ˜í”Œë§: ì„±ëŠ¥ì— ë”°ë¼ ìƒ˜í”Œ ìˆ˜ë¥¼ ë™ì  ì¡°ì •"""
        
        print(f"ğŸ”„ ì ì‘ì  ìƒ˜í”Œë§ ì‹œì‘ (ì´ˆê¸°: {initial_samples}ê°œ, ìµœëŒ€: {max_samples}ê°œ)")
        
        current_samples = initial_samples
        selected_indices = []
        
        while current_samples <= max_samples:
            # í˜„ì¬ ìƒ˜í”Œ ìˆ˜ë¡œ ê³„ì¸µì  ìƒ˜í”Œë§ ìˆ˜í–‰
            indices = self.stratified_sampling(current_samples)
            
            # ì„±ëŠ¥ í‰ê°€ (ê°„ë‹¨í•œ ë¶ˆí™•ì‹¤ì„± ì¸¡ì •)
            subset = Subset(self.target_dataset, indices)
            temp_loader = DataLoader(subset, batch_size=32, shuffle=False)
            
            uncertainties = []
            model.eval()
            with torch.no_grad():
                for data, _ in temp_loader:
                    data = data.to(device)
                    output = model(data)
                    probs = torch.softmax(output, dim=1)
                    entropy = -torch.sum(probs * torch.log(probs + 1e-8), dim=1)
                    uncertainties.extend(entropy.cpu().numpy())
            
            avg_uncertainty = np.mean(uncertainties)
            print(f"ğŸ“Š {current_samples}ê°œ ìƒ˜í”Œ í‰ê·  ë¶ˆí™•ì‹¤ì„±: {avg_uncertainty:.4f}")
            
            # ë¶ˆí™•ì‹¤ì„±ì´ ì¶©ë¶„íˆ ë†’ìœ¼ë©´ ì¤‘ë‹¨
            if avg_uncertainty > threshold:
                selected_indices = indices
                break
            
            # ìƒ˜í”Œ ìˆ˜ ì¦ê°€
            current_samples = min(current_samples * 2, max_samples)
        
        print(f"âœ… ì ì‘ì  ìƒ˜í”Œë§ ì™„ë£Œ: {len(selected_indices)}ê°œ ì„ íƒ")
        return selected_indices

def compare_sampling_methods(target_dataset, model, num_classes=65):
    """ë‹¤ì–‘í•œ ìƒ˜í”Œë§ ë°©ë²• ë¹„êµ"""
    
    print("ğŸ” íƒ€ê²Ÿ ìƒ˜í”Œë§ ë°©ë²• ë¹„êµ ë¶„ì„")
    print("="*60)
    
    sampler = ImprovedTargetSampler(target_dataset, num_classes)
    
    methods = {
        'stratified': lambda: sampler.stratified_sampling(320),
        'uncertainty': lambda: sampler.uncertainty_based_sampling(model, 320),
        'diverse': lambda: sampler.diverse_sampling(320),
        'adaptive': lambda: sampler.adaptive_sampling(model, 64, 320)
    }
    
    results = {}
    
    for method_name, method_func in methods.items():
        print(f"\nğŸ¯ {method_name.upper()} ìƒ˜í”Œë§ í…ŒìŠ¤íŠ¸")
        print("-" * 40)
        
        try:
            indices = method_func()
            
            # í´ë˜ìŠ¤ ë¶„í¬ ë¶„ì„
            labels = [target_dataset[i][1] for i in indices]
            class_counts = Counter(labels)
            
            results[method_name] = {
                'total_samples': len(indices),
                'unique_classes': len(class_counts),
                'class_distribution': dict(class_counts),
                'balance_score': len(class_counts) / num_classes  # í´ë˜ìŠ¤ ì»¤ë²„ë¦¬ì§€
            }
            
            print(f"âœ… ì´ ìƒ˜í”Œ: {len(indices)}ê°œ")
            print(f"ğŸ“Š ì»¤ë²„ëœ í´ë˜ìŠ¤: {len(class_counts)}/{num_classes}ê°œ")
            print(f"ğŸ¯ ê· í˜• ì ìˆ˜: {results[method_name]['balance_score']:.3f}")
            
        except Exception as e:
            print(f"âŒ {method_name} ì‹¤íŒ¨: {e}")
            results[method_name] = {'error': str(e)}
    
    # ê²°ê³¼ ìš”ì•½
    print(f"\nğŸ“‹ ìƒ˜í”Œë§ ë°©ë²• ë¹„êµ ìš”ì•½")
    print("="*60)
    
    for method, result in results.items():
        if 'error' not in result:
            print(f"{method:12} | {result['total_samples']:3d}ê°œ | "
                  f"{result['unique_classes']:2d}í´ë˜ìŠ¤ | "
                  f"ê· í˜•: {result['balance_score']:.3f}")
    
    return results

def main():
    """í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    print("ğŸ¯ íƒ€ê²Ÿ ë„ë©”ì¸ ìƒ˜í”Œë§ ê°œì„  ë°©ì•ˆ í…ŒìŠ¤íŠ¸")
    
    # ì‹¤ì œ ì‚¬ìš© ì˜ˆì‹œëŠ” officehome_loaderì™€ ì—°ë™ í•„ìš”
    print("ğŸ’¡ ì‹¤ì œ ì‚¬ìš©ì„ ìœ„í•´ì„œëŠ” ë‹¤ìŒê³¼ ê°™ì´ í†µí•©:")
    print("1. officehome_loader.pyì™€ ì—°ë™")
    print("2. main.pyì˜ íƒ€ê²Ÿ ìƒ˜í”Œë§ ë¶€ë¶„ êµì²´")
    print("3. ì‹¤í—˜ íŒŒë¼ë¯¸í„° ì¡°ì •")

if __name__ == "__main__":
    main() 