# gpu_config.py
# A100 80GB í™˜ê²½ ìµœì í™” ì„¤ì •

import torch

class A100Config:
    """A100 80GB GPU ìµœì í™” ì„¤ì •"""
    
    # ê¸°ë³¸ ì„¤ì •
    BATCH_SIZE = 512  # A100ì—ì„œ ë” í° ë°°ì¹˜ í¬ê¸° ì‚¬ìš© ê°€ëŠ¥
    NUM_EPOCHS = 25   # í° ëª¨ë¸ì—ì„œëŠ” ë” ì ì€ ì—í¬í¬ë¡œë„ ì¶©ë¶„
    LEARNING_RATE = 1e-4  # ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ì—ëŠ” ë‚®ì€ í•™ìŠµë¥ 
    
    # ë°ì´í„° ì„¤ì •
    TARGET_SUBSET_SIZE = 5000  # ë” ë§ì€ íƒ€ê²Ÿ ë°ì´í„° ì‚¬ìš©
    NUM_UNLEARN_STEPS = 20     # ë” ë§ì€ ì–¸ëŸ¬ë‹ ìŠ¤í…
    
    # ì¶”ì²œ ì•„í‚¤í…ì²˜ (A100 80GB)
    ARCHITECTURES = {
        'high_performance': 'vit_l_16',      # ìµœê³  ì„±ëŠ¥ (Vision Transformer Large)
        'balanced': 'resnet152',             # ê· í˜•ì¡íŒ ì„±ëŠ¥
        'fast_training': 'resnet101',        # ë¹ ë¥¸ í›ˆë ¨
        'memory_efficient': 'resnet50',      # ë©”ëª¨ë¦¬ íš¨ìœ¨ì 
        'lightweight': 'efficientnet_b3'    # ê²½ëŸ‰ ëª¨ë¸
    }
    
    # ì‚¬ì „ í›ˆë ¨ ê°€ì¤‘ì¹˜ ì‚¬ìš© ê¶Œì¥
    USE_PRETRAINED = True
    
    # A100 íŠ¹í™” ìµœì í™”
    ENABLE_TENSOR_CORE = True      # Tensor Core í™œì„±í™”
    ENABLE_MIXED_PRECISION = True  # Mixed Precision í›ˆë ¨
    ENABLE_BENCHMARK = True        # CUDNN ë²¤ì¹˜ë§ˆí¬ ëª¨ë“œ
    
    @staticmethod
    def apply_optimizations():
        """A100 ìµœì í™” ì„¤ì • ì ìš©"""
        if torch.cuda.is_available():
            # Tensor Core ìµœì í™”
            if A100Config.ENABLE_TENSOR_CORE:
                torch.backends.cuda.matmul.allow_tf32 = True
                torch.backends.cudnn.allow_tf32 = True
                print("ğŸš€ A100 Tensor Core (TF32) í™œì„±í™”!")
            
            # CUDNN ë²¤ì¹˜ë§ˆí¬
            if A100Config.ENABLE_BENCHMARK:
                torch.backends.cudnn.benchmark = True
                print("âš¡ CUDNN ë²¤ì¹˜ë§ˆí¬ ëª¨ë“œ í™œì„±í™”!")
            
            print("âœ… A100 ìµœì í™” ì„¤ì • ì ìš© ì™„ë£Œ!")
    
    @staticmethod
    def get_model_config(performance_level: str = 'balanced'):
        """ì„±ëŠ¥ ë ˆë²¨ì— ë”°ë¥¸ ëª¨ë¸ ì„¤ì • ë°˜í™˜"""
        
        configs = {
            'high_performance': {
                'architecture': 'vit_l_16',
                'batch_size': 256,  # ViTëŠ” ìƒëŒ€ì ìœ¼ë¡œ í° ë°°ì¹˜ í¬ê¸°
                'num_epochs': 15,   # í° ëª¨ë¸ì€ ë¹ ë¥´ê²Œ ìˆ˜ë ´
                'learning_rate': 5e-5
            },
            'balanced': {
                'architecture': 'resnet152',
                'batch_size': 384,
                'num_epochs': 20,
                'learning_rate': 1e-4
            },
            'fast_training': {
                'architecture': 'resnet101',
                'batch_size': 512,
                'num_epochs': 25,
                'learning_rate': 1e-4
            },
            'memory_efficient': {
                'architecture': 'resnet50',
                'batch_size': 768,
                'num_epochs': 30,
                'learning_rate': 2e-4
            },
            'lightweight': {
                'architecture': 'efficientnet_b3',
                'batch_size': 1024,
                'num_epochs': 35,
                'learning_rate': 3e-4
            }
        }
        
        if performance_level not in configs:
            print(f"âš ï¸ ì•Œ ìˆ˜ ì—†ëŠ” ì„±ëŠ¥ ë ˆë²¨: {performance_level}")
            print(f"ì‚¬ìš© ê°€ëŠ¥í•œ ì˜µì…˜: {list(configs.keys())}")
            performance_level = 'balanced'
        
        config = configs[performance_level]
        print(f"ğŸ¯ ì„ íƒëœ ì„±ëŠ¥ ë ˆë²¨: {performance_level}")
        print(f"ğŸ—ï¸ ì•„í‚¤í…ì²˜: {config['architecture']}")
        print(f"ğŸ“¦ ë°°ì¹˜ í¬ê¸°: {config['batch_size']}")
        print(f"ğŸ”„ ì—í¬í¬ ìˆ˜: {config['num_epochs']}")
        print(f"ğŸ“š í•™ìŠµë¥ : {config['learning_rate']}")
        
        return config

class GeneralConfig:
    """ì¼ë°˜ GPU í™˜ê²½ ì„¤ì •"""
    
    BATCH_SIZE = 128
    NUM_EPOCHS = 50
    LEARNING_RATE = 1e-3
    TARGET_SUBSET_SIZE = 2000
    NUM_UNLEARN_STEPS = 10
    
    ARCHITECTURES = {
        'recommended': 'resnet50',
        'lightweight': 'resnet18',
        'custom': 'custom_cnn'
    }

def get_optimal_config():
    """í˜„ì¬ GPU í™˜ê²½ì— ìµœì í™”ëœ ì„¤ì • ë°˜í™˜"""
    
    if not torch.cuda.is_available():
        print("ğŸ’» CPU í™˜ê²½ ê°ì§€: ê¸°ë³¸ ì„¤ì • ì‚¬ìš©")
        return {
            'architecture': 'custom_cnn',
            'batch_size': 32,
            'num_epochs': 10,
            'learning_rate': 1e-3,
            'target_subset_size': 1000,
            'num_unlearn_steps': 5
        }
    
    # GPU ì •ë³´ í™•ì¸
    gpu_name = torch.cuda.get_device_name(0)
    gpu_memory_gb = torch.cuda.get_device_properties(0).total_memory / (1024**3)
    
    print(f"ğŸ” GPU ê°ì§€: {gpu_name}")
    print(f"ğŸ’¾ GPU ë©”ëª¨ë¦¬: {gpu_memory_gb:.1f}GB")
    
    if "A100" in gpu_name and gpu_memory_gb >= 70:
        print("ğŸ”¥ A100 80GB í™˜ê²½ ê°ì§€!")
        A100Config.apply_optimizations()
        return A100Config.get_model_config('balanced')  # ê¸°ë³¸ê°’
    
    elif gpu_memory_gb >= 40:
        print("ğŸš€ ê³ ì„±ëŠ¥ GPU í™˜ê²½ (40GB+)")
        return {
            'architecture': 'resnet101',
            'batch_size': 256,
            'num_epochs': 25,
            'learning_rate': 1e-4,
            'target_subset_size': 3000,
            'num_unlearn_steps': 15
        }
    
    elif gpu_memory_gb >= 20:
        print("âš¡ ì¤‘ê³ ì„±ëŠ¥ GPU í™˜ê²½ (20GB+)")
        return {
            'architecture': 'resnet50',
            'batch_size': 128,
            'num_epochs': 30,
            'learning_rate': 1e-4,
            'target_subset_size': 2500,
            'num_unlearn_steps': 12
        }
    
    else:
        print("ğŸ’» ì¼ë°˜ GPU í™˜ê²½")
        return {
            'architecture': 'resnet18',
            'batch_size': 64,
            'num_epochs': 40,
            'learning_rate': 2e-4,
            'target_subset_size': 2000,
            'num_unlearn_steps': 10
        }

if __name__ == "__main__":
    # ì„¤ì • í…ŒìŠ¤íŠ¸
    config = get_optimal_config()
    print("\nğŸ“‹ ìµœì í™”ëœ ì„¤ì •:")
    for key, value in config.items():
        print(f"  {key}: {value}")
    
    # A100 íŠ¹í™” ì„¤ì •ë“¤ ì¶œë ¥
    if torch.cuda.is_available() and "A100" in torch.cuda.get_device_name(0):
        print("\nğŸ¯ A100 ì„±ëŠ¥ ë ˆë²¨ë³„ ì„¤ì •:")
        for level in ['high_performance', 'balanced', 'fast_training', 'memory_efficient', 'lightweight']:
            print(f"\nğŸ”¸ {level}:")
            A100Config.get_model_config(level) 