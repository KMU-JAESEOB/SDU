#!/usr/bin/env python3
# main.py - SDA-U í”„ë ˆì„ì›Œí¬ ë©”ì¸ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader, Subset
import numpy as np
import json
import os
from datetime import datetime
from tqdm import tqdm
import warnings
import argparse
warnings.filterwarnings('ignore')

print("ğŸš€ SDA-U í”„ë ˆì„ì›Œí¬ ë©”ì¸ ì‹¤í–‰!")
print("=" * 60)

# ì„¤ì • ë¡œë“œ
try:
    from config import get_config
    config = get_config()
    print(f"ğŸš€ ì„¤ì • ë¡œë“œ ì™„ë£Œ: {config['gpu_name']}")
except ImportError:
    # í´ë°± ì„¤ì •
    config = {
        'batch_size': 64,
        'num_epochs': 3,
        'architecture': 'resnet50',  # ê¸°ì¡´ ëª¨ë¸ë¡œ ë³€ê²½
        'target_subset_size': 1000,
        'num_unlearn_steps': 5,
        'influence_samples': 300,
        'lambda_u': 0.6,
        'beta': 0.1,
        'learning_rate': 1e-3,
        'save_models': True,
        'save_results': True,
        'gpu_name': 'Default'
    }
    print("âš™ï¸ ê¸°ë³¸ ì„¤ì • ì‚¬ìš© (ResNet50)")

# ë””ë°”ì´ìŠ¤ ì„¤ì •
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"ğŸ”§ ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")

# ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
os.makedirs('models', exist_ok=True)
os.makedirs('results', exist_ok=True)

# ì „ì—­ ë³€ìˆ˜
best_target_accuracy = 0.0
performance_history = []

# 1. ë°ì´í„° ë³€í™˜ - Office-31 í˜¸í™˜ì„± ê°œì„  (ì¤‘ë³µ ì œê±°)
def get_transforms(architecture):
    """ì•„í‚¤í…ì²˜ì— ë§ëŠ” ë°ì´í„° ë³€í™˜ì„ ë°˜í™˜í•©ë‹ˆë‹¤ (Office-31 í˜¸í™˜)"""
    if 'vit' in architecture.lower():
        # Vision TransformerëŠ” 224x224 í¬ê¸° ì‚¬ìš©
        return transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Lambda(lambda x: x.repeat(3, 1, 1) if x.size(0) == 1 else x),
            transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))  # ImageNet ì •ê·œí™”
        ])
    elif 'resnet' in architecture.lower():
        # ResNetì€ Office-31ì— ë§ê²Œ 224x224, 3ì±„ë„ ì‚¬ìš©
        return transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Lambda(lambda x: x.repeat(3, 1, 1) if x.size(0) == 1 else x),
            transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))  # ImageNet ì •ê·œí™”
        ])
    else:
        # ê¸°ë³¸ ë³€í™˜ (Office-31 í˜¸í™˜)
        return transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Lambda(lambda x: x.repeat(3, 1, 1) if x.size(0) == 1 else x),
            transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))
        ])

# 2. CNN ëª¨ë¸ ì •ì˜
class SimpleCNN(nn.Module):
    def __init__(self, num_classes=31):  # Office-31ì— ë§ê²Œ ê¸°ë³¸ê°’ ë³€ê²½
        super(SimpleCNN, self).__init__()
        self.features = nn.Sequential(
            nn.Conv2d(3, 32, 3, padding=1),  # Office-31ì€ RGB ì´ë¯¸ì§€
            nn.ReLU(),
            nn.MaxPool2d(2),
            nn.Conv2d(32, 64, 3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2),
            nn.Conv2d(64, 128, 3, padding=1),
            nn.ReLU(),
            nn.AdaptiveAvgPool2d((1, 1))
        )
        self.classifier = nn.Sequential(
            nn.Dropout(0.5),
            nn.Linear(128, num_classes)
        )
    
    def forward(self, x):
        x = self.features(x)
        x = x.view(x.size(0), -1)
        x = self.classifier(x)
        return x

# 3. ëª¨ë¸ ìƒì„± (Office-31 í˜¸í™˜ì„± ê°•í™”)
def create_model(architecture, num_classes=31):
    """Office-31 í˜¸í™˜ ëª¨ë¸ ìƒì„± (ì‚¬ì „ í›ˆë ¨ ê°€ì¤‘ì¹˜ + ì±„ë„ ìµœì í™”)"""
    print(f"ğŸ—ï¸ ëª¨ë¸ ìƒì„± ì¤‘: {architecture}")
    
    # ì‚¬ì „ í›ˆë ¨ ê°€ì¤‘ì¹˜ ì‚¬ìš© ì—¬ë¶€ í™•ì¸
    use_pretrained = config.get('use_pretrained', True)
    weights = 'IMAGENET1K_V1' if use_pretrained else None
    
    try:
        if architecture == 'custom_cnn':
            model = SimpleCNN(num_classes=num_classes)
            print("âœ… ì»¤ìŠ¤í…€ CNN ìƒì„± ì™„ë£Œ (3ì±„ë„ RGB ì…ë ¥)")
            return model
            
        elif 'resnet' in architecture.lower():
            # ResNet ê³„ì—´ ëª¨ë¸ ìƒì„± (ì‚¬ì „ í›ˆë ¨ ê°€ì¤‘ì¹˜ ì‚¬ìš©)
            if architecture == 'resnet18':
                model = torchvision.models.resnet18(weights=weights)
                print(f"ğŸ“¦ ResNet18 ë¡œë“œ (ì‚¬ì „í›ˆë ¨: {use_pretrained})")
            elif architecture == 'resnet50':
                model = torchvision.models.resnet50(weights=weights)
                print(f"ğŸ“¦ ResNet50 ë¡œë“œ (ì‚¬ì „í›ˆë ¨: {use_pretrained})")
            else:
                # ê¸°ë³¸ê°’ìœ¼ë¡œ ResNet50 ì‚¬ìš©
                model = torchvision.models.resnet50(weights=weights)
                print(f"ğŸ“¦ ì•Œ ìˆ˜ ì—†ëŠ” ResNet ë³€í˜• {architecture}, ResNet50 ì‚¬ìš© (ì‚¬ì „í›ˆë ¨: {use_pretrained})")
            
            # ğŸš¨ í•µì‹¬: ì²« ë²ˆì§¸ convolutionì€ ì‚¬ì „í›ˆë ¨ì‹œ ì´ë¯¸ 3ì±„ë„ì´ë¯€ë¡œ ê·¸ëŒ€ë¡œ ì‚¬ìš©
            if use_pretrained:
                print(f"âœ… ì‚¬ì „ í›ˆë ¨ëœ conv1 ì‚¬ìš©: {model.conv1}")
            else:
                # ì‚¬ì „ í›ˆë ¨ì„ ì‚¬ìš©í•˜ì§€ ì•Šì„ ë•Œë§Œ ìˆ˜ë™ìœ¼ë¡œ 3ì±„ë„ ì„¤ì •
                model.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)
                print(f"âœ… ìƒˆë¡œìš´ conv1: {model.conv1} (3ì±„ë„ â†’ 64ì±„ë„)")
            
            # âš¡ ë°±ë³¸ ë™ê²° ì„¤ì • (fine-tuning vs full training)
            freeze_backbone = config.get('freeze_backbone', False)
            if freeze_backbone and use_pretrained:
                for param in model.parameters():
                    param.requires_grad = False
                # ë¶„ë¥˜ ë ˆì´ì–´ë§Œ í•™ìŠµ ê°€ëŠ¥í•˜ê²Œ ì„¤ì •
                model.fc.requires_grad_(True)
                print("ğŸ”’ ë°±ë³¸ ë™ê²°, ë¶„ë¥˜ ë ˆì´ì–´ë§Œ í•™ìŠµ")
            else:
                print("ğŸ”“ ì „ì²´ ëª¨ë¸ fine-tuning")
            
            # ë§ˆì§€ë§‰ ë¶„ë¥˜ ë ˆì´ì–´ë¥¼ Office-31 í´ë˜ìŠ¤ ìˆ˜ì— ë§ê²Œ ì¡°ì •
            model.fc = nn.Linear(model.fc.in_features, num_classes)
            print(f"âœ… ë¶„ë¥˜ ë ˆì´ì–´ ì¡°ì •: â†’ {num_classes}ê°œ í´ë˜ìŠ¤")
            
            return model
            
        elif 'vit' in architecture.lower():
            # Vision Transformer ëª¨ë¸ ìƒì„±
            if architecture == 'vit_b_16':
                model = torchvision.models.vit_b_16(weights=weights)
                model.heads = nn.Linear(model.heads.head.in_features, num_classes)
            elif architecture == 'vit_l_16':
                model = torchvision.models.vit_l_16(weights=weights)
                model.heads = nn.Linear(model.heads.head.in_features, num_classes)
            else:
                model = torchvision.models.vit_b_16(weights=weights)
                model.heads = nn.Linear(model.heads.head.in_features, num_classes)
            
            print(f"âœ… ViT ëª¨ë¸ ìƒì„± ì™„ë£Œ (224x224 3ì±„ë„ ì…ë ¥, ì‚¬ì „í›ˆë ¨: {use_pretrained})")
            return model
            
        else:
            # ì•Œ ìˆ˜ ì—†ëŠ” ì•„í‚¤í…ì²˜ëŠ” ResNet50ìœ¼ë¡œ ëŒ€ì²´
            print(f"âš ï¸ ì•Œ ìˆ˜ ì—†ëŠ” ì•„í‚¤í…ì²˜: {architecture}, ResNet50ìœ¼ë¡œ ëŒ€ì²´")
            model = torchvision.models.resnet50(weights=weights)
            model.fc = nn.Linear(model.fc.in_features, num_classes)
            print(f"âœ… ResNet50 ëŒ€ì²´ ëª¨ë¸ ìƒì„± ì™„ë£Œ (3ì±„ë„ RGB ì…ë ¥, ì‚¬ì „í›ˆë ¨: {use_pretrained})")
            return model
            
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ìƒì„± ì‹¤íŒ¨: {e}")
        print("ğŸ”„ ê¸°ë³¸ CNNìœ¼ë¡œ ëŒ€ì²´")
        model = SimpleCNN(num_classes=num_classes)
        print("âœ… ê¸°ë³¸ CNN ìƒì„± ì™„ë£Œ (3ì±„ë„ RGB ì…ë ¥)")
        return model

# 4. ë°ì´í„° ë¡œë” (í†µí•©ëœ ë°ì´í„°ì…‹ ê´€ë¦¬ì ì‚¬ìš©)
def get_data_loaders(batch_size, architecture, source_dataset_name='SVHN', target_dataset_name='MNIST'):
    """í†µí•©ëœ ë°ì´í„°ì…‹ ê´€ë¦¬ìë¥¼ ì‚¬ìš©í•˜ëŠ” ë°ì´í„° ë¡œë” ìƒì„± (ì¡°ê±´ë¶€ ì±„ë„ í†µì¼)"""
    
    from dataset_manager import dataset_manager
    
    print(f"ğŸ¯ ë„ë©”ì¸ ì ì‘ ì„¤ì •:")
    print(f"   ğŸ“¤ ì†ŒìŠ¤: {source_dataset_name}")
    print(f"   ğŸ“¥ íƒ€ê²Ÿ: {target_dataset_name}")
    
    # ğŸ”§ ì±„ë„ í†µì¼ì´ í•„ìš”í•œ ì¡°í•© ì •ì˜
    channel_unification_needed = [
        ('SVHN', 'MNIST'),          # 3ì±„ë„ â†’ 1ì±„ë„
        ('MNIST', 'SVHN'),          # 1ì±„ë„ â†’ 3ì±„ë„  
        ('SVHN', 'FashionMNIST'),   # 3ì±„ë„ â†’ 1ì±„ë„
        ('FashionMNIST', 'SVHN'),   # 1ì±„ë„ â†’ 3ì±„ë„
        ('CIFAR10', 'MNIST'),       # 3ì±„ë„ â†’ 1ì±„ë„
        ('MNIST', 'CIFAR10'),       # 1ì±„ë„ â†’ 3ì±„ë„
        ('CIFAR10', 'FashionMNIST'), # 3ì±„ë„ â†’ 1ì±„ë„
        ('FashionMNIST', 'CIFAR10'), # 1ì±„ë„ â†’ 3ì±„ë„
    ]
    
    current_combination = (source_dataset_name, target_dataset_name)
    needs_unification = current_combination in channel_unification_needed
    
    try:
        if needs_unification:
            print(f"ğŸ”§ ì±„ë„ í†µì¼ ì ìš©: {source_dataset_name}({dataset_manager.get_dataset_info(source_dataset_name)['channels']}ch) â†” {target_dataset_name}({dataset_manager.get_dataset_info(target_dataset_name)['channels']}ch)")
            
            # ğŸ”„ í†µì¼ëœ ë„ë©”ì¸ ì ì‘ ë¡œë” ì‚¬ìš©
            source_train_loader, target_train_loader, source_test_loader, target_test_loader = \
                dataset_manager.load_dataset_for_domain_adaptation(
                    source_dataset_name, target_dataset_name, 
                    batch_size=batch_size, shuffle=True
                )
        else:
            print(f"ğŸ“¦ ì¼ë°˜ ë¡œë” ì‚¬ìš©: ì±„ë„ í†µì¼ ë¶ˆí•„ìš”")
            
            # ğŸ”„ ì¼ë°˜ ë°ì´í„°ì…‹ ë¡œë” ì‚¬ìš© (ê¸°ì¡´ ë°©ì‹)
            source_train_loader, source_test_loader = dataset_manager.load_dataset(
                source_dataset_name, batch_size=batch_size, shuffle=True)
            target_train_loader, target_test_loader = dataset_manager.load_dataset(
                target_dataset_name, batch_size=batch_size, shuffle=True)
        
        print(f"âœ… ë°ì´í„° ë¡œë” ìƒì„± ì™„ë£Œ!")
        print(f"   ğŸ“Š ì†ŒìŠ¤ í›ˆë ¨ ë°°ì¹˜ ìˆ˜: {len(source_train_loader)}")
        print(f"   ğŸ“Š íƒ€ê²Ÿ í›ˆë ¨ ë°°ì¹˜ ìˆ˜: {len(target_train_loader)}")
        
        return source_train_loader, target_train_loader, source_test_loader, target_test_loader
        
    except Exception as e:
        print(f"âŒ ë°ì´í„° ë¡œë” ìƒì„± ì‹¤íŒ¨: {str(e)}")
        raise

# 5. ëª¨ë¸ ì €ì¥ í•¨ìˆ˜ (ë„ë©”ì¸ë³„ êµ¬ë¶„ ì €ì¥)
def save_model(model, filename, additional_info=None, source_domain=None, target_domain=None):
    """ëª¨ë¸ê³¼ ì¶”ê°€ ì •ë³´ë¥¼ ë„ë©”ì¸ë³„ ë””ë ‰í† ë¦¬ì— ì €ì¥í•©ë‹ˆë‹¤."""
    
    # ë„ë©”ì¸ ì •ë³´ ì¶”ì¶œ (íŒŒë¼ë¯¸í„° ìš°ì„ , ì—†ìœ¼ë©´ configì—ì„œ)
    source_dataset = source_domain if source_domain else config.get('source_dataset', 'Unknown')
    target_dataset = target_domain if target_domain else config.get('target_dataset', 'Unknown')
    
    # ë„ë©”ì¸ ì´ë¦„ ì •ë¦¬ (Office31_Amazon â†’ Amazon)
    source_name = source_dataset.split('_')[-1] if '_' in source_dataset else source_dataset
    target_name = target_dataset.split('_')[-1] if '_' in target_dataset else target_dataset
    
    # ì‹¤í—˜ ì´ë¦„ ìƒì„±
    experiment_name = f"{source_name}2{target_name}"
    
    # ë„ë©”ì¸ë³„ ë””ë ‰í† ë¦¬ ìƒì„±
    domain_dir = f'models/{experiment_name}'
    os.makedirs(domain_dir, exist_ok=True)
    
    # íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ê°€ (ì¤‘ë³µ ë°©ì§€)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    base_name = filename.replace('.pt', '')
    timestamped_filename = f"{base_name}_{timestamp}.pt"
    
    # ì €ì¥í•  ì •ë³´ êµ¬ì„±
    save_dict = {
        'model_state_dict': model.state_dict(),
        'config': config,
        'experiment_name': experiment_name,
        'source_dataset': source_dataset,
        'target_dataset': target_dataset,
        'timestamp': datetime.now().isoformat(),
        'filename': filename
    }
    if additional_info:
        save_dict.update(additional_info)
    
    # ë‘ ê°€ì§€ ë°©ì‹ìœ¼ë¡œ ì €ì¥
    full_path = f'{domain_dir}/{filename}'
    timestamped_path = f'{domain_dir}/{timestamped_filename}'
    
    # 1. ìµœì‹  ë²„ì „ (ë®ì–´ì“°ê¸°)
    torch.save(save_dict, full_path)
    
    # 2. íƒ€ì„ìŠ¤íƒ¬í”„ ë²„ì „ (ë³´ì¡´ìš©)
    torch.save(save_dict, timestamped_path)
    
    print(f"ğŸ’¾ ëª¨ë¸ ì €ì¥: {full_path}")
    print(f"ğŸ’¾ ë°±ì—… ì €ì¥: {timestamped_path}")
    
    return full_path, timestamped_path

# 6. ì„±ëŠ¥ ì¶”ì  í•¨ìˆ˜
def track_performance(epoch, source_acc, target_acc, loss):
    """ì„±ëŠ¥ì„ ì¶”ì í•˜ê³  ìµœê³  ì„±ëŠ¥ ëª¨ë¸ì„ ì €ì¥í•©ë‹ˆë‹¤."""
    global best_target_accuracy, performance_history
    
    performance_history.append({
        'epoch': epoch,
        'source_accuracy': source_acc,
        'target_accuracy': target_acc,
        'loss': loss,
        'timestamp': datetime.now().isoformat()
    })
    
    if target_acc > best_target_accuracy:
        best_target_accuracy = target_acc
        return True  # ìƒˆë¡œìš´ ìµœê³  ì„±ëŠ¥
    return False

# 7. í–¥ìƒëœ í›ˆë ¨ í•¨ìˆ˜ (ê³ ê¸‰ ì„¤ì • ì ìš©)
def train_model_with_evaluation(model, source_loader, target_loader, num_epochs=3, source_domain=None, target_domain=None):
    print(f"ğŸ‹ï¸ ê³ ì„±ëŠ¥ ëª¨ë¸ í›ˆë ¨ ì‹œì‘ ({num_epochs} ì—í¬í¬)")
    
    model.train()
    criterion = nn.CrossEntropyLoss()
    
    # ğŸ”§ ê³ ê¸‰ ì˜µí‹°ë§ˆì´ì € ì„¤ì •
    learning_rate = config.get('learning_rate', 2e-4)
    weight_decay = config.get('weight_decay', 1e-4)
    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)
    print(f"âš™ï¸ ì˜µí‹°ë§ˆì´ì €: AdamW (lr={learning_rate}, wd={weight_decay})")
    
    # ğŸ”¥ í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •
    scheduler_type = config.get('scheduler_type', 'cosine')
    warmup_epochs = config.get('warmup_epochs', 2)
    
    if scheduler_type == 'cosine':
        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)
        print("ğŸ“ˆ ì½”ì‚¬ì¸ ì–´ë‹ë§ ìŠ¤ì¼€ì¤„ëŸ¬ ì‚¬ìš©")
    elif scheduler_type == 'step':
        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=num_epochs//3, gamma=0.1)
        print("ğŸ“‰ ìŠ¤í… ìŠ¤ì¼€ì¤„ëŸ¬ ì‚¬ìš©")
    else:
        scheduler = None
        print("ğŸ“Š ê³ ì • í•™ìŠµë¥  ì‚¬ìš©")
    
    # ğŸ¯ ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘ ì„¤ì •
    gradient_clip = config.get('gradient_clip', 1.0)
    print(f"âœ‚ï¸ ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘: {gradient_clip}")
    
    losses = []
    
    for epoch in range(num_epochs):
        epoch_loss = 0.0
        batch_count = 0
        
        # ì›Œë°ì—… í•™ìŠµë¥  ì¡°ì •
        if epoch < warmup_epochs and scheduler is not None:
            warmup_lr = learning_rate * (epoch + 1) / warmup_epochs
            for param_group in optimizer.param_groups:
                param_group['lr'] = warmup_lr
            print(f"ğŸ”¥ ì›Œë°ì—… í•™ìŠµë¥ : {warmup_lr:.6f}")
        
        pbar = tqdm(source_loader, desc=f"ì—í¬í¬ {epoch+1}/{num_epochs}")
        
        for batch_idx, (data, target) in enumerate(pbar):
            data, target = data.to(device), target.to(device)
            
            optimizer.zero_grad()
            output = model(data)
            loss = criterion(output, target)
            loss.backward()
            
            # ğŸ¯ ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘ ì ìš©
            if gradient_clip > 0:
                torch.nn.utils.clip_grad_norm_(model.parameters(), gradient_clip)
            
            optimizer.step()
            
            epoch_loss += loss.item()
            batch_count += 1
            
            # í˜„ì¬ í•™ìŠµë¥  í‘œì‹œ
            current_lr = optimizer.param_groups[0]['lr']
            pbar.set_postfix({
                'Loss': f'{loss.item():.4f}',
                'LR': f'{current_lr:.6f}'
            })
            
            # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ëª¨ë“œ (ì„¤ì •ì—ì„œ ì œì–´)
            if config.get('quick_test', False) and batch_count >= 30:
                break
        
        avg_loss = epoch_loss / batch_count
        losses.append(avg_loss)
        
        # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸ (ì›Œë°ì—… ì´í›„)
        if epoch >= warmup_epochs and scheduler is not None:
            scheduler.step()
            print(f"ğŸ“Š í•™ìŠµë¥  ì—…ë°ì´íŠ¸: {optimizer.param_groups[0]['lr']:.6f}")
        
        # ì¤‘ê°„ í‰ê°€ (ë™ì  ë°ì´í„°ì…‹ ì´ë¦„ í‘œì‹œ)
        print(f"\nğŸ“Š ì—í¬í¬ {epoch+1} ì¤‘ê°„ í‰ê°€:")
        source_dataset_name = source_domain if source_domain else 'Unknown'
        target_dataset_name = target_domain if target_domain else 'Unknown'
        source_acc = evaluate_model(model, source_loader, max_batches=15, domain_name=f"ì†ŒìŠ¤({source_dataset_name})")
        target_acc = evaluate_model(model, target_loader, max_batches=15, domain_name=f"íƒ€ê²Ÿ({target_dataset_name})")
        
        # ì„±ëŠ¥ ì¶”ì  ë° ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥
        is_best = track_performance(epoch+1, source_acc, target_acc, avg_loss)
        if is_best:
            save_model(model, 'best_model.pt', {
                'epoch': epoch+1,
                'source_accuracy': source_acc,
                'target_accuracy': target_acc,
                'loss': avg_loss,
                'learning_rate': optimizer.param_groups[0]['lr']
            }, source_domain=source_domain, target_domain=target_domain)
            print(f"ğŸ† ìƒˆë¡œìš´ ìµœê³  ì„±ëŠ¥! íƒ€ê²Ÿ ì •í™•ë„: {target_acc:.2f}%")
        
        print(f"ì—í¬í¬ {epoch+1} ì™„ë£Œ, í‰ê·  ì†ì‹¤: {avg_loss:.4f}")
    
    # ì†ŒìŠ¤ ëª¨ë¸ ì €ì¥
    save_model(model, 'source_model.pt', {
        'final_source_accuracy': source_acc,
        'final_target_accuracy': target_acc,
        'training_losses': losses,
        'final_learning_rate': optimizer.param_groups[0]['lr']
    }, source_domain=source_domain, target_domain=target_domain)
    
    print(f"âœ… í›ˆë ¨ ì™„ë£Œ! ìµœì¢… í•™ìŠµë¥ : {optimizer.param_groups[0]['lr']:.6f}")
    return losses

# 8. í–¥ìƒëœ ì˜í–¥ë„ ê³„ì‚° (ë…¼ë¬¸ ê³µì‹ ê¸°ë°˜) - ì•ˆì „ ì¥ì¹˜ ì¶”ê°€
def compute_influence_scores_enhanced(model, source_loader, target_batch, num_samples=300):
    print("ğŸ” í–¥ìƒëœ ì˜í–¥ë„ ì ìˆ˜ ê³„ì‚° ì¤‘... (Pseudo-label ì‚¬ìš©)")
    
    model.eval()
    criterion = nn.CrossEntropyLoss()
    
    target_data, _ = target_batch  # âŒ ì‹¤ì œ ë¼ë²¨ ë¬´ì‹œ (Unsupervised)
    target_data = target_data.to(device)
    
    # ğŸ¯ Pseudo-label ìƒì„± (ì‹¤ì œ ë¼ë²¨ ëŒ€ì‹  ì‚¬ìš©)
    with torch.no_grad():
        target_output = model(target_data)
        target_pseudo_labels = torch.argmax(target_output, dim=1)
        confidence_scores = torch.max(torch.softmax(target_output, dim=1), dim=1)[0]
    
    print(f"ğŸ¯ Pseudo-label ìƒì„± ì™„ë£Œ: í‰ê·  í™•ì‹ ë„ {confidence_scores.mean().item():.3f}")
    
    # ğŸš¨ í´ë˜ìŠ¤ ì¸ë±ìŠ¤ ì•ˆì „ ê²€ì¦ ì¶”ê°€
    num_classes = model.fc.out_features if hasattr(model, 'fc') else model.classifier[-1].out_features
    print(f"ğŸ“Š ëª¨ë¸ í´ë˜ìŠ¤ ìˆ˜: {num_classes}")
    print(f"ğŸ¯ Pseudo-label ë²”ìœ„: {target_pseudo_labels.min().item()} ~ {target_pseudo_labels.max().item()}")
    
    # ë¼ë²¨ ë²”ìœ„ ê²€ì¦ (Pseudo-labelì€ ì´ë¯¸ ëª¨ë¸ ì¶œë ¥ì´ë¯€ë¡œ ì•ˆì „í•´ì•¼ í•¨)
    if target_pseudo_labels.max().item() >= num_classes or target_pseudo_labels.min().item() < 0:
        print(f"âŒ Pseudo-label ë²”ìœ„ ì˜¤ë¥˜! ë²”ìœ„: [{target_pseudo_labels.min().item()}, {target_pseudo_labels.max().item()}]")
        target_pseudo_labels = torch.clamp(target_pseudo_labels, 0, num_classes - 1)
        print(f"âœ… ìˆ˜ì •ëœ Pseudo-label ë²”ìœ„: {target_pseudo_labels.min().item()} ~ {target_pseudo_labels.max().item()}")
    
    # íƒ€ê²Ÿ ë°°ì¹˜ ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° (Pseudo-label ì‚¬ìš©)
    model.zero_grad()
    target_output = model(target_data)
    target_loss = criterion(target_output, target_pseudo_labels)  # âœ… Pseudo-label ì‚¬ìš©
    target_loss.backward()
    
    target_grads = []
    for param in model.parameters():
        if param.grad is not None:
            target_grads.append(param.grad.data.flatten())
    target_grad = torch.cat(target_grads) if target_grads else torch.zeros(1).to(device)
    
    # í—¤ì‹œì•ˆ ê·¼ì‚¬ (ë‹¨ìˆœí™”ëœ ë²„ì „)
    hessian_inv_approx = 1.0  # ì‹¤ì œë¡œëŠ” ë” ë³µì¡í•œ ê³„ì‚°ì´ í•„ìš”
    
    influence_scores = []
    sample_count = 0
    invalid_samples = 0
    
    print(f"ğŸ¯ {num_samples}ê°œ ìƒ˜í”Œì˜ ì˜í–¥ë„ ê³„ì‚°...")
    
    for batch_idx, (data, labels) in enumerate(source_loader):
        if sample_count >= num_samples:
            break
            
        data, labels = data.to(device), labels.to(device)
        
        # ğŸš¨ ì†ŒìŠ¤ ë¼ë²¨ ë²”ìœ„ ê²€ì¦ ë° ìˆ˜ì •
        original_labels = labels.clone()
        labels = torch.clamp(labels, 0, num_classes - 1)
        
        # ìˆ˜ì •ëœ ë¼ë²¨ ìˆ˜ ì¹´ìš´íŠ¸
        modified_count = (original_labels != labels).sum().item()
        if modified_count > 0:
            invalid_samples += modified_count
        
        for i in range(min(len(data), num_samples - sample_count)):
            try:
                model.zero_grad()
                output = model(data[i:i+1])
                
                # ì¶”ê°€ ì•ˆì „ ê²€ì¦
                if labels[i] >= num_classes or labels[i] < 0:
                    print(f"âš ï¸ ê±´ë„ˆë›°ëŠ” ìƒ˜í”Œ: ë¼ë²¨ {labels[i].item()}, í´ë˜ìŠ¤ ìˆ˜ {num_classes}")
                    continue
                
                loss = criterion(output, labels[i:i+1])
                loss.backward()
                
                sample_grads = []
                for param in model.parameters():
                    if param.grad is not None:
                        sample_grads.append(param.grad.data.flatten())
                sample_grad = torch.cat(sample_grads) if sample_grads else torch.zeros(1).to(device)
                
                # ë…¼ë¬¸ ê³µì‹: I_up(z_i, D_T^batch) = -âˆ‡_Î¸ L(D_T^batch, Î¸)^T H_Î¸^(-1) âˆ‡_Î¸ L(z_i, Î¸)
                influence = -torch.dot(target_grad, sample_grad).item() * hessian_inv_approx
                influence_scores.append(influence)
                sample_count += 1
                
            except RuntimeError as e:
                if "assert" in str(e).lower() or "index" in str(e).lower():
                    print(f"âš ï¸ ìƒ˜í”Œ ê±´ë„ˆë›°ê¸° (ë¼ë²¨ ì˜¤ë¥˜): {e}")
                    invalid_samples += 1
                    continue
                else:
                    raise e
        
        if sample_count >= num_samples:
            break
    
    if invalid_samples > 0:
        print(f"âš ï¸ ìˆ˜ì •/ê±´ë„ˆë›´ ìƒ˜í”Œ: {invalid_samples}ê°œ")
    
    print(f"âœ… {len(influence_scores)}ê°œ ìƒ˜í”Œ ì˜í–¥ë„ ê³„ì‚° ì™„ë£Œ (Pseudo-label ê¸°ë°˜)")
    return influence_scores

# 9. í•˜ì´ë¸Œë¦¬ë“œ ìŠ¤ì½”ì–´ë§ (ë…¼ë¬¸ êµ¬í˜„)
def unsupervised_target_sample_selection(model, target_loader, num_samples=800):
    """íƒ€ê²Ÿ ë¼ë²¨ì„ ëª¨ë¥¸ë‹¤ê³  ê°€ì •í•˜ê³  ë¶ˆí™•ì‹¤ì„± ê¸°ë°˜ìœ¼ë¡œ íƒ€ê²Ÿ ìƒ˜í”Œ ì„ ë³„"""
    print(f"ğŸ¯ 1ë‹¨ê³„: ë¶ˆí™•ì‹¤ì„± ê¸°ë°˜ íƒ€ê²Ÿ ìƒ˜í”Œ ì„ ë³„ ({num_samples}ê°œ)")
    
    model.eval()
    uncertainty_scores = []
    all_samples = []
    
    with torch.no_grad():
        for batch_idx, batch_data in enumerate(target_loader):
            # ë°°ì¹˜ ë°ì´í„° ì²˜ë¦¬ (ë¼ë²¨ ìœ ë¬´ì— ê´€ê³„ì—†ì´)
            if len(batch_data) == 2:
                data, labels = batch_data
            else:
                data = batch_data
                labels = None
            
            data = data.to(device)
            output = model(data)
            probs = torch.softmax(output, dim=1)
            
            # ë¶ˆí™•ì‹¤ì„± ê³„ì‚° (ì—”íŠ¸ë¡œí”¼)
            for i in range(data.size(0)):
                sample_probs = probs[i]
                entropy = -torch.sum(sample_probs * torch.log(sample_probs + 1e-8)).item()
                
                uncertainty_scores.append(entropy)
                if labels is not None:
                    all_samples.append((data[i].cpu(), labels[i].cpu()))
                else:
                    all_samples.append((data[i].cpu(), torch.tensor(0)))  # ë”ë¯¸ ë¼ë²¨
    
    # ë¶ˆí™•ì‹¤ì„±ì´ ë†’ì€ ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ìƒìœ„ num_samplesê°œ ì„ íƒ
    sorted_indices = sorted(range(len(uncertainty_scores)), 
                           key=lambda i: uncertainty_scores[i], reverse=True)
    
    selected_samples = [all_samples[i] for i in sorted_indices[:num_samples]]
    selected_scores = [uncertainty_scores[i] for i in sorted_indices[:num_samples]]
    
    print(f"âœ… ë¶ˆí™•ì‹¤ì„± ê¸°ë°˜ íƒ€ê²Ÿ ìƒ˜í”Œ ì„ ë³„ ì™„ë£Œ: {len(selected_samples)}ê°œ")
    print(f"ğŸ“Š í‰ê·  ë¶ˆí™•ì‹¤ì„±: {np.mean(selected_scores):.4f}")
    
    return selected_samples, selected_scores

def compute_influence_and_select_harmful(model, source_loader, target_samples, num_harmful=50):
    """ì„ ë³„ëœ íƒ€ê²Ÿ ìƒ˜í”Œê³¼ ë¹„êµí•˜ì—¬ ìœ í•´í•œ ì†ŒìŠ¤ ìƒ˜í”Œ ì„ ë³„"""
    print(f"ğŸ¯ 2ë‹¨ê³„: ìœ í•´ ì†ŒìŠ¤ ìƒ˜í”Œ ì„ ë³„ ({num_harmful}ê°œ)")
    
    # íƒ€ê²Ÿ ìƒ˜í”Œë“¤ì— ëŒ€í•œ ì˜í–¥ë„ ê³„ì‚°
    target_batch_for_influence = []
    for i, (data, label) in enumerate(target_samples[:100]):  # ì²˜ìŒ 100ê°œë§Œ ì‚¬ìš©
        target_batch_for_influence.append((data, label))
    
    # ì˜í–¥ë„ ì ìˆ˜ ê³„ì‚°
    influence_scores = compute_influence_scores_enhanced(
        model, source_loader, target_batch_for_influence, num_samples=500)
    
    # ì˜í–¥ë„ê°€ ê°€ì¥ ë†’ì€ (ìœ í•´í•œ) ì†ŒìŠ¤ ìƒ˜í”Œë“¤ ì„ ë³„
    harmful_indices = sorted(range(len(influence_scores)), 
                           key=lambda i: abs(influence_scores[i]), reverse=True)[:num_harmful]
    
    # ìœ í•´í•œ ì†ŒìŠ¤ ìƒ˜í”Œë“¤ ìˆ˜ì§‘
    harmful_samples = []
    harmful_scores = []
    
    sample_count = 0
    for batch_idx, (data, labels) in enumerate(source_loader):
        for i in range(data.size(0)):
            if sample_count in harmful_indices:
                harmful_samples.append((data[i].cpu(), labels[i].cpu()))
                harmful_scores.append(influence_scores[sample_count])
            sample_count += 1
            
            if len(harmful_samples) >= num_harmful:
                break
        if len(harmful_samples) >= num_harmful:
            break
    
    print(f"âœ… ìœ í•´ ì†ŒìŠ¤ ìƒ˜í”Œ ì„ ë³„ ì™„ë£Œ: {len(harmful_samples)}ê°œ")
    print(f"ğŸ“Š í‰ê·  ì˜í–¥ë„: {np.mean([abs(s) for s in harmful_scores]):.4f}")
    
    return harmful_samples, harmful_scores

def compute_hybrid_scores(model, target_samples, influence_scores, lambda_u=0.6, beta=0.1):
    """ë…¼ë¬¸ì˜ í•˜ì´ë¸Œë¦¬ë“œ ìŠ¤ì½”ì–´ë§: S(x) = Î»_uÂ·I(x;Î¸') + (1-Î»_u)Â·div(x, D_T^sub) + Î²Â·H(x)"""
    print("ğŸ¯ í•˜ì´ë¸Œë¦¬ë“œ ìŠ¤ì½”ì–´ ê³„ì‚° ì¤‘...")
    
    model.eval()
    hybrid_scores = []
    
    with torch.no_grad():
        for i, sample in enumerate(target_samples):
            if i >= len(influence_scores):
                break
                
            # ìƒ˜í”Œ ë°ì´í„° ì²˜ë¦¬ - ìœ ì—°í•œ ì–¸íŒ¨í‚¹
            if isinstance(sample, (tuple, list)) and len(sample) >= 2:
                data, _ = sample[0], sample[1]
            else:
                data = sample
            
            # ë°ì´í„°ë¥¼ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™í•˜ê³  ë°°ì¹˜ ì°¨ì› ì¶”ê°€
            data = data.to(device)
            if len(data.shape) == 3:  # [C, H, W] -> [1, C, H, W]
                data = data.unsqueeze(0)
            
            output = model(data)
            probs = torch.softmax(output, dim=1)
            
            # 1. ì˜í–¥ë„ ìŠ¤ì½”ì–´ (ì •ê·œí™”)
            influence_score = influence_scores[i]
            
            # 2. ë‹¤ì–‘ì„± ìŠ¤ì½”ì–´ (ë‹¨ìˆœí™”ëœ ë²„ì „)
            diversity_score = torch.max(probs).item()  # ìµœëŒ€ í™•ë¥ ì˜ ì—­ìˆ˜ë¡œ ê·¼ì‚¬
            
            # 3. ë¶ˆí™•ì‹¤ì„± ìŠ¤ì½”ì–´ (ì—”íŠ¸ë¡œí”¼)
            entropy = -torch.sum(probs * torch.log(probs + 1e-8)).item()
            
            # í•˜ì´ë¸Œë¦¬ë“œ ìŠ¤ì½”ì–´ ê³„ì‚°
            hybrid_score = (lambda_u * influence_score + 
                          (1 - lambda_u) * (1 - diversity_score) + 
                          beta * entropy)
            
            hybrid_scores.append(hybrid_score)
    
    print(f"âœ… {len(hybrid_scores)}ê°œ í•˜ì´ë¸Œë¦¬ë“œ ìŠ¤ì½”ì–´ ê³„ì‚° ì™„ë£Œ")
    return hybrid_scores

# 10. í–¥ìƒëœ ì–¸ëŸ¬ë‹ (DOS ì•Œê³ ë¦¬ì¦˜) - BatchNorm ì•ˆì „ ë²„ì „
def perform_dos_unlearning(model, harmful_data, influence_scores, num_steps=5):
    print("ğŸ”„ DOS (Dynamic Orthogonal Scaling) ì–¸ëŸ¬ë‹ ìˆ˜í–‰ ì¤‘...")
    
    # BatchNorm ë¬¸ì œ í•´ê²°: í›ˆë ¨ ëª¨ë“œ ìœ ì§€í•˜ë˜ ë°°ì¹˜ í¬ê¸° ì¡°ì •
    model.train()
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=1e-4)
    
    # ë™ì  ê°€ì¤‘ì¹˜ ê³„ì‚° (ì˜í–¥ë„ì— ë¹„ë¡€)
    abs_scores = [abs(s) for s in influence_scores]
    max_abs = max(abs_scores) if abs_scores else 1.0
    weights = [s / max_abs for s in abs_scores]  # ì •ê·œí™”ëœ ê°€ì¤‘ì¹˜
    
    unlearn_losses = []
    
    # BatchNorm ì•ˆì •ì„±ì„ ìœ„í•œ ìµœì†Œ ë°°ì¹˜ í¬ê¸° ë³´ì¥
    min_batch_size = 2  # BatchNormì´ ì•ˆì •ì ìœ¼ë¡œ ì‘ë™í•˜ëŠ” ìµœì†Œ í¬ê¸°
    effective_batch_size = max(min_batch_size, min(8, len(harmful_data)))
    
    for step in range(num_steps):
        total_loss = 0.0
        processed_batches = 0
        
        # ë°ì´í„°ë¥¼ ë°°ì¹˜ë¡œ ê·¸ë£¹í•‘
        for batch_start in range(0, len(harmful_data), effective_batch_size):
            batch_end = min(batch_start + effective_batch_size, len(harmful_data))
            
            # ë°°ì¹˜ê°€ ë„ˆë¬´ ì‘ìœ¼ë©´ ê±´ë„ˆë›°ê¸° (BatchNorm ì•ˆì •ì„±)
            if batch_end - batch_start < min_batch_size:
                continue
            
            batch_data = []
            batch_labels = []
            batch_weights = []
            
            # ë°°ì¹˜ ë°ì´í„° ì¤€ë¹„
            for i in range(batch_start, batch_end):
                if i >= len(harmful_data) or i >= len(weights):
                    break
                    
                data, label = harmful_data[i]
                
                # ë°ì´í„° íƒ€ì… í™•ì¸ ë° ë³€í™˜
                data = data.to(device)
                if isinstance(label, int):
                    label = torch.tensor(label).to(device)
                else:
                    label = label.to(device)
                
                batch_data.append(data)
                batch_labels.append(label)
                batch_weights.append(abs(weights[i]))
            
            if len(batch_data) < min_batch_size:
                continue
                
            # ë°°ì¹˜ í…ì„œ ìƒì„±
            batch_data_tensor = torch.stack(batch_data)
            batch_labels_tensor = torch.stack(batch_labels)
            batch_weights_tensor = torch.tensor(batch_weights, device=device)
            
            optimizer.zero_grad()
            
            # ìˆœì „íŒŒ (BatchNormì´ ì•ˆì „í•˜ê²Œ ì‘ë™)
            output = model(batch_data_tensor)
            
            # ê°œë³„ ìƒ˜í”Œ ì†ì‹¤ ê³„ì‚° í›„ ê°€ì¤‘ì¹˜ ì ìš©
            batch_loss = 0.0
            for j in range(len(batch_data_tensor)):
                sample_loss = criterion(output[j:j+1], batch_labels_tensor[j:j+1])
                weighted_sample_loss = sample_loss * batch_weights_tensor[j]
                batch_loss += weighted_sample_loss
            
            # ë°°ì¹˜ í‰ê·  ì†ì‹¤
            avg_batch_loss = batch_loss / len(batch_data_tensor)
            
            # ì–¸ëŸ¬ë‹ì„ ìœ„í•œ ê·¸ë˜ë””ì–¸íŠ¸ ë°˜ì „ (ìŒì˜ ê·¸ë˜ë””ì–¸íŠ¸)
            (-avg_batch_loss).backward()
            optimizer.step()
            
            total_loss += avg_batch_loss.item()
            processed_batches += 1
        
        avg_loss = total_loss / max(1, processed_batches)
        unlearn_losses.append(avg_loss)
        
        if (step + 1) % 2 == 0:
            print(f"  DOS ìŠ¤í… {step + 1}/{num_steps}, ê°€ì¤‘ ì†ì‹¤: {avg_loss:.4f}")
    
    print("âœ… DOS ì–¸ëŸ¬ë‹ ì™„ë£Œ")
    return unlearn_losses

# 11. íƒ€ê²Ÿ ë„ë©”ì¸ ì¬í•™ìŠµ (Self-training)
def retrain_with_curated_target_samples(model, curated_samples, adaptation_epochs=5):
    """íë ˆì´ì…˜ëœ íƒ€ê²Ÿ ìƒ˜í”Œë¡œ Fine-tuning ìˆ˜í–‰ (ì‹¤ì œ ë¼ë²¨ ì‚¬ìš©)"""
    print(f"ğŸ“ 4ë‹¨ê³„: ì„ ë³„ëœ íƒ€ê²Ÿ ìƒ˜í”Œë¡œ Fine-tuning ({adaptation_epochs}ì—í¬í¬)")
    print("ğŸ“‹ ì¤‘ìš”: ì´ ë‹¨ê³„ì—ì„œëŠ” ì‹¤ì œ íƒ€ê²Ÿ ë¼ë²¨ì„ ì‚¬ìš©í•©ë‹ˆë‹¤!")
    
    model.train()
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=1e-4)
    
    for epoch in range(adaptation_epochs):
        total_loss = 0.0
        correct_predictions = 0
        total_samples = 0
        
        # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬
        batch_size = 16
        for i in range(0, len(curated_samples), batch_size):
            batch_end = min(i + batch_size, len(curated_samples))
            batch_data = []
            batch_labels = []
            
            for j in range(i, batch_end):
                data, label = curated_samples[j]  # âœ… ì‹¤ì œ ë¼ë²¨ ì‚¬ìš© (Fine-tuningìš©)
                batch_data.append(data)
                batch_labels.append(label)
            
            if len(batch_data) == 0:
                continue
                
            batch_tensor = torch.stack(batch_data).to(device)
            batch_labels_tensor = torch.stack(batch_labels).to(device)
            
            # ğŸš¨ í´ë˜ìŠ¤ ìˆ˜ ì•ˆì „ ê²€ì¦
            num_classes = model.fc.out_features if hasattr(model, 'fc') else model.classifier[-1].out_features
            batch_labels_tensor = torch.clamp(batch_labels_tensor, 0, num_classes - 1)
            
            total_samples += len(batch_data)
            
            # ì‹¤ì œ ë¼ë²¨ë¡œ Fine-tuning
            optimizer.zero_grad()
            output = model(batch_tensor)
            loss = criterion(output, batch_labels_tensor)
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
            
            # ì •í™•ë„ ê³„ì‚°
            _, predicted = torch.max(output.data, 1)
            correct_predictions += (predicted == batch_labels_tensor).sum().item()
        
        avg_loss = total_loss / max(1, total_samples // batch_size)
        accuracy = correct_predictions / max(1, total_samples) * 100
        
        if (epoch + 1) % 2 == 0:
            print(f"  ì—í¬í¬ {epoch+1}/{adaptation_epochs}: "
                  f"ì†ì‹¤={avg_loss:.4f}, "
                  f"ì •í™•ë„={accuracy:.1f}% ({correct_predictions}/{total_samples})")
    
    print(f"âœ… íƒ€ê²Ÿ ë„ë©”ì¸ Fine-tuning ì™„ë£Œ (ìµœì¢… ì •í™•ë„: {accuracy:.1f}%)")

# 12. í–¥ìƒëœ í‰ê°€ í•¨ìˆ˜
def evaluate_model(model, data_loader, max_batches=20, domain_name=""):
    model.eval()
    correct = 0
    total = 0
    
    with torch.no_grad():
        for batch_idx, (data, target) in enumerate(data_loader):
            if batch_idx >= max_batches:
                break
                
            data, target = data.to(device), target.to(device)
            output = model(data)
            _, predicted = torch.max(output.data, 1)
            total += target.size(0)
            correct += (predicted == target).sum().item()
    
    accuracy = 100 * correct / total
    print(f"ğŸ“Š {domain_name} ì •í™•ë„: {accuracy:.2f}% ({correct}/{total})")
    return accuracy

# 12. ê²°ê³¼ ì €ì¥ ë° ë¶„ì„
def save_comprehensive_results(results):
    """í¬ê´„ì ì¸ ê²°ê³¼ë¥¼ ì €ì¥í•©ë‹ˆë‹¤."""
    
    # JSON ê²°ê³¼ ì €ì¥
    with open('results/sda_u_comprehensive_results.json', 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    # ì„±ëŠ¥ íˆìŠ¤í† ë¦¬ ì €ì¥
    with open('results/performance_history.json', 'w', encoding='utf-8') as f:
        json.dump(performance_history, f, indent=2, ensure_ascii=False)
    
    print("ğŸ’¾ í¬ê´„ì ì¸ ê²°ê³¼ ì €ì¥ ì™„ë£Œ")

# 13. í†µí•©ëœ ë°ì´í„° ë¡œë”© í•¨ìˆ˜
def load_data(args):
    """í†µí•©ëœ ë°ì´í„° ë¡œë”© í•¨ìˆ˜ (Office-Home, Office-31, CIFAR ë“± ì§€ì›)"""
    
    print(f"ğŸ“¦ ë°ì´í„° ë¡œë”© ì¤‘...")
    print(f"ğŸ¯ ë°ì´í„°ì…‹: {args.dataset}")
    
    if args.dataset == 'Office31':
        from office31_loader import Office31Loader
        loader = Office31Loader()
        
        print(f"ğŸ“¤ ì†ŒìŠ¤ ë„ë©”ì¸: {args.source_domain}")
        print(f"ğŸ“¥ íƒ€ê²Ÿ ë„ë©”ì¸: {args.target_domain}")
        
        # ë„ë©”ì¸ë³„ ë°ì´í„° ë¡œë“œ
        source_train_dataset, source_test_dataset = loader.load_domain_data(args.source_domain)
        target_train_dataset, target_test_dataset = loader.load_domain_data(args.target_domain)
        
        # ë°ì´í„° ë¡œë” ìƒì„±
        source_train_loader = DataLoader(source_train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=2)
        source_test_loader = DataLoader(source_test_dataset, batch_size=args.batch_size, shuffle=False, num_workers=2)
        target_train_loader = DataLoader(target_train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=2)
        target_test_loader = DataLoader(target_test_dataset, batch_size=args.batch_size, shuffle=False, num_workers=2)
        
        num_classes = 31  # Office-31 í´ë˜ìŠ¤ ìˆ˜
        
    elif args.dataset == 'OfficeHome':
        from officehome_loader import OfficeHomeLoader
        loader = OfficeHomeLoader()
        
        print(f"ğŸ“¤ ì†ŒìŠ¤ ë„ë©”ì¸: {args.source_domain}")
        print(f"ğŸ“¥ íƒ€ê²Ÿ ë„ë©”ì¸: {args.target_domain}")
        
        # ë„ë©”ì¸ë³„ ë°ì´í„° ë¡œë“œ
        source_train_dataset, source_test_dataset = loader.load_domain_data(args.source_domain)
        target_train_dataset, target_test_dataset = loader.load_domain_data(args.target_domain)
        
        # ë°ì´í„° ë¡œë” ìƒì„±
        source_train_loader = DataLoader(source_train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=2)
        source_test_loader = DataLoader(source_test_dataset, batch_size=args.batch_size, shuffle=False, num_workers=2)
        target_train_loader = DataLoader(target_train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=2)
        target_test_loader = DataLoader(target_test_dataset, batch_size=args.batch_size, shuffle=False, num_workers=2)
        
        num_classes = 65  # Office-Home í´ë˜ìŠ¤ ìˆ˜
        
    else:
        # ê¸°íƒ€ ë°ì´í„°ì…‹ (SVHN, MNIST, CIFAR ë“±)
        from dataset_manager import DatasetManager
        manager = DatasetManager()
        
        # source_domainê³¼ target_domainì„ ë°ì´í„°ì…‹ ì´ë¦„ìœ¼ë¡œ ì‚¬ìš©
        source_dataset_name = args.source_domain
        target_dataset_name = args.target_domain
        
        print(f"ğŸ“¤ ì†ŒìŠ¤ ë°ì´í„°ì…‹: {source_dataset_name}")
        print(f"ğŸ“¥ íƒ€ê²Ÿ ë°ì´í„°ì…‹: {target_dataset_name}")
        
        # ë°ì´í„°ì…‹ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        source_info = manager.get_dataset_info(source_dataset_name)
        target_info = manager.get_dataset_info(target_dataset_name)
        
        print(f"ğŸ“Š ì†ŒìŠ¤ í´ë˜ìŠ¤: {source_info['num_classes']}ê°œ")
        print(f"ğŸ“Š íƒ€ê²Ÿ í´ë˜ìŠ¤: {target_info['num_classes']}ê°œ")
        
        # ğŸš¨ CIFAR íŠ¹ë³„ ì²˜ë¦¬: í´ë˜ìŠ¤ ìˆ˜ ë¶ˆì¼ì¹˜ í•´ê²°
        if (source_dataset_name == 'CIFAR10' and target_dataset_name == 'CIFAR100') or \
           (source_dataset_name == 'CIFAR100' and target_dataset_name == 'CIFAR10'):
            
            print("ğŸ”§ CIFAR10â†”CIFAR100 í´ë˜ìŠ¤ ìˆ˜ ë¶ˆì¼ì¹˜ ê°ì§€ - íŠ¹ë³„ ì²˜ë¦¬ ëª¨ë“œ")
            num_classes = 10  # CIFAR10 ê¸°ì¤€ìœ¼ë¡œ í†µì¼
            print(f"âœ… í´ë˜ìŠ¤ ìˆ˜ë¥¼ CIFAR10 ê¸°ì¤€(10ê°œ)ìœ¼ë¡œ í†µì¼")
            print(f"âš ï¸ CIFAR100 ë¼ë²¨ì€ 0-9 ë²”ìœ„ë¡œ ìë™ í´ë¦¬í•‘ë©ë‹ˆë‹¤")
            
        else:
            # ì¼ë°˜ì ì¸ ê²½ìš°: ë” ì‘ì€ í´ë˜ìŠ¤ ìˆ˜ ì‚¬ìš©
            num_classes = min(source_info['num_classes'], target_info['num_classes'])
            print(f"ğŸ”§ í†µì¼ëœ í´ë˜ìŠ¤ ìˆ˜: {num_classes}ê°œ")
        
        # ë°ì´í„°ì…‹ ë¡œë“œ
        source_train_loader, source_test_loader = manager.load_dataset(source_dataset_name, args.batch_size)
        target_train_loader, target_test_loader = manager.load_dataset(target_dataset_name, args.batch_size)
    
    print(f"âœ… ë°ì´í„° ë¡œë”© ì™„ë£Œ! (í´ë˜ìŠ¤ ìˆ˜: {num_classes})")
    return source_train_loader, source_test_loader, target_train_loader, target_test_loader, num_classes

# 14. ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    parser = argparse.ArgumentParser(description='SDA-U (Selective Domain Adaptation with Unlearning)')
    
    # ë°ì´í„°ì…‹ ì„¤ì •
    parser.add_argument('--dataset', type=str, default='Office31', 
                       choices=['Office31', 'OfficeHome', 'SVHN', 'MNIST', 'FashionMNIST', 'CIFAR10', 'CIFAR100', 'STL10'],
                       help='ë°ì´í„°ì…‹ ì„ íƒ')
    parser.add_argument('--source_domain', type=str, required=True,
                       help='ì†ŒìŠ¤ ë„ë©”ì¸ (Office31: amazon/webcam/dslr, OfficeHome: art/clipart/product/real_world)')
    parser.add_argument('--target_domain', type=str, required=True, 
                       help='íƒ€ê²Ÿ ë„ë©”ì¸ (Office31: amazon/webcam/dslr, OfficeHome: art/clipart/product/real_world)')
    
    # í›ˆë ¨ íŒŒë¼ë¯¸í„°
    parser.add_argument('--batch_size', type=int, default=32, help='ë°°ì¹˜ í¬ê¸°')
    parser.add_argument('--num_epochs', type=int, default=20, help='í›ˆë ¨ ì—í¬í¬')
    parser.add_argument('--learning_rate', type=float, default=2e-4, help='í•™ìŠµë¥ ')
    parser.add_argument('--architecture', type=str, default='resnet50', 
                       choices=['resnet18', 'resnet50', 'simple_cnn'], help='ëª¨ë¸ ì•„í‚¤í…ì²˜')
    
    # SDA-U íŒŒë¼ë¯¸í„°
    parser.add_argument('--influence_samples', type=int, default=500, help='ì˜í–¥ë„ ê³„ì‚° ìƒ˜í”Œ ìˆ˜')
    parser.add_argument('--target_samples', type=int, default=800, help='íƒ€ê²Ÿ ìƒ˜í”Œ ìˆ˜')
    parser.add_argument('--unlearn_steps', type=int, default=8, help='ì–¸ëŸ¬ë‹ ìŠ¤í… ìˆ˜')
    parser.add_argument('--lambda_u', type=float, default=0.6, help='í•˜ì´ë¸Œë¦¬ë“œ ìŠ¤ì½”ì–´ë§ Î»_u')
    parser.add_argument('--beta', type=float, default=0.1, help='í•˜ì´ë¸Œë¦¬ë“œ ìŠ¤ì½”ì–´ë§ Î²')
    parser.add_argument('--adaptation_epochs', type=int, default=10, help='ì ì‘ í›ˆë ¨ ì—í¬í¬')
    
    # ì €ì¥ ê²½ë¡œ íŒŒë¼ë¯¸í„°
    parser.add_argument('--model_save_dir', type=str, default='models', help='ëª¨ë¸ ì €ì¥ ë””ë ‰í† ë¦¬')
    parser.add_argument('--results_file', type=str, default='results/sda_u_comprehensive_results.json', help='ê²°ê³¼ íŒŒì¼ ê²½ë¡œ')
    
    # ğŸš€ ì‚¬ì „ í›ˆë ¨ ëª¨ë¸ ê´€ë ¨ ì˜µì…˜
    parser.add_argument('--pretrained_model', type=str, default=None, 
                       help='ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ ê²½ë¡œ (ìˆìœ¼ë©´ ì†ŒìŠ¤ ë„ë©”ì¸ í›ˆë ¨ ìŠ¤í‚µ)')
    parser.add_argument('--skip_source_training', action='store_true', 
                       help='ì†ŒìŠ¤ ë„ë©”ì¸ í›ˆë ¨ ìŠ¤í‚µ (ì‚¬ì „ í›ˆë ¨ëœ ê°€ì¤‘ì¹˜ ì‚¬ìš©)')
    parser.add_argument('--use_pretrained_weights', action='store_true', default=True,
                       help='ImageNet ì‚¬ì „ í›ˆë ¨ ê°€ì¤‘ì¹˜ ì‚¬ìš©')
    
    args = parser.parse_args()

    # GPU ì„¤ì • ë° ìµœì í™”
    try:
        from gpu_config import setup_gpu_optimizations, get_gpu_info
        print("ğŸš€ A100 ìµœì í™” í™œì„±í™”!")
        setup_gpu_optimizations()
        gpu_info = get_gpu_info()
        print(f"ğŸš€ ì„¤ì • ë¡œë“œ ì™„ë£Œ: {gpu_info['name']}")
    except ImportError:
        print("âš ï¸ GPU ìµœì í™” ì„¤ì • íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ì„¤ì • ì‚¬ìš©")
        
    global device
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ”§ ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")

    # ë°ì´í„° ë¡œë”©
    source_train_loader, source_test_loader, target_train_loader, target_test_loader, num_classes = load_data(args)
    
    # ëª¨ë¸ ìƒì„±
    model = create_model(args.architecture, num_classes).to(device)
    
    print(f"ğŸ¤– ëª¨ë¸ ìƒì„± ì™„ë£Œ: {args.architecture} (í´ë˜ìŠ¤: {num_classes})")
    print(f"ğŸ“Š íŒŒë¼ë¯¸í„° ìˆ˜: {sum(p.numel() for p in model.parameters()):,}")

    # ì „ì²´ SDA-U ì‹¤í–‰
    print("\n" + "="*80)
    print("ğŸš€ SDA-U í”„ë ˆì„ì›Œí¬ ì‹¤í–‰!")
    print("="*80)
    
    try:
        # ğŸš€ ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ ë¡œë“œ ë˜ëŠ” ì†ŒìŠ¤ ë„ë©”ì¸ í›ˆë ¨
        if args.pretrained_model and os.path.exists(args.pretrained_model):
            print(f"ğŸ“¦ ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ ë¡œë“œ: {args.pretrained_model}")
            checkpoint = torch.load(args.pretrained_model, map_location=device)
            model.load_state_dict(checkpoint['model_state_dict'])
            print("âœ… ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
            
        elif args.skip_source_training:
            print("â­ï¸ ì†ŒìŠ¤ ë„ë©”ì¸ í›ˆë ¨ ìŠ¤í‚µ - ImageNet ì‚¬ì „ í›ˆë ¨ ê°€ì¤‘ì¹˜ë§Œ ì‚¬ìš©")
            print("ğŸ’¡ ì´ ëª¨ë“œì—ì„œëŠ” ResNet ë“±ì˜ ImageNet ì‚¬ì „ í›ˆë ¨ ê°€ì¤‘ì¹˜ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤")
            
        else:
            # 1. ì†ŒìŠ¤ ë„ë©”ì¸ ì‚¬ì „ í›ˆë ¨
            print("ğŸ“š 1ë‹¨ê³„: ì†ŒìŠ¤ ë„ë©”ì¸ ì‚¬ì „ í›ˆë ¨")
            train_model_with_evaluation(model, source_train_loader, source_test_loader, num_epochs=args.num_epochs, 
                                      source_domain=args.source_domain, target_domain=args.target_domain)
        
        # 2. íƒ€ê²Ÿ ë„ë©”ì¸ í‰ê°€ (ì‚¬ì „ í›ˆë ¨ í›„)
        print("\nğŸ“Š 2ë‹¨ê³„: íƒ€ê²Ÿ ë„ë©”ì¸ ì´ˆê¸° í‰ê°€")
        initial_target_acc = evaluate_model(model, target_test_loader, domain_name="íƒ€ê²Ÿ(ì´ˆê¸°)")
        
        # ì†ŒìŠ¤ ë„ë©”ì¸ ì´ˆê¸° ì„±ëŠ¥ë„ ì¸¡ì • (ì°¸ê³ ìš©)
        initial_source_acc = evaluate_model(model, source_test_loader, domain_name="ì†ŒìŠ¤(ì´ˆê¸°)")
        print(f"ğŸ“Š ì†ŒìŠ¤ ë„ë©”ì¸ ì´ˆê¸° ì„±ëŠ¥: {initial_source_acc:.2f}%")
        
        # 3. ì˜¬ë°”ë¥¸ SDA-U ì•Œê³ ë¦¬ì¦˜ ìˆ˜í–‰
        print("\nğŸ¯ SDA-U ì•Œê³ ë¦¬ì¦˜ ì‹œì‘ (ì˜¬ë°”ë¥¸ ìˆœì„œ)")
        
        # 3-1. íƒ€ê²Ÿ ìƒ˜í”Œ ì„ ë³„ (ë¼ë²¨ ì—†ì´)
        curated_samples, uncertainty_scores = unsupervised_target_sample_selection(
            model, target_train_loader, num_samples=args.target_samples)
        
        # 3-2. ìœ í•´í•œ ì†ŒìŠ¤ ìƒ˜í”Œ ì„ ë³„
        harmful_samples, harmful_scores = compute_influence_and_select_harmful(
            model, source_train_loader, curated_samples, num_harmful=50)
        
        # 3-3. ë¨¸ì‹  ì–¸ëŸ¬ë‹ ìˆ˜í–‰
        print("\nğŸ”„ 3ë‹¨ê³„: ë¨¸ì‹  ì–¸ëŸ¬ë‹ ìˆ˜í–‰")
        unlearn_losses = perform_dos_unlearning(
            model, harmful_samples, harmful_scores, 
            num_steps=args.unlearn_steps)
        
        # 3-4. ì„ ë³„ëœ íƒ€ê²Ÿ ìƒ˜í”Œë¡œ Fine-tuning
        retrain_with_curated_target_samples(model, curated_samples, args.adaptation_epochs)
        
        # 4. ìµœì¢… í‰ê°€
        print("\nğŸ“ˆ 4ë‹¨ê³„: ìµœì¢… ì„±ëŠ¥ í‰ê°€")
        final_source_acc = evaluate_model(model, source_test_loader, domain_name="ì†ŒìŠ¤(ìµœì¢…)")
        final_target_acc = evaluate_model(model, target_test_loader, domain_name="íƒ€ê²Ÿ(ìµœì¢…)")
        
        # 5. ê²°ê³¼ ì €ì¥
        print("\nğŸ’¾ 5ë‹¨ê³„: ê²°ê³¼ ì €ì¥")
        results = {
            'experiment_info': {
                'dataset': args.dataset,
                'source_domain': args.source_domain,
                'target_domain': args.target_domain,
                'architecture': args.architecture,
                'num_classes': num_classes
            },
            'performance': {
                'initial_source_accuracy': initial_source_acc,
                'initial_target_accuracy': initial_target_acc,
                'final_source_accuracy': final_source_acc,
                'final_target_accuracy': final_target_acc,
                'target_improvement': final_target_acc - initial_target_acc,
                'source_change': final_source_acc - initial_source_acc
            },
            'sda_u_metrics': {
                'harmful_samples': len(harmful_samples),
                'curated_samples': len(curated_samples),
                'unlearn_steps': args.unlearn_steps,
                'unlearn_losses': unlearn_losses
            },
            'hyperparameters': {
                'batch_size': args.batch_size,
                'learning_rate': args.learning_rate,
                'lambda_u': args.lambda_u,
                'beta': args.beta
            }
        }
        
        # ê²°ê³¼ ì €ì¥ (ì§€ì •ëœ ê²½ë¡œ ì‚¬ìš©)
        results_dir = os.path.dirname(args.results_file)
        os.makedirs(results_dir, exist_ok=True)
        
        # ê²°ê³¼ë¥¼ ì§€ì •ëœ íŒŒì¼ì— ì €ì¥
        with open(args.results_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        print(f"ğŸ’¾ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {args.results_file}")
        
        # ëª¨ë¸ ì €ì¥ (ì§€ì •ëœ ê²½ë¡œ ì‚¬ìš©)
        model_dir = os.path.dirname(args.model_save_dir)
        os.makedirs(model_dir, exist_ok=True)
        model_filename = f"{args.source_domain}2{args.target_domain}_sda_u_model.pt"
        model_path = os.path.join(args.model_save_dir, model_filename)
        
        save_dict = {
            'model_state_dict': model.state_dict(),
            'results': results,
            'args': vars(args)
        }
        torch.save(save_dict, model_path)
        print(f"ğŸ¤– ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {model_path}")
        
        print("\nğŸ‰ SDA-U ì‹¤í—˜ ì™„ë£Œ!")
        print(f"ğŸ“Š íƒ€ê²Ÿ ì •í™•ë„ í–¥ìƒ: {initial_target_acc:.2f}% â†’ {final_target_acc:.2f}% (+{final_target_acc-initial_target_acc:.2f}%)")
        print(f"ğŸ“Š ì†ŒìŠ¤ ì •í™•ë„ ë³€í™”: {initial_source_acc:.2f}% â†’ {final_source_acc:.2f}% ({final_source_acc-initial_source_acc:+.2f}%)")
        
        # ì„±ëŠ¥ ìš”ì•½
        if final_target_acc > initial_target_acc:
            print("âœ… íƒ€ê²Ÿ ë„ë©”ì¸ ì„±ëŠ¥ í–¥ìƒ ì„±ê³µ!")
        else:
            print("âŒ íƒ€ê²Ÿ ë„ë©”ì¸ ì„±ëŠ¥ í–¥ìƒ ì‹¤íŒ¨")
            
        if abs(final_source_acc - initial_source_acc) < 2.0:  # 2% ì´ë‚´ ë³€í™”
            print("âœ… ì†ŒìŠ¤ ë„ë©”ì¸ ì„±ëŠ¥ ìœ ì§€ ì„±ê³µ!")
        else:
            print("âš ï¸ ì†ŒìŠ¤ ë„ë©”ì¸ ì„±ëŠ¥ í¬ê²Œ ë³€í™”ë¨")
        
    except Exception as e:
        print(f"\nâŒ ì‹¤í—˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        import traceback
        traceback.print_exc()
        return 1
    
    return 0

if __name__ == "__main__":
    main() 