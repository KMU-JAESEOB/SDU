#!/usr/bin/env python3
# main.py - SDA-U í”„ë ˆì„ì›Œí¬ ë©”ì¸ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader, Subset
import numpy as np
import json
import os
from datetime import datetime
from tqdm import tqdm
import warnings
import argparse
warnings.filterwarnings('ignore')

print("ğŸš€ SDA-U í”„ë ˆì„ì›Œí¬ ë©”ì¸ ì‹¤í–‰!")
print("=" * 60)

# ì„¤ì • ë¡œë“œ
try:
    from config import get_config
    config = get_config()
    print(f"ğŸš€ ì„¤ì • ë¡œë“œ ì™„ë£Œ: {config['gpu_name']}")
except ImportError:
    # í´ë°± ì„¤ì •
    config = {
        'batch_size': 64,
        'num_epochs': 3,
        'architecture': 'resnet50',  # ê¸°ì¡´ ëª¨ë¸ë¡œ ë³€ê²½
        'target_subset_size': 1000,
        'num_unlearn_steps': 5,
        'influence_samples': 300,
        'lambda_u': 0.6,
        'beta': 0.1,
        'learning_rate': 1e-3,
        'save_models': True,
        'save_results': True,
        'gpu_name': 'Default'
    }
    print("âš™ï¸ ê¸°ë³¸ ì„¤ì • ì‚¬ìš© (ResNet50)")

# ë””ë°”ì´ìŠ¤ ì„¤ì •
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"ğŸ”§ ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")

# ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
os.makedirs('models', exist_ok=True)
os.makedirs('results', exist_ok=True)

# ì „ì—­ ë³€ìˆ˜
best_target_accuracy = 0.0
performance_history = []

# 1. ë°ì´í„° ë³€í™˜ - Office-31 í˜¸í™˜ì„± ê°œì„  (ì¤‘ë³µ ì œê±°)
def get_transforms(architecture):
    """ì•„í‚¤í…ì²˜ì— ë§ëŠ” ë°ì´í„° ë³€í™˜ì„ ë°˜í™˜í•©ë‹ˆë‹¤ (Office-31 í˜¸í™˜)"""
    if 'vit' in architecture.lower():
        # Vision TransformerëŠ” 224x224 í¬ê¸° ì‚¬ìš©
        return transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Lambda(lambda x: x.repeat(3, 1, 1) if x.size(0) == 1 else x),
            transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))  # ImageNet ì •ê·œí™”
        ])
    elif 'resnet' in architecture.lower():
        # ResNetì€ Office-31ì— ë§ê²Œ 224x224, 3ì±„ë„ ì‚¬ìš©
        return transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Lambda(lambda x: x.repeat(3, 1, 1) if x.size(0) == 1 else x),
            transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))  # ImageNet ì •ê·œí™”
        ])
    else:
        # ê¸°ë³¸ ë³€í™˜ (Office-31 í˜¸í™˜)
        return transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Lambda(lambda x: x.repeat(3, 1, 1) if x.size(0) == 1 else x),
            transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))
        ])

# 2. CNN ëª¨ë¸ ì •ì˜
class SimpleCNN(nn.Module):
    def __init__(self, num_classes=31):  # Office-31ì— ë§ê²Œ ê¸°ë³¸ê°’ ë³€ê²½
        super(SimpleCNN, self).__init__()
        self.features = nn.Sequential(
            nn.Conv2d(3, 32, 3, padding=1),  # Office-31ì€ RGB ì´ë¯¸ì§€
            nn.ReLU(),
            nn.MaxPool2d(2),
            nn.Conv2d(32, 64, 3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2),
            nn.Conv2d(64, 128, 3, padding=1),
            nn.ReLU(),
            nn.AdaptiveAvgPool2d((1, 1))
        )
        self.classifier = nn.Sequential(
            nn.Dropout(0.5),
            nn.Linear(128, num_classes)
        )
    
    def forward(self, x):
        x = self.features(x)
        x = x.view(x.size(0), -1)
        x = self.classifier(x)
        return x

# 3. ëª¨ë¸ ìƒì„± (Office-31 í˜¸í™˜ì„± ê°•í™”)
def create_model(architecture, num_classes=31):
    """Office-31 í˜¸í™˜ ëª¨ë¸ ìƒì„± (ì‚¬ì „ í›ˆë ¨ ê°€ì¤‘ì¹˜ + ì±„ë„ ìµœì í™”)"""
    print(f"ğŸ—ï¸ ëª¨ë¸ ìƒì„± ì¤‘: {architecture}")
    
    # ì‚¬ì „ í›ˆë ¨ ê°€ì¤‘ì¹˜ ì‚¬ìš© ì—¬ë¶€ í™•ì¸
    use_pretrained = config.get('use_pretrained', True)
    weights = 'IMAGENET1K_V1' if use_pretrained else None
    
    try:
        if architecture == 'custom_cnn':
            model = SimpleCNN(num_classes=num_classes)
            print("âœ… ì»¤ìŠ¤í…€ CNN ìƒì„± ì™„ë£Œ (3ì±„ë„ RGB ì…ë ¥)")
            return model
            
        elif 'resnet' in architecture.lower():
            # ResNet ê³„ì—´ ëª¨ë¸ ìƒì„± (ì‚¬ì „ í›ˆë ¨ ê°€ì¤‘ì¹˜ ì‚¬ìš©)
            if architecture == 'resnet18':
                model = torchvision.models.resnet18(weights=weights)
                print(f"ğŸ“¦ ResNet18 ë¡œë“œ (ì‚¬ì „í›ˆë ¨: {use_pretrained})")
            elif architecture == 'resnet50':
                model = torchvision.models.resnet50(weights=weights)
                print(f"ğŸ“¦ ResNet50 ë¡œë“œ (ì‚¬ì „í›ˆë ¨: {use_pretrained})")
            else:
                # ê¸°ë³¸ê°’ìœ¼ë¡œ ResNet50 ì‚¬ìš©
                model = torchvision.models.resnet50(weights=weights)
                print(f"ğŸ“¦ ì•Œ ìˆ˜ ì—†ëŠ” ResNet ë³€í˜• {architecture}, ResNet50 ì‚¬ìš© (ì‚¬ì „í›ˆë ¨: {use_pretrained})")
            
            # ğŸš¨ í•µì‹¬: ì²« ë²ˆì§¸ convolutionì€ ì‚¬ì „í›ˆë ¨ì‹œ ì´ë¯¸ 3ì±„ë„ì´ë¯€ë¡œ ê·¸ëŒ€ë¡œ ì‚¬ìš©
            if use_pretrained:
                print(f"âœ… ì‚¬ì „ í›ˆë ¨ëœ conv1 ì‚¬ìš©: {model.conv1}")
            else:
                # ì‚¬ì „ í›ˆë ¨ì„ ì‚¬ìš©í•˜ì§€ ì•Šì„ ë•Œë§Œ ìˆ˜ë™ìœ¼ë¡œ 3ì±„ë„ ì„¤ì •
                model.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)
                print(f"âœ… ìƒˆë¡œìš´ conv1: {model.conv1} (3ì±„ë„ â†’ 64ì±„ë„)")
            
            # âš¡ ë°±ë³¸ ë™ê²° ì„¤ì • (fine-tuning vs full training)
            freeze_backbone = config.get('freeze_backbone', False)
            if freeze_backbone and use_pretrained:
                for param in model.parameters():
                    param.requires_grad = False
                # ë¶„ë¥˜ ë ˆì´ì–´ë§Œ í•™ìŠµ ê°€ëŠ¥í•˜ê²Œ ì„¤ì •
                model.fc.requires_grad_(True)
                print("ğŸ”’ ë°±ë³¸ ë™ê²°, ë¶„ë¥˜ ë ˆì´ì–´ë§Œ í•™ìŠµ")
            else:
                print("ğŸ”“ ì „ì²´ ëª¨ë¸ fine-tuning")
            
            # ë§ˆì§€ë§‰ ë¶„ë¥˜ ë ˆì´ì–´ë¥¼ Office-31 í´ë˜ìŠ¤ ìˆ˜ì— ë§ê²Œ ì¡°ì •
            model.fc = nn.Linear(model.fc.in_features, num_classes)
            print(f"âœ… ë¶„ë¥˜ ë ˆì´ì–´ ì¡°ì •: â†’ {num_classes}ê°œ í´ë˜ìŠ¤")
            
            return model
            
        elif 'vit' in architecture.lower():
            # Vision Transformer ëª¨ë¸ ìƒì„±
            if architecture == 'vit_b_16':
                model = torchvision.models.vit_b_16(weights=weights)
                model.heads = nn.Linear(model.heads.head.in_features, num_classes)
            elif architecture == 'vit_l_16':
                model = torchvision.models.vit_l_16(weights=weights)
                model.heads = nn.Linear(model.heads.head.in_features, num_classes)
            else:
                model = torchvision.models.vit_b_16(weights=weights)
                model.heads = nn.Linear(model.heads.head.in_features, num_classes)
            
            print(f"âœ… ViT ëª¨ë¸ ìƒì„± ì™„ë£Œ (224x224 3ì±„ë„ ì…ë ¥, ì‚¬ì „í›ˆë ¨: {use_pretrained})")
            return model
            
        else:
            # ì•Œ ìˆ˜ ì—†ëŠ” ì•„í‚¤í…ì²˜ëŠ” ResNet50ìœ¼ë¡œ ëŒ€ì²´
            print(f"âš ï¸ ì•Œ ìˆ˜ ì—†ëŠ” ì•„í‚¤í…ì²˜: {architecture}, ResNet50ìœ¼ë¡œ ëŒ€ì²´")
            model = torchvision.models.resnet50(weights=weights)
            model.fc = nn.Linear(model.fc.in_features, num_classes)
            print(f"âœ… ResNet50 ëŒ€ì²´ ëª¨ë¸ ìƒì„± ì™„ë£Œ (3ì±„ë„ RGB ì…ë ¥, ì‚¬ì „í›ˆë ¨: {use_pretrained})")
            return model
            
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ìƒì„± ì‹¤íŒ¨: {e}")
        print("ğŸ”„ ê¸°ë³¸ CNNìœ¼ë¡œ ëŒ€ì²´")
        model = SimpleCNN(num_classes=num_classes)
        print("âœ… ê¸°ë³¸ CNN ìƒì„± ì™„ë£Œ (3ì±„ë„ RGB ì…ë ¥)")
        return model

# 4. ë°ì´í„° ë¡œë” (í†µí•©ëœ ë°ì´í„°ì…‹ ê´€ë¦¬ì ì‚¬ìš©)
def get_data_loaders(batch_size, architecture, source_dataset_name='SVHN', target_dataset_name='MNIST'):
    """í†µí•©ëœ ë°ì´í„°ì…‹ ê´€ë¦¬ìë¥¼ ì‚¬ìš©í•˜ëŠ” ë°ì´í„° ë¡œë” ìƒì„± (ì¡°ê±´ë¶€ ì±„ë„ í†µì¼)"""
    
    from dataset_manager import dataset_manager
    
    print(f"ğŸ¯ ë„ë©”ì¸ ì ì‘ ì„¤ì •:")
    print(f"   ğŸ“¤ ì†ŒìŠ¤: {source_dataset_name}")
    print(f"   ğŸ“¥ íƒ€ê²Ÿ: {target_dataset_name}")
    
    # ğŸ”§ ì±„ë„ í†µì¼ì´ í•„ìš”í•œ ì¡°í•© ì •ì˜
    channel_unification_needed = [
        ('SVHN', 'MNIST'),          # 3ì±„ë„ â†’ 1ì±„ë„
        ('MNIST', 'SVHN'),          # 1ì±„ë„ â†’ 3ì±„ë„  
        ('SVHN', 'FashionMNIST'),   # 3ì±„ë„ â†’ 1ì±„ë„
        ('FashionMNIST', 'SVHN'),   # 1ì±„ë„ â†’ 3ì±„ë„
        ('CIFAR10', 'MNIST'),       # 3ì±„ë„ â†’ 1ì±„ë„
        ('MNIST', 'CIFAR10'),       # 1ì±„ë„ â†’ 3ì±„ë„
        ('CIFAR10', 'FashionMNIST'), # 3ì±„ë„ â†’ 1ì±„ë„
        ('FashionMNIST', 'CIFAR10'), # 1ì±„ë„ â†’ 3ì±„ë„
    ]
    
    current_combination = (source_dataset_name, target_dataset_name)
    needs_unification = current_combination in channel_unification_needed
    
    try:
        if needs_unification:
            print(f"ğŸ”§ ì±„ë„ í†µì¼ ì ìš©: {source_dataset_name}({dataset_manager.get_dataset_info(source_dataset_name)['channels']}ch) â†” {target_dataset_name}({dataset_manager.get_dataset_info(target_dataset_name)['channels']}ch)")
            
            # ğŸ”„ í†µì¼ëœ ë„ë©”ì¸ ì ì‘ ë¡œë” ì‚¬ìš©
            source_train_loader, target_train_loader, source_test_loader, target_test_loader = \
                dataset_manager.load_dataset_for_domain_adaptation(
                    source_dataset_name, target_dataset_name, 
                    batch_size=batch_size, shuffle=True
                )
        else:
            print(f"ğŸ“¦ ì¼ë°˜ ë¡œë” ì‚¬ìš©: ì±„ë„ í†µì¼ ë¶ˆí•„ìš”")
            
            # ğŸ”„ ì¼ë°˜ ë°ì´í„°ì…‹ ë¡œë” ì‚¬ìš© (ê¸°ì¡´ ë°©ì‹)
            source_train_loader, source_test_loader = dataset_manager.load_dataset(
                source_dataset_name, batch_size=batch_size, shuffle=True)
            target_train_loader, target_test_loader = dataset_manager.load_dataset(
                target_dataset_name, batch_size=batch_size, shuffle=True)
        
        print(f"âœ… ë°ì´í„° ë¡œë” ìƒì„± ì™„ë£Œ!")
        print(f"   ğŸ“Š ì†ŒìŠ¤ í›ˆë ¨ ë°°ì¹˜ ìˆ˜: {len(source_train_loader)}")
        print(f"   ğŸ“Š íƒ€ê²Ÿ í›ˆë ¨ ë°°ì¹˜ ìˆ˜: {len(target_train_loader)}")
        
        return source_train_loader, target_train_loader, source_test_loader, target_test_loader
        
    except Exception as e:
        print(f"âŒ ë°ì´í„° ë¡œë” ìƒì„± ì‹¤íŒ¨: {str(e)}")
        raise

# 5. ëª¨ë¸ ì €ì¥ í•¨ìˆ˜ (ë„ë©”ì¸ë³„ êµ¬ë¶„ ì €ì¥)
def save_model(model, filename, additional_info=None, source_domain=None, target_domain=None):
    """ëª¨ë¸ê³¼ ì¶”ê°€ ì •ë³´ë¥¼ ë„ë©”ì¸ë³„ ë””ë ‰í† ë¦¬ì— ì €ì¥í•©ë‹ˆë‹¤."""
    
    # ë„ë©”ì¸ ì •ë³´ ì¶”ì¶œ (íŒŒë¼ë¯¸í„° ìš°ì„ , ì—†ìœ¼ë©´ configì—ì„œ)
    source_dataset = source_domain if source_domain else config.get('source_dataset', 'Unknown')
    target_dataset = target_domain if target_domain else config.get('target_dataset', 'Unknown')
    
    # ë„ë©”ì¸ ì´ë¦„ ì •ë¦¬ (Office31_Amazon â†’ Amazon)
    source_name = source_dataset.split('_')[-1] if '_' in source_dataset else source_dataset
    target_name = target_dataset.split('_')[-1] if '_' in target_dataset else target_dataset
    
    # ì‹¤í—˜ ì´ë¦„ ìƒì„±
    experiment_name = f"{source_name}2{target_name}"
    
    # ë„ë©”ì¸ë³„ ë””ë ‰í† ë¦¬ ìƒì„±
    domain_dir = f'models/{experiment_name}'
    os.makedirs(domain_dir, exist_ok=True)
    
    # íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ê°€ (ì¤‘ë³µ ë°©ì§€)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    base_name = filename.replace('.pt', '')
    timestamped_filename = f"{base_name}_{timestamp}.pt"
    
    # ì €ì¥í•  ì •ë³´ êµ¬ì„±
    save_dict = {
        'model_state_dict': model.state_dict(),
        'config': config,
        'experiment_name': experiment_name,
        'source_dataset': source_dataset,
        'target_dataset': target_dataset,
        'timestamp': datetime.now().isoformat(),
        'filename': filename
    }
    if additional_info:
        save_dict.update(additional_info)
    
    # ë‘ ê°€ì§€ ë°©ì‹ìœ¼ë¡œ ì €ì¥
    full_path = f'{domain_dir}/{filename}'
    timestamped_path = f'{domain_dir}/{timestamped_filename}'
    
    # 1. ìµœì‹  ë²„ì „ (ë®ì–´ì“°ê¸°)
    torch.save(save_dict, full_path)
    
    # 2. íƒ€ì„ìŠ¤íƒ¬í”„ ë²„ì „ (ë³´ì¡´ìš©)
    torch.save(save_dict, timestamped_path)
    
    print(f"ğŸ’¾ ëª¨ë¸ ì €ì¥: {full_path}")
    print(f"ğŸ’¾ ë°±ì—… ì €ì¥: {timestamped_path}")
    
    return full_path, timestamped_path

# 6. ì„±ëŠ¥ ì¶”ì  í•¨ìˆ˜
def track_performance(epoch, source_acc, target_acc, loss):
    """ì„±ëŠ¥ì„ ì¶”ì í•˜ê³  ìµœê³  ì„±ëŠ¥ ëª¨ë¸ì„ ì €ì¥í•©ë‹ˆë‹¤."""
    global best_target_accuracy, performance_history
    
    performance_history.append({
        'epoch': epoch,
        'source_accuracy': source_acc,
        'target_accuracy': target_acc,
        'loss': loss,
        'timestamp': datetime.now().isoformat()
    })
    
    if target_acc > best_target_accuracy:
        best_target_accuracy = target_acc
        return True  # ìƒˆë¡œìš´ ìµœê³  ì„±ëŠ¥
    return False

# 7. í–¥ìƒëœ í›ˆë ¨ í•¨ìˆ˜ (ê³ ê¸‰ ì„¤ì • ì ìš©)
def train_model_with_evaluation(model, source_loader, target_loader, num_epochs=3, source_domain=None, target_domain=None):
    print(f"ğŸ‹ï¸ ê³ ì„±ëŠ¥ ëª¨ë¸ í›ˆë ¨ ì‹œì‘ ({num_epochs} ì—í¬í¬)")
    
    model.train()
    criterion = nn.CrossEntropyLoss()
    
    # ğŸ”§ ê³ ê¸‰ ì˜µí‹°ë§ˆì´ì € ì„¤ì •
    learning_rate = config.get('learning_rate', 2e-4)
    weight_decay = config.get('weight_decay', 1e-4)
    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)
    print(f"âš™ï¸ ì˜µí‹°ë§ˆì´ì €: AdamW (lr={learning_rate}, wd={weight_decay})")
    
    # ğŸ”¥ í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •
    scheduler_type = config.get('scheduler_type', 'cosine')
    warmup_epochs = config.get('warmup_epochs', 2)
    
    if scheduler_type == 'cosine':
        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)
        print("ğŸ“ˆ ì½”ì‚¬ì¸ ì–´ë‹ë§ ìŠ¤ì¼€ì¤„ëŸ¬ ì‚¬ìš©")
    elif scheduler_type == 'step':
        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=num_epochs//3, gamma=0.1)
        print("ğŸ“‰ ìŠ¤í… ìŠ¤ì¼€ì¤„ëŸ¬ ì‚¬ìš©")
    else:
        scheduler = None
        print("ğŸ“Š ê³ ì • í•™ìŠµë¥  ì‚¬ìš©")
    
    # ğŸ¯ ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘ ì„¤ì •
    gradient_clip = config.get('gradient_clip', 1.0)
    print(f"âœ‚ï¸ ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘: {gradient_clip}")
    
    losses = []
    
    for epoch in range(num_epochs):
        epoch_loss = 0.0
        batch_count = 0
        
        # ì›Œë°ì—… í•™ìŠµë¥  ì¡°ì •
        if epoch < warmup_epochs and scheduler is not None:
            warmup_lr = learning_rate * (epoch + 1) / warmup_epochs
            for param_group in optimizer.param_groups:
                param_group['lr'] = warmup_lr
            print(f"ğŸ”¥ ì›Œë°ì—… í•™ìŠµë¥ : {warmup_lr:.6f}")
        
        pbar = tqdm(source_loader, desc=f"ì—í¬í¬ {epoch+1}/{num_epochs}")
        
        for batch_idx, (data, target) in enumerate(pbar):
            data, target = data.to(device), target.to(device)
            
            optimizer.zero_grad()
            output = model(data)
            loss = criterion(output, target)
            loss.backward()
            
            # ğŸ¯ ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘ ì ìš©
            if gradient_clip > 0:
                torch.nn.utils.clip_grad_norm_(model.parameters(), gradient_clip)
            
            optimizer.step()
            
            epoch_loss += loss.item()
            batch_count += 1
            
            # í˜„ì¬ í•™ìŠµë¥  í‘œì‹œ
            current_lr = optimizer.param_groups[0]['lr']
            pbar.set_postfix({
                'Loss': f'{loss.item():.4f}',
                'LR': f'{current_lr:.6f}'
            })
            
            # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ëª¨ë“œ (ì„¤ì •ì—ì„œ ì œì–´)
            if config.get('quick_test', False) and batch_count >= 30:
                break
        
        avg_loss = epoch_loss / batch_count
        losses.append(avg_loss)
        
        # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸ (ì›Œë°ì—… ì´í›„)
        if epoch >= warmup_epochs and scheduler is not None:
            scheduler.step()
            print(f"ğŸ“Š í•™ìŠµë¥  ì—…ë°ì´íŠ¸: {optimizer.param_groups[0]['lr']:.6f}")
        
        # ì¤‘ê°„ í‰ê°€ (ë™ì  ë°ì´í„°ì…‹ ì´ë¦„ í‘œì‹œ)
        print(f"\nğŸ“Š ì—í¬í¬ {epoch+1} ì¤‘ê°„ í‰ê°€:")
        source_dataset_name = source_domain if source_domain else 'Unknown'
        target_dataset_name = target_domain if target_domain else 'Unknown'
        source_acc = evaluate_model(model, source_loader, max_batches=15, domain_name=f"ì†ŒìŠ¤({source_dataset_name})")
        target_acc = evaluate_model(model, target_loader, max_batches=15, domain_name=f"íƒ€ê²Ÿ({target_dataset_name})")
        
        # ì„±ëŠ¥ ì¶”ì  ë° ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥
        is_best = track_performance(epoch+1, source_acc, target_acc, avg_loss)
        if is_best:
            save_model(model, 'best_model.pt', {
                'epoch': epoch+1,
                'source_accuracy': source_acc,
                'target_accuracy': target_acc,
                'loss': avg_loss,
                'learning_rate': optimizer.param_groups[0]['lr']
            }, source_domain=source_domain, target_domain=target_domain)
            print(f"ğŸ† ìƒˆë¡œìš´ ìµœê³  ì„±ëŠ¥! íƒ€ê²Ÿ ì •í™•ë„: {target_acc:.2f}%")
        
        print(f"ì—í¬í¬ {epoch+1} ì™„ë£Œ, í‰ê·  ì†ì‹¤: {avg_loss:.4f}")
    
    # ì†ŒìŠ¤ ëª¨ë¸ ì €ì¥
    save_model(model, 'source_model.pt', {
        'final_source_accuracy': source_acc,
        'final_target_accuracy': target_acc,
        'training_losses': losses,
        'final_learning_rate': optimizer.param_groups[0]['lr']
    }, source_domain=source_domain, target_domain=target_domain)
    
    print(f"âœ… í›ˆë ¨ ì™„ë£Œ! ìµœì¢… í•™ìŠµë¥ : {optimizer.param_groups[0]['lr']:.6f}")
    return losses

# 8. í–¥ìƒëœ ì˜í–¥ë„ ê³„ì‚° (ë…¼ë¬¸ ê³µì‹ ê¸°ë°˜) - ì•ˆì „ ì¥ì¹˜ ì¶”ê°€
def compute_influence_scores_enhanced(model, source_loader, target_batch, num_samples=300):
    print("ğŸ” í–¥ìƒëœ ì˜í–¥ë„ ì ìˆ˜ ê³„ì‚° ì¤‘... (Pseudo-label ì‚¬ìš©)")
    
    model.eval()
    criterion = nn.CrossEntropyLoss()
    
    target_data, _ = target_batch  # âŒ ì‹¤ì œ ë¼ë²¨ ë¬´ì‹œ (Unsupervised)
    target_data = target_data.to(device)
    
    # ğŸ¯ Pseudo-label ìƒì„± (ì‹¤ì œ ë¼ë²¨ ëŒ€ì‹  ì‚¬ìš©)
    with torch.no_grad():
        target_output = model(target_data)
        target_pseudo_labels = torch.argmax(target_output, dim=1)
        confidence_scores = torch.max(torch.softmax(target_output, dim=1), dim=1)[0]
    
    print(f"ğŸ¯ Pseudo-label ìƒì„± ì™„ë£Œ: í‰ê·  í™•ì‹ ë„ {confidence_scores.mean().item():.3f}")
    
    # ğŸš¨ í´ë˜ìŠ¤ ì¸ë±ìŠ¤ ì•ˆì „ ê²€ì¦ ì¶”ê°€
    num_classes = model.fc.out_features if hasattr(model, 'fc') else model.classifier[-1].out_features
    print(f"ğŸ“Š ëª¨ë¸ í´ë˜ìŠ¤ ìˆ˜: {num_classes}")
    print(f"ğŸ¯ Pseudo-label ë²”ìœ„: {target_pseudo_labels.min().item()} ~ {target_pseudo_labels.max().item()}")
    
    # ë¼ë²¨ ë²”ìœ„ ê²€ì¦ (Pseudo-labelì€ ì´ë¯¸ ëª¨ë¸ ì¶œë ¥ì´ë¯€ë¡œ ì•ˆì „í•´ì•¼ í•¨)
    if target_pseudo_labels.max().item() >= num_classes or target_pseudo_labels.min().item() < 0:
        print(f"âŒ Pseudo-label ë²”ìœ„ ì˜¤ë¥˜! ë²”ìœ„: [{target_pseudo_labels.min().item()}, {target_pseudo_labels.max().item()}]")
        target_pseudo_labels = torch.clamp(target_pseudo_labels, 0, num_classes - 1)
        print(f"âœ… ìˆ˜ì •ëœ Pseudo-label ë²”ìœ„: {target_pseudo_labels.min().item()} ~ {target_pseudo_labels.max().item()}")
    
    # íƒ€ê²Ÿ ë°°ì¹˜ ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° (Pseudo-label ì‚¬ìš©)
    model.zero_grad()
    target_output = model(target_data)
    target_loss = criterion(target_output, target_pseudo_labels)  # âœ… Pseudo-label ì‚¬ìš©
    target_loss.backward()
    
    target_grads = []
    for param in model.parameters():
        if param.grad is not None:
            target_grads.append(param.grad.data.flatten())
    target_grad = torch.cat(target_grads) if target_grads else torch.zeros(1).to(device)
    
    # í—¤ì‹œì•ˆ ê·¼ì‚¬ (ë‹¨ìˆœí™”ëœ ë²„ì „)
    hessian_inv_approx = 1.0  # ì‹¤ì œë¡œëŠ” ë” ë³µì¡í•œ ê³„ì‚°ì´ í•„ìš”
    
    influence_scores = []
    sample_count = 0
    invalid_samples = 0
    
    print(f"ğŸ¯ {num_samples}ê°œ ìƒ˜í”Œì˜ ì˜í–¥ë„ ê³„ì‚°...")
    
    for batch_idx, (data, labels) in enumerate(source_loader):
        if sample_count >= num_samples:
            break
            
        data, labels = data.to(device), labels.to(device)
        
        # ğŸš¨ ì†ŒìŠ¤ ë¼ë²¨ ë²”ìœ„ ê²€ì¦ ë° ìˆ˜ì •
        original_labels = labels.clone()
        labels = torch.clamp(labels, 0, num_classes - 1)
        
        # ìˆ˜ì •ëœ ë¼ë²¨ ìˆ˜ ì¹´ìš´íŠ¸
        modified_count = (original_labels != labels).sum().item()
        if modified_count > 0:
            invalid_samples += modified_count
        
        for i in range(min(len(data), num_samples - sample_count)):
            try:
                model.zero_grad()
                output = model(data[i:i+1])
                
                # ì¶”ê°€ ì•ˆì „ ê²€ì¦
                if labels[i] >= num_classes or labels[i] < 0:
                    print(f"âš ï¸ ê±´ë„ˆë›°ëŠ” ìƒ˜í”Œ: ë¼ë²¨ {labels[i].item()}, í´ë˜ìŠ¤ ìˆ˜ {num_classes}")
                    continue
                
                loss = criterion(output, labels[i:i+1])
                loss.backward()
                
                sample_grads = []
                for param in model.parameters():
                    if param.grad is not None:
                        sample_grads.append(param.grad.data.flatten())
                sample_grad = torch.cat(sample_grads) if sample_grads else torch.zeros(1).to(device)
                
                # ë…¼ë¬¸ ê³µì‹: I_up(z_i, D_T^batch) = -âˆ‡_Î¸ L(D_T^batch, Î¸)^T H_Î¸^(-1) âˆ‡_Î¸ L(z_i, Î¸)
                influence = -torch.dot(target_grad, sample_grad).item() * hessian_inv_approx
                influence_scores.append(influence)
                sample_count += 1
                
            except RuntimeError as e:
                if "assert" in str(e).lower() or "index" in str(e).lower():
                    print(f"âš ï¸ ìƒ˜í”Œ ê±´ë„ˆë›°ê¸° (ë¼ë²¨ ì˜¤ë¥˜): {e}")
                    invalid_samples += 1
                    continue
                else:
                    raise e
        
        if sample_count >= num_samples:
            break
    
    if invalid_samples > 0:
        print(f"âš ï¸ ìˆ˜ì •/ê±´ë„ˆë›´ ìƒ˜í”Œ: {invalid_samples}ê°œ")
    
    print(f"âœ… {len(influence_scores)}ê°œ ìƒ˜í”Œ ì˜í–¥ë„ ê³„ì‚° ì™„ë£Œ (Pseudo-label ê¸°ë°˜)")
    return influence_scores

# 9. í•˜ì´ë¸Œë¦¬ë“œ ìŠ¤ì½”ì–´ë§ (ë…¼ë¬¸ êµ¬í˜„)
def compute_hybrid_scores(model, target_samples, influence_scores, lambda_u=0.6, beta=0.1):
    """ë…¼ë¬¸ì˜ í•˜ì´ë¸Œë¦¬ë“œ ìŠ¤ì½”ì–´ë§: S(x) = Î»_uÂ·I(x;Î¸') + (1-Î»_u)Â·div(x, D_T^sub) + Î²Â·H(x)"""
    print("ğŸ¯ í•˜ì´ë¸Œë¦¬ë“œ ìŠ¤ì½”ì–´ ê³„ì‚° ì¤‘...")
    
    model.eval()
    hybrid_scores = []
    
    with torch.no_grad():
        for i, (data, _) in enumerate(target_samples):
            if i >= len(influence_scores):
                break
                
            # ë°ì´í„°ë¥¼ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™í•˜ê³  ë°°ì¹˜ ì°¨ì› ì¶”ê°€
            data = data.to(device)
            if len(data.shape) == 3:  # [C, H, W] -> [1, C, H, W]
                data = data.unsqueeze(0)
            
            output = model(data)
            probs = torch.softmax(output, dim=1)
            
            # 1. ì˜í–¥ë„ ìŠ¤ì½”ì–´ (ì •ê·œí™”)
            influence_score = influence_scores[i]
            
            # 2. ë‹¤ì–‘ì„± ìŠ¤ì½”ì–´ (ë‹¨ìˆœí™”ëœ ë²„ì „)
            diversity_score = torch.max(probs).item()  # ìµœëŒ€ í™•ë¥ ì˜ ì—­ìˆ˜ë¡œ ê·¼ì‚¬
            
            # 3. ë¶ˆí™•ì‹¤ì„± ìŠ¤ì½”ì–´ (ì—”íŠ¸ë¡œí”¼)
            entropy = -torch.sum(probs * torch.log(probs + 1e-8)).item()
            
            # í•˜ì´ë¸Œë¦¬ë“œ ìŠ¤ì½”ì–´ ê³„ì‚°
            hybrid_score = (lambda_u * influence_score + 
                          (1 - lambda_u) * (1 - diversity_score) + 
                          beta * entropy)
            
            hybrid_scores.append(hybrid_score)
    
    print(f"âœ… {len(hybrid_scores)}ê°œ í•˜ì´ë¸Œë¦¬ë“œ ìŠ¤ì½”ì–´ ê³„ì‚° ì™„ë£Œ")
    return hybrid_scores

# 10. í–¥ìƒëœ ì–¸ëŸ¬ë‹ (DOS ì•Œê³ ë¦¬ì¦˜) - BatchNorm ì•ˆì „ ë²„ì „
def perform_dos_unlearning(model, harmful_data, influence_scores, num_steps=5):
    print("ğŸ”„ DOS (Dynamic Orthogonal Scaling) ì–¸ëŸ¬ë‹ ìˆ˜í–‰ ì¤‘...")
    
    # BatchNorm ë¬¸ì œ í•´ê²°: í›ˆë ¨ ëª¨ë“œ ìœ ì§€í•˜ë˜ ë°°ì¹˜ í¬ê¸° ì¡°ì •
    model.train()
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=1e-4)
    
    # ë™ì  ê°€ì¤‘ì¹˜ ê³„ì‚° (ì˜í–¥ë„ì— ë¹„ë¡€)
    abs_scores = [abs(s) for s in influence_scores]
    max_abs = max(abs_scores) if abs_scores else 1.0
    weights = [s / max_abs for s in abs_scores]  # ì •ê·œí™”ëœ ê°€ì¤‘ì¹˜
    
    unlearn_losses = []
    
    # BatchNorm ì•ˆì •ì„±ì„ ìœ„í•œ ìµœì†Œ ë°°ì¹˜ í¬ê¸° ë³´ì¥
    min_batch_size = 2  # BatchNormì´ ì•ˆì •ì ìœ¼ë¡œ ì‘ë™í•˜ëŠ” ìµœì†Œ í¬ê¸°
    effective_batch_size = max(min_batch_size, min(8, len(harmful_data)))
    
    for step in range(num_steps):
        total_loss = 0.0
        processed_batches = 0
        
        # ë°ì´í„°ë¥¼ ë°°ì¹˜ë¡œ ê·¸ë£¹í•‘
        for batch_start in range(0, len(harmful_data), effective_batch_size):
            batch_end = min(batch_start + effective_batch_size, len(harmful_data))
            
            # ë°°ì¹˜ê°€ ë„ˆë¬´ ì‘ìœ¼ë©´ ê±´ë„ˆë›°ê¸° (BatchNorm ì•ˆì •ì„±)
            if batch_end - batch_start < min_batch_size:
                continue
            
            batch_data = []
            batch_labels = []
            batch_weights = []
            
            # ë°°ì¹˜ ë°ì´í„° ì¤€ë¹„
            for i in range(batch_start, batch_end):
                if i >= len(harmful_data) or i >= len(weights):
                    break
                    
                data, label = harmful_data[i]
                
                # ë°ì´í„° íƒ€ì… í™•ì¸ ë° ë³€í™˜
                data = data.to(device)
                if isinstance(label, int):
                    label = torch.tensor(label).to(device)
                else:
                    label = label.to(device)
                
                batch_data.append(data)
                batch_labels.append(label)
                batch_weights.append(abs(weights[i]))
            
            if len(batch_data) < min_batch_size:
                continue
                
            # ë°°ì¹˜ í…ì„œ ìƒì„±
            batch_data_tensor = torch.stack(batch_data)
            batch_labels_tensor = torch.stack(batch_labels)
            batch_weights_tensor = torch.tensor(batch_weights, device=device)
            
            optimizer.zero_grad()
            
            # ìˆœì „íŒŒ (BatchNormì´ ì•ˆì „í•˜ê²Œ ì‘ë™)
            output = model(batch_data_tensor)
            
            # ê°œë³„ ìƒ˜í”Œ ì†ì‹¤ ê³„ì‚° í›„ ê°€ì¤‘ì¹˜ ì ìš©
            batch_loss = 0.0
            for j in range(len(batch_data_tensor)):
                sample_loss = criterion(output[j:j+1], batch_labels_tensor[j:j+1])
                weighted_sample_loss = sample_loss * batch_weights_tensor[j]
                batch_loss += weighted_sample_loss
            
            # ë°°ì¹˜ í‰ê·  ì†ì‹¤
            avg_batch_loss = batch_loss / len(batch_data_tensor)
            
            # ì–¸ëŸ¬ë‹ì„ ìœ„í•œ ê·¸ë˜ë””ì–¸íŠ¸ ë°˜ì „ (ìŒì˜ ê·¸ë˜ë””ì–¸íŠ¸)
            (-avg_batch_loss).backward()
            optimizer.step()
            
            total_loss += avg_batch_loss.item()
            processed_batches += 1
        
        avg_loss = total_loss / max(1, processed_batches)
        unlearn_losses.append(avg_loss)
        
        if (step + 1) % 2 == 0:
            print(f"  DOS ìŠ¤í… {step + 1}/{num_steps}, ê°€ì¤‘ ì†ì‹¤: {avg_loss:.4f}")
    
    print("âœ… DOS ì–¸ëŸ¬ë‹ ì™„ë£Œ")
    return unlearn_losses

# 11. íƒ€ê²Ÿ ë„ë©”ì¸ ì¬í•™ìŠµ (Self-training)
def retrain_with_curated_target_samples(model, curated_samples, adaptation_epochs=5):
    """íë ˆì´ì…˜ëœ íƒ€ê²Ÿ ìƒ˜í”Œë¡œ Self-training ìˆ˜í–‰"""
    print(f"ğŸ“ íë ˆì´ì…˜ëœ íƒ€ê²Ÿ ìƒ˜í”Œë¡œ ì¬í•™ìŠµ ì‹œì‘ ({adaptation_epochs}ì—í¬í¬)")
    
    model.train()
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=1e-4)
    
    confidence_threshold = 0.8  # í™•ì‹  ìˆëŠ” ì˜ˆì¸¡ë§Œ ì‚¬ìš©
    
    for epoch in range(adaptation_epochs):
        total_loss = 0.0
        confident_samples = 0
        total_samples = 0
        
        # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬
        batch_size = 16
        for i in range(0, len(curated_samples), batch_size):
            batch_end = min(i + batch_size, len(curated_samples))
            batch_data = []
            
            for j in range(i, batch_end):
                data, _ = curated_samples[j]  # ì‹¤ì œ ë¼ë²¨ ë¬´ì‹œ
                batch_data.append(data)
            
            if len(batch_data) == 0:
                continue
                
            batch_tensor = torch.stack(batch_data).to(device)
            
            # Pseudo-label ìƒì„± ë° í™•ì‹ ë„ ê³„ì‚°
            with torch.no_grad():
                output = model(batch_tensor)
                probs = torch.softmax(output, dim=1)
                confidence, pseudo_labels = torch.max(probs, dim=1)
            
            # í™•ì‹  ìˆëŠ” ìƒ˜í”Œë§Œ ì„ ë³„
            confident_mask = confidence > confidence_threshold
            total_samples += len(batch_data)
            
            if confident_mask.sum() > 0:
                confident_data = batch_tensor[confident_mask]
                confident_labels = pseudo_labels[confident_mask]
                confident_samples += confident_mask.sum().item()
                
                # í™•ì‹  ìˆëŠ” ìƒ˜í”Œë¡œ í•™ìŠµ
                optimizer.zero_grad()
                output = model(confident_data)
                loss = criterion(output, confident_labels)
                loss.backward()
                optimizer.step()
                
                total_loss += loss.item()
        
        avg_loss = total_loss / max(1, confident_samples)
        confidence_ratio = confident_samples / max(1, total_samples)
        
        if (epoch + 1) % 2 == 0:
            print(f"  ì—í¬í¬ {epoch+1}/{adaptation_epochs}: "
                  f"ì†ì‹¤={avg_loss:.4f}, "
                  f"í™•ì‹  ìƒ˜í”Œ={confident_samples}/{total_samples} ({confidence_ratio:.1%})")
    
    print(f"âœ… íƒ€ê²Ÿ ë„ë©”ì¸ ì¬í•™ìŠµ ì™„ë£Œ (ìµœì¢… í™•ì‹ ë„: {confidence_ratio:.1%})")

# 12. í–¥ìƒëœ í‰ê°€ í•¨ìˆ˜
def evaluate_model(model, data_loader, max_batches=20, domain_name=""):
    model.eval()
    correct = 0
    total = 0
    
    with torch.no_grad():
        for batch_idx, (data, target) in enumerate(data_loader):
            if batch_idx >= max_batches:
                break
                
            data, target = data.to(device), target.to(device)
            output = model(data)
            _, predicted = torch.max(output.data, 1)
            total += target.size(0)
            correct += (predicted == target).sum().item()
    
    accuracy = 100 * correct / total
    print(f"ğŸ“Š {domain_name} ì •í™•ë„: {accuracy:.2f}% ({correct}/{total})")
    return accuracy

# 12. ê²°ê³¼ ì €ì¥ ë° ë¶„ì„
def save_comprehensive_results(results):
    """í¬ê´„ì ì¸ ê²°ê³¼ë¥¼ ì €ì¥í•©ë‹ˆë‹¤."""
    
    # JSON ê²°ê³¼ ì €ì¥
    with open('results/sda_u_comprehensive_results.json', 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    # ì„±ëŠ¥ íˆìŠ¤í† ë¦¬ ì €ì¥
    with open('results/performance_history.json', 'w', encoding='utf-8') as f:
        json.dump(performance_history, f, indent=2, ensure_ascii=False)
    
    print("ğŸ’¾ í¬ê´„ì ì¸ ê²°ê³¼ ì €ì¥ ì™„ë£Œ")

# 13. í†µí•©ëœ ë°ì´í„° ë¡œë”© í•¨ìˆ˜
def load_data(args):
    """í†µí•©ëœ ë°ì´í„° ë¡œë”© í•¨ìˆ˜ (Office-Home, Office-31, CIFAR ë“± ì§€ì›)"""
    
    print(f"ğŸ“¦ ë°ì´í„° ë¡œë”© ì¤‘...")
    print(f"ğŸ¯ ë°ì´í„°ì…‹: {args.dataset}")
    
    if args.dataset == 'Office31':
        from office31_loader import Office31Loader
        loader = Office31Loader()
        
        print(f"ğŸ“¤ ì†ŒìŠ¤ ë„ë©”ì¸: {args.source_domain}")
        print(f"ğŸ“¥ íƒ€ê²Ÿ ë„ë©”ì¸: {args.target_domain}")
        
        # ë„ë©”ì¸ë³„ ë°ì´í„° ë¡œë“œ
        source_train_dataset, source_test_dataset = loader.load_domain_data(args.source_domain)
        target_train_dataset, target_test_dataset = loader.load_domain_data(args.target_domain)
        
        # ë°ì´í„° ë¡œë” ìƒì„±
        source_train_loader = DataLoader(source_train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=2)
        source_test_loader = DataLoader(source_test_dataset, batch_size=args.batch_size, shuffle=False, num_workers=2)
        target_train_loader = DataLoader(target_train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=2)
        target_test_loader = DataLoader(target_test_dataset, batch_size=args.batch_size, shuffle=False, num_workers=2)
        
        num_classes = 31  # Office-31 í´ë˜ìŠ¤ ìˆ˜
        
    elif args.dataset == 'OfficeHome':
        from officehome_loader import OfficeHomeLoader
        loader = OfficeHomeLoader()
        
        print(f"ğŸ“¤ ì†ŒìŠ¤ ë„ë©”ì¸: {args.source_domain}")
        print(f"ğŸ“¥ íƒ€ê²Ÿ ë„ë©”ì¸: {args.target_domain}")
        
        # ë„ë©”ì¸ë³„ ë°ì´í„° ë¡œë“œ
        source_train_dataset, source_test_dataset = loader.load_domain_data(args.source_domain)
        target_train_dataset, target_test_dataset = loader.load_domain_data(args.target_domain)
        
        # ë°ì´í„° ë¡œë” ìƒì„±
        source_train_loader = DataLoader(source_train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=2)
        source_test_loader = DataLoader(source_test_dataset, batch_size=args.batch_size, shuffle=False, num_workers=2)
        target_train_loader = DataLoader(target_train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=2)
        target_test_loader = DataLoader(target_test_dataset, batch_size=args.batch_size, shuffle=False, num_workers=2)
        
        num_classes = 65  # Office-Home í´ë˜ìŠ¤ ìˆ˜
        
    else:
        # ê¸°íƒ€ ë°ì´í„°ì…‹ (SVHN, MNIST, CIFAR ë“±)
        from dataset_manager import DatasetManager
        manager = DatasetManager()
        
        # source_domainê³¼ target_domainì„ ë°ì´í„°ì…‹ ì´ë¦„ìœ¼ë¡œ ì‚¬ìš©
        source_dataset_name = args.source_domain
        target_dataset_name = args.target_domain
        
        print(f"ğŸ“¤ ì†ŒìŠ¤ ë°ì´í„°ì…‹: {source_dataset_name}")
        print(f"ğŸ“¥ íƒ€ê²Ÿ ë°ì´í„°ì…‹: {target_dataset_name}")
        
        # ë°ì´í„°ì…‹ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        source_info = manager.get_dataset_info(source_dataset_name)
        target_info = manager.get_dataset_info(target_dataset_name)
        
        print(f"ğŸ“Š ì†ŒìŠ¤ í´ë˜ìŠ¤: {source_info['num_classes']}ê°œ")
        print(f"ğŸ“Š íƒ€ê²Ÿ í´ë˜ìŠ¤: {target_info['num_classes']}ê°œ")
        
        # ğŸš¨ CIFAR íŠ¹ë³„ ì²˜ë¦¬: í´ë˜ìŠ¤ ìˆ˜ ë¶ˆì¼ì¹˜ í•´ê²°
        if (source_dataset_name == 'CIFAR10' and target_dataset_name == 'CIFAR100') or \
           (source_dataset_name == 'CIFAR100' and target_dataset_name == 'CIFAR10'):
            
            print("ğŸ”§ CIFAR10â†”CIFAR100 í´ë˜ìŠ¤ ìˆ˜ ë¶ˆì¼ì¹˜ ê°ì§€ - íŠ¹ë³„ ì²˜ë¦¬ ëª¨ë“œ")
            num_classes = 10  # CIFAR10 ê¸°ì¤€ìœ¼ë¡œ í†µì¼
            print(f"âœ… í´ë˜ìŠ¤ ìˆ˜ë¥¼ CIFAR10 ê¸°ì¤€(10ê°œ)ìœ¼ë¡œ í†µì¼")
            print(f"âš ï¸ CIFAR100 ë¼ë²¨ì€ 0-9 ë²”ìœ„ë¡œ ìë™ í´ë¦¬í•‘ë©ë‹ˆë‹¤")
            
        else:
            # ì¼ë°˜ì ì¸ ê²½ìš°: ë” ì‘ì€ í´ë˜ìŠ¤ ìˆ˜ ì‚¬ìš©
            num_classes = min(source_info['num_classes'], target_info['num_classes'])
            print(f"ğŸ”§ í†µì¼ëœ í´ë˜ìŠ¤ ìˆ˜: {num_classes}ê°œ")
        
        # ë°ì´í„°ì…‹ ë¡œë“œ
        source_train_loader, source_test_loader = manager.load_dataset(source_dataset_name, args.batch_size)
        target_train_loader, target_test_loader = manager.load_dataset(target_dataset_name, args.batch_size)
    
    print(f"âœ… ë°ì´í„° ë¡œë”© ì™„ë£Œ! (í´ë˜ìŠ¤ ìˆ˜: {num_classes})")
    return source_train_loader, source_test_loader, target_train_loader, target_test_loader, num_classes

# 14. ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    parser = argparse.ArgumentParser(description='SDA-U (Selective Domain Adaptation with Unlearning)')
    
    # ë°ì´í„°ì…‹ ì„¤ì •
    parser.add_argument('--dataset', type=str, default='Office31', 
                       choices=['Office31', 'OfficeHome', 'SVHN', 'MNIST', 'FashionMNIST', 'CIFAR10', 'CIFAR100', 'STL10'],
                       help='ë°ì´í„°ì…‹ ì„ íƒ')
    parser.add_argument('--source_domain', type=str, required=True,
                       help='ì†ŒìŠ¤ ë„ë©”ì¸ (Office31: amazon/webcam/dslr, OfficeHome: art/clipart/product/real_world)')
    parser.add_argument('--target_domain', type=str, required=True, 
                       help='íƒ€ê²Ÿ ë„ë©”ì¸ (Office31: amazon/webcam/dslr, OfficeHome: art/clipart/product/real_world)')
    
    # í›ˆë ¨ íŒŒë¼ë¯¸í„°
    parser.add_argument('--batch_size', type=int, default=32, help='ë°°ì¹˜ í¬ê¸°')
    parser.add_argument('--num_epochs', type=int, default=20, help='í›ˆë ¨ ì—í¬í¬')
    parser.add_argument('--learning_rate', type=float, default=2e-4, help='í•™ìŠµë¥ ')
    parser.add_argument('--architecture', type=str, default='resnet50', 
                       choices=['resnet18', 'resnet50', 'simple_cnn'], help='ëª¨ë¸ ì•„í‚¤í…ì²˜')
    
    # SDA-U íŒŒë¼ë¯¸í„°
    parser.add_argument('--influence_samples', type=int, default=500, help='ì˜í–¥ë„ ê³„ì‚° ìƒ˜í”Œ ìˆ˜')
    parser.add_argument('--target_samples', type=int, default=800, help='íƒ€ê²Ÿ ìƒ˜í”Œ ìˆ˜')
    parser.add_argument('--unlearn_steps', type=int, default=8, help='ì–¸ëŸ¬ë‹ ìŠ¤í… ìˆ˜')
    parser.add_argument('--lambda_u', type=float, default=0.6, help='í•˜ì´ë¸Œë¦¬ë“œ ìŠ¤ì½”ì–´ë§ Î»_u')
    parser.add_argument('--beta', type=float, default=0.1, help='í•˜ì´ë¸Œë¦¬ë“œ ìŠ¤ì½”ì–´ë§ Î²')
    parser.add_argument('--adaptation_epochs', type=int, default=10, help='ì ì‘ í›ˆë ¨ ì—í¬í¬')
    
    # ì €ì¥ ê²½ë¡œ íŒŒë¼ë¯¸í„°
    parser.add_argument('--model_save_dir', type=str, default='models', help='ëª¨ë¸ ì €ì¥ ë””ë ‰í† ë¦¬')
    parser.add_argument('--results_file', type=str, default='results/sda_u_comprehensive_results.json', help='ê²°ê³¼ íŒŒì¼ ê²½ë¡œ')
    
    args = parser.parse_args()

    # GPU ì„¤ì • ë° ìµœì í™”
    try:
        from gpu_config import setup_gpu_optimizations, get_gpu_info
        print("ğŸš€ A100 ìµœì í™” í™œì„±í™”!")
        setup_gpu_optimizations()
        gpu_info = get_gpu_info()
        print(f"ğŸš€ ì„¤ì • ë¡œë“œ ì™„ë£Œ: {gpu_info['name']}")
    except ImportError:
        print("âš ï¸ GPU ìµœì í™” ì„¤ì • íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ì„¤ì • ì‚¬ìš©")
        
    global device
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ”§ ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")

    # ë°ì´í„° ë¡œë”©
    source_train_loader, source_test_loader, target_train_loader, target_test_loader, num_classes = load_data(args)
    
    # ëª¨ë¸ ìƒì„±
    model = create_model(args.architecture, num_classes).to(device)
    
    print(f"ğŸ¤– ëª¨ë¸ ìƒì„± ì™„ë£Œ: {args.architecture} (í´ë˜ìŠ¤: {num_classes})")
    print(f"ğŸ“Š íŒŒë¼ë¯¸í„° ìˆ˜: {sum(p.numel() for p in model.parameters()):,}")

    # ì „ì²´ SDA-U ì‹¤í–‰
    print("\n" + "="*80)
    print("ğŸš€ SDA-U í”„ë ˆì„ì›Œí¬ ì‹¤í–‰!")
    print("="*80)
    
    try:
        # 1. ì†ŒìŠ¤ ë„ë©”ì¸ ì‚¬ì „ í›ˆë ¨
        print("ğŸ“š 1ë‹¨ê³„: ì†ŒìŠ¤ ë„ë©”ì¸ ì‚¬ì „ í›ˆë ¨")
        train_model_with_evaluation(model, source_train_loader, source_test_loader, num_epochs=args.num_epochs, 
                                  source_domain=args.source_domain, target_domain=args.target_domain)
        
        # 2. íƒ€ê²Ÿ ë„ë©”ì¸ í‰ê°€ (ì‚¬ì „ í›ˆë ¨ í›„)
        print("\nğŸ“Š 2ë‹¨ê³„: íƒ€ê²Ÿ ë„ë©”ì¸ ì´ˆê¸° í‰ê°€")
        initial_target_acc = evaluate_model(model, target_test_loader, domain_name="íƒ€ê²Ÿ(ì´ˆê¸°)")
        
        # 3. ì˜¬ë°”ë¥¸ íƒ€ê²Ÿ ì„œë¸Œì…‹ íë ˆì´ì…˜ (ìˆœì„œ ìˆ˜ì •)
        print("\nğŸ¯ 3ë‹¨ê³„: íƒ€ê²Ÿ ì„œë¸Œì…‹ íë ˆì´ì…˜ (ì˜¬ë°”ë¥¸ ìˆœì„œ)")
        
        # 3-1. í° íƒ€ê²Ÿ í›„ë³´êµ° ì¤€ë¹„
        print("ğŸ“¦ 3-1ë‹¨ê³„: íƒ€ê²Ÿ í›„ë³´êµ° ì¤€ë¹„")
        candidate_batches = []
        target_batch_iter = iter(target_train_loader)
        
        # í›„ë³´êµ° í¬ê¸° ê³„ì‚° (target_samplesì˜ 2-3ë°°ë¡œ ì¶©ë¶„í•œ í›„ë³´êµ° í™•ë³´)
        candidate_samples_needed = min(args.target_samples * 3, 1200)  # ìµœëŒ€ 1200ê°œ
        num_batches_needed = (candidate_samples_needed + args.batch_size - 1) // args.batch_size
        
        print(f"ğŸ¯ ëª©í‘œ í›„ë³´êµ°: {candidate_samples_needed}ê°œ ({num_batches_needed}ê°œ ë°°ì¹˜)")
        
        for batch_idx in range(num_batches_needed):
            try:
                batch = next(target_batch_iter)
                candidate_batches.append(batch)
            except StopIteration:
                print(f"âš ï¸ ë°ì´í„° ë¶€ì¡±: {batch_idx}ê°œ ë°°ì¹˜ë§Œ ìˆ˜ì§‘ë¨")
                break
        
        total_candidates = sum(len(batch[0]) for batch in candidate_batches)
        print(f"âœ… í›„ë³´êµ° ì¤€ë¹„ ì™„ë£Œ: {len(candidate_batches)}ê°œ ë°°ì¹˜, {total_candidates}ê°œ ìƒ˜í”Œ")
        
        # 3-2. ì „ì²´ í›„ë³´êµ°ì— ëŒ€í•´ ì˜í–¥ë„ ê³„ì‚°
        print("ğŸ” 3-2ë‹¨ê³„: í›„ë³´êµ° ì˜í–¥ë„ ê³„ì‚°")
        all_influence_scores = []
        all_target_samples = []
        
        for batch_idx, batch in enumerate(candidate_batches):
            print(f"  ë°°ì¹˜ {batch_idx+1}/{len(candidate_batches)} ì˜í–¥ë„ ê³„ì‚° ì¤‘...")
            
            influence_scores = compute_influence_scores_enhanced(
                model, source_train_loader, batch, num_samples=args.influence_samples)
            
            all_influence_scores.extend(influence_scores)
            
            # ìƒ˜í”Œ ì €ì¥ (ì´í›„ íë ˆì´ì…˜ì— ì‚¬ìš©)
            for i in range(len(batch[0])):
                all_target_samples.append((batch[0][i], batch[1][i]))
        
        print(f"âœ… ì „ì²´ ì˜í–¥ë„ ê³„ì‚° ì™„ë£Œ: {len(all_influence_scores)}ê°œ ìƒ˜í”Œ")
        
        # 3-3. í•˜ì´ë¸Œë¦¬ë“œ ìŠ¤ì½”ì–´ë§
        print("ğŸ”¢ 3-3ë‹¨ê³„: í•˜ì´ë¸Œë¦¬ë“œ ìŠ¤ì½”ì–´ë§")
        hybrid_scores = compute_hybrid_scores(
            model, all_target_samples, all_influence_scores, 
            lambda_u=args.lambda_u, beta=args.beta)
        
        print(f"âœ… í•˜ì´ë¸Œë¦¬ë“œ ìŠ¤ì½”ì–´ ê³„ì‚° ì™„ë£Œ: {len(hybrid_scores)}ê°œ")
        
        # 3-4. ìµœì¢… íƒ€ê²Ÿ ì„œë¸Œì…‹ íë ˆì´ì…˜ (ìƒìœ„ ì ìˆ˜ ê¸°ì¤€ ì„ íƒ)
        print("ğŸ¯ 3-4ë‹¨ê³„: ìµœì¢… íƒ€ê²Ÿ ì„œë¸Œì…‹ íë ˆì´ì…˜")
        
        # íë ˆì´ì…˜í•  ìµœì¢… ì„œë¸Œì…‹ í¬ê¸°
        final_subset_size = min(args.target_samples, len(hybrid_scores))
        
        # í•˜ì´ë¸Œë¦¬ë“œ ìŠ¤ì½”ì–´ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ (ë†’ì€ ì ìˆ˜ ìˆœ)
        sorted_indices = sorted(range(len(hybrid_scores)), 
                              key=lambda i: hybrid_scores[i], reverse=True)
        
        # ìƒìœ„ ì ìˆ˜ ìƒ˜í”Œë“¤ë¡œ ìµœì¢… ì„œë¸Œì…‹ êµ¬ì„±
        curated_indices = sorted_indices[:final_subset_size]
        curated_samples = [all_target_samples[i] for i in curated_indices]
        curated_scores = [hybrid_scores[i] for i in curated_indices]
        
        print(f"âœ… ìµœì¢… íƒ€ê²Ÿ ì„œë¸Œì…‹ íë ˆì´ì…˜ ì™„ë£Œ: {len(curated_samples)}ê°œ ì„ ë³„")
        print(f"ğŸ“Š íë ˆì´ì…˜ ë¹„ìœ¨: {len(curated_samples)}/{len(all_target_samples)} ({len(curated_samples)/len(all_target_samples)*100:.1f}%)")
        print(f"ğŸ“ˆ ì„ ë³„ëœ ìƒ˜í”Œ ìŠ¤ì½”ì–´ ë²”ìœ„: {min(curated_scores):.4f} ~ {max(curated_scores):.4f}")
        
        # ê¸°ì¡´ ë³€ìˆ˜ë“¤ì„ íë ˆì´ì…˜ëœ ê²°ê³¼ë¡œ ëŒ€ì²´ (ì´í›„ ì½”ë“œ í˜¸í™˜ì„±)
        target_batch = (
            torch.stack([sample[0] for sample in curated_samples]),
            torch.stack([sample[1] for sample in curated_samples])
        )
        influence_scores = [all_influence_scores[i] for i in curated_indices]
        hybrid_scores = curated_scores
        
        # 4. ìœ í•´ ìƒ˜í”Œ ì‹ë³„ ë° ì–¸ëŸ¬ë‹
        print("\nğŸ”„ 4ë‹¨ê³„: DOS ì–¸ëŸ¬ë‹ ìˆ˜í–‰")
        
        # ìƒìœ„ ì ìˆ˜ ìƒ˜í”Œë“¤ì„ ìœ í•´ ìƒ˜í”Œë¡œ ê°„ì£¼
        num_harmful = min(50, len(hybrid_scores) // 4)  # ìƒìœ„ 25% ë˜ëŠ” ìµœëŒ€ 50ê°œ
        sorted_indices = sorted(range(len(hybrid_scores)), key=lambda i: hybrid_scores[i], reverse=True)
        harmful_indices = sorted_indices[:num_harmful]
        
        harmful_data = []
        for idx in harmful_indices:
            if idx < len(target_batch[0]):
                harmful_data.append((target_batch[0][idx], target_batch[1][idx]))
        
        print(f"ğŸ¯ ìœ í•´ ìƒ˜í”Œ {len(harmful_data)}ê°œ ì‹ë³„")
        
        # DOS ì–¸ëŸ¬ë‹ ìˆ˜í–‰
        unlearn_losses = perform_dos_unlearning(
            model, harmful_data, [hybrid_scores[i] for i in harmful_indices], 
            num_steps=args.unlearn_steps)
        
        # 5. íë ˆì´ì…˜ëœ íƒ€ê²Ÿ ìƒ˜í”Œë¡œ ì¬í•™ìŠµ (Self-training)
        print("\nğŸ“ 5ë‹¨ê³„: íƒ€ê²Ÿ ë„ë©”ì¸ ì¬í•™ìŠµ (Self-training)")
        retrain_with_curated_target_samples(model, curated_samples, args.adaptation_epochs)
        
        # 6. ìµœì¢… í‰ê°€
        print("\nğŸ“ˆ 6ë‹¨ê³„: ìµœì¢… ì„±ëŠ¥ í‰ê°€")
        final_source_acc = evaluate_model(model, source_test_loader, domain_name="ì†ŒìŠ¤(ìµœì¢…)")
        final_target_acc = evaluate_model(model, target_test_loader, domain_name="íƒ€ê²Ÿ(ìµœì¢…)")
        
        # 7. ê²°ê³¼ ì €ì¥
        print("\nğŸ’¾ 7ë‹¨ê³„: ê²°ê³¼ ì €ì¥")
        results = {
            'experiment_info': {
                'dataset': args.dataset,
                'source_domain': args.source_domain,
                'target_domain': args.target_domain,
                'architecture': args.architecture,
                'num_classes': num_classes
            },
            'performance': {
                'initial_target_accuracy': initial_target_acc,
                'final_source_accuracy': final_source_acc,
                'final_target_accuracy': final_target_acc,
                'accuracy_improvement': final_target_acc - initial_target_acc
            },
            'sda_u_metrics': {
                'influence_samples': len(influence_scores),
                'harmful_samples': len(harmful_data),
                'unlearn_steps': args.unlearn_steps,
                'unlearn_losses': unlearn_losses
            },
            'hyperparameters': {
                'batch_size': args.batch_size,
                'learning_rate': args.learning_rate,
                'lambda_u': args.lambda_u,
                'beta': args.beta
            }
        }
        
        # ê²°ê³¼ ì €ì¥ (ì§€ì •ëœ ê²½ë¡œ ì‚¬ìš©)
        results_dir = os.path.dirname(args.results_file)
        os.makedirs(results_dir, exist_ok=True)
        
        # ê²°ê³¼ë¥¼ ì§€ì •ëœ íŒŒì¼ì— ì €ì¥
        with open(args.results_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        print(f"ğŸ’¾ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {args.results_file}")
        
        # ëª¨ë¸ ì €ì¥ (ì§€ì •ëœ ê²½ë¡œ ì‚¬ìš©)
        model_dir = os.path.dirname(args.model_save_dir)
        os.makedirs(model_dir, exist_ok=True)
        model_filename = f"{args.source_domain}2{args.target_domain}_sda_u_model.pt"
        model_path = os.path.join(args.model_save_dir, model_filename)
        
        save_dict = {
            'model_state_dict': model.state_dict(),
            'results': results,
            'args': vars(args)
        }
        torch.save(save_dict, model_path)
        print(f"ğŸ¤– ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {model_path}")
        
        print("\nğŸ‰ SDA-U ì‹¤í—˜ ì™„ë£Œ!")
        print(f"ğŸ“Š íƒ€ê²Ÿ ì •í™•ë„ í–¥ìƒ: {initial_target_acc:.2f}% â†’ {final_target_acc:.2f}% (+{final_target_acc-initial_target_acc:.2f}%)")
        
    except Exception as e:
        print(f"\nâŒ ì‹¤í—˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        import traceback
        traceback.print_exc()
        return 1
    
    return 0

if __name__ == "__main__":
    main() 