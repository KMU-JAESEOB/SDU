# dataset_manager.py - ë‹¤ì–‘í•œ ë°ì´í„°ì…‹ ì§€ì›

"""
ğŸ¯ SDA-Uë¥¼ ìœ„í•œ ë‹¤ì–‘í•œ ë°ì´í„°ì…‹ ì§€ì›
- ì†ŒìŠ¤/íƒ€ê²Ÿ ë„ë©”ì¸ ì¡°í•© ì œê³µ
- ìë™ ë‹¤ìš´ë¡œë“œ ë° ì „ì²˜ë¦¬
- ë„ë©”ì¸ ì ì‘ì— ì í•©í•œ ë°ì´í„°ì…‹ ìŒ ì¶”ì²œ
"""

import torch
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader, Subset
import numpy as np
import os

class DatasetManager:
    """ë‹¤ì–‘í•œ ë°ì´í„°ì…‹ì„ í†µí•© ê´€ë¦¬í•˜ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.data_dir = './data'
        os.makedirs(self.data_dir, exist_ok=True)
        
        # ğŸ¯ ì§€ì› ë°ì´í„°ì…‹ ëª©ë¡ í™•ì¥
        self.supported_datasets = {
            # ê¸°ì¡´ ë°ì´í„°ì…‹
            'MNIST', 'SVHN', 'CIFAR10', 'CIFAR100',
            # ìƒˆë¡œ ì¶”ê°€ëœ ë°ì´í„°ì…‹
            'FashionMNIST', 'STL10', 
            # Office-31 ë°ì´í„°ì…‹
            'Office31_Amazon', 'Office31_Webcam', 'Office31_DSLR',
            # Office-Home ë°ì´í„°ì…‹
            'OfficeHome'
        }
        
        print(f"ğŸ“¦ ë°ì´í„°ì…‹ ë§¤ë‹ˆì € ì´ˆê¸°í™” ì™„ë£Œ!")
        print(f"ğŸ¯ ì§€ì› ë°ì´í„°ì…‹: {', '.join(sorted(self.supported_datasets))}")
    
    def get_dataset_info(self, dataset_name):
        """ë°ì´í„°ì…‹ ì •ë³´ ë°˜í™˜"""
        
        dataset_info = {
            'MNIST': {
                'num_classes': 10,
                'image_size': 28,
                'channels': 1,
                'mean': [0.1307],
                'std': [0.3081],
                'description': 'ì†ê¸€ì”¨ ìˆ«ì (0-9)'
            },
            'FashionMNIST': {
                'num_classes': 10,
                'image_size': 28,
                'channels': 1,
                'mean': [0.2860],
                'std': [0.3530],
                'description': 'íŒ¨ì…˜ ì•„ì´í…œ (10ê°œ ì¹´í…Œê³ ë¦¬)'
            },
            'SVHN': {
                'num_classes': 10,
                'image_size': 32,
                'channels': 3,
                'mean': [0.4377, 0.4438, 0.4728],
                'std': [0.1980, 0.2010, 0.1970],
                'description': 'ìì—°í™˜ê²½ ìˆ«ì (Street View)'
            },
            'CIFAR10': {
                'num_classes': 10,
                'image_size': 32,
                'channels': 3,
                'mean': [0.4914, 0.4822, 0.4465],
                'std': [0.2023, 0.1994, 0.2010],
                'description': 'ìì—° ì´ë¯¸ì§€ (10ê°œ í´ë˜ìŠ¤)'
            },
            'CIFAR100': {
                'num_classes': 100,
                'image_size': 32,
                'channels': 3,
                'mean': [0.5071, 0.4867, 0.4408],
                'std': [0.2675, 0.2565, 0.2761],
                'description': 'ìì—° ì´ë¯¸ì§€ (100ê°œ í´ë˜ìŠ¤)'
            },
            'STL10': {
                'num_classes': 10,
                'image_size': 96,
                'channels': 3,
                'mean': [0.4467, 0.4398, 0.4066],
                'std': [0.2603, 0.2566, 0.2713],
                'description': 'ê³ í•´ìƒë„ ìì—° ì´ë¯¸ì§€'
            },
            'Office31_Amazon': {
                'num_classes': 31,
                'image_size': 224,
                'channels': 3,
                'mean': [0.485, 0.456, 0.406],
                'std': [0.229, 0.224, 0.225],
                'description': 'Amazon ì œí’ˆ ì´ë¯¸ì§€'
            },
            'Office31_Webcam': {
                'num_classes': 31,
                'image_size': 224,
                'channels': 3,
                'mean': [0.485, 0.456, 0.406],
                'std': [0.229, 0.224, 0.225],
                'description': 'Webcam ì œí’ˆ ì´ë¯¸ì§€'
            },
            'Office31_DSLR': {
                'num_classes': 31,
                'image_size': 224,
                'channels': 3,
                'mean': [0.485, 0.456, 0.406],
                'std': [0.229, 0.224, 0.225],
                'description': 'DSLR ì œí’ˆ ì´ë¯¸ì§€'
            },
            'OfficeHome': {
                'num_classes': 65,
                'image_size': 224,
                'channels': 3,
                'mean': [0.485, 0.456, 0.406],
                'std': [0.229, 0.224, 0.225],
                'description': 'Office-Home ë°ì´í„°ì…‹'
            }
        }
        
        return dataset_info.get(dataset_name, {
            'num_classes': 10,
            'image_size': 32,
            'channels': 3,
            'mean': [0.5, 0.5, 0.5],
            'std': [0.5, 0.5, 0.5],
            'description': 'ì•Œ ìˆ˜ ì—†ëŠ” ë°ì´í„°ì…‹'
        })
    
    def get_transforms(self, dataset_name, is_training=True):
        """ë°ì´í„°ì…‹ë³„ ìµœì í™”ëœ ë³€í™˜ ë°˜í™˜"""
        
        info = self.get_dataset_info(dataset_name)
        image_size = info['image_size']
        channels = info['channels']
        mean = info['mean']
        std = info['std']
        
        if is_training:
            if channels == 1:  # í‘ë°± ì´ë¯¸ì§€ (MNIST, Fashion-MNIST)
                transform = transforms.Compose([
                    transforms.Resize((image_size, image_size)),
                    transforms.RandomRotation(10),
                    transforms.ToTensor(),
                    transforms.Normalize(mean=mean, std=std)
                ])
            else:  # ì»¬ëŸ¬ ì´ë¯¸ì§€
                if image_size <= 32:  # ì‘ì€ ì´ë¯¸ì§€ (CIFAR-10, SVHN)
                    transform = transforms.Compose([
                        transforms.Resize((image_size, image_size)),
                        transforms.RandomHorizontalFlip(0.5),
                        transforms.RandomRotation(10),
                        transforms.ColorJitter(brightness=0.2, contrast=0.2),
                        transforms.ToTensor(),
                        transforms.Normalize(mean=mean, std=std)
                    ])
                else:  # í° ì´ë¯¸ì§€ (STL-10, Office-31)
                    transform = transforms.Compose([
                        transforms.Resize((image_size, image_size)),
                        transforms.RandomHorizontalFlip(0.5),
                        transforms.RandomRotation(15),
                        transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3),
                        transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),
                        transforms.ToTensor(),
                        transforms.Normalize(mean=mean, std=std)
                    ])
        else:
            # í…ŒìŠ¤íŠ¸ìš© ë³€í™˜ (ë°ì´í„° ì¦ê°• ì—†ìŒ)
            transform = transforms.Compose([
                transforms.Resize((image_size, image_size)),
                transforms.ToTensor(),
                transforms.Normalize(mean=mean, std=std)
            ])
        
        return transform
    
    def load_dataset(self, dataset_name, batch_size=32, shuffle=True, num_workers=2):
        """ë°ì´í„°ì…‹ ë¡œë“œ ë° DataLoader ë°˜í™˜"""
        
        print(f"ğŸ“¦ {dataset_name} ë°ì´í„°ì…‹ ë¡œë”© ì¤‘...")
        
        train_transform = self.get_transforms(dataset_name, is_training=True)
        test_transform = self.get_transforms(dataset_name, is_training=False)
        
        try:
            if dataset_name == 'MNIST':
                train_dataset = torchvision.datasets.MNIST(
                    root=self.data_dir, train=True, download=True, transform=train_transform)
                test_dataset = torchvision.datasets.MNIST(
                    root=self.data_dir, train=False, download=True, transform=test_transform)
                    
            elif dataset_name == 'FashionMNIST':
                train_dataset = torchvision.datasets.FashionMNIST(
                    root=self.data_dir, train=True, download=True, transform=train_transform)
                test_dataset = torchvision.datasets.FashionMNIST(
                    root=self.data_dir, train=False, download=True, transform=test_transform)
                    
            elif dataset_name == 'SVHN':
                train_dataset = torchvision.datasets.SVHN(
                    root=self.data_dir, split='train', download=True, transform=train_transform)
                test_dataset = torchvision.datasets.SVHN(
                    root=self.data_dir, split='test', download=True, transform=test_transform)
                    
            elif dataset_name == 'CIFAR10':
                train_dataset = torchvision.datasets.CIFAR10(
                    root=self.data_dir, train=True, download=True, transform=train_transform)
                test_dataset = torchvision.datasets.CIFAR10(
                    root=self.data_dir, train=False, download=True, transform=test_transform)
                    
            elif dataset_name == 'CIFAR100':
                train_dataset = torchvision.datasets.CIFAR100(
                    root=self.data_dir, train=True, download=True, transform=train_transform)
                test_dataset = torchvision.datasets.CIFAR100(
                    root=self.data_dir, train=False, download=True, transform=test_transform)
                    
            elif dataset_name == 'STL10':
                train_dataset = torchvision.datasets.STL10(
                    root=self.data_dir, split='train', download=True, transform=train_transform)
                test_dataset = torchvision.datasets.STL10(
                    root=self.data_dir, split='test', download=True, transform=test_transform)
                    
            elif dataset_name.startswith('Office31_'):
                # Office-31 ë°ì´í„°ì…‹ì€ ë³„ë„ ë¡œë” ì‚¬ìš©
                from office31_loader import Office31Loader
                office31_loader = Office31Loader()
                domain = dataset_name.split('_')[1].lower()
                train_dataset, test_dataset = office31_loader.load_domain_data(domain)
                
            elif dataset_name == 'OfficeHome':
                # Office-Home ë°ì´í„°ì…‹ì€ ë³„ë„ ë¡œë” ì‚¬ìš©
                from officehome_loader import OfficeHomeLoader
                officehome_loader = OfficeHomeLoader(root=self.data_dir)
                train_dataset, test_dataset = officehome_loader.load_domain_data(domain)
                
            else:
                raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ë°ì´í„°ì…‹: {dataset_name}")
            
            # DataLoader ìƒì„±
            train_loader = DataLoader(
                train_dataset, batch_size=batch_size, shuffle=shuffle, 
                num_workers=num_workers, pin_memory=True)
            test_loader = DataLoader(
                test_dataset, batch_size=batch_size, shuffle=False, 
                num_workers=num_workers, pin_memory=True)
            
            info = self.get_dataset_info(dataset_name)
            print(f"âœ… {dataset_name} ë¡œë”© ì™„ë£Œ!")
            print(f"   ğŸ“Š í›ˆë ¨ ìƒ˜í”Œ: {len(train_dataset):,}ê°œ")
            print(f"   ğŸ“Š í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ: {len(test_dataset):,}ê°œ")
            print(f"   ğŸ¯ í´ë˜ìŠ¤ ìˆ˜: {info['num_classes']}ê°œ")
            print(f"   ğŸ“ ì´ë¯¸ì§€ í¬ê¸°: {info['image_size']}x{info['image_size']}x{info['channels']}")
            
            return train_loader, test_loader
            
        except Exception as e:
            print(f"âŒ {dataset_name} ë¡œë”© ì‹¤íŒ¨: {str(e)}")
            raise
    
    def get_subset_loader(self, dataset_name, subset_size=500, batch_size=32, is_training=True):
        """ë°ì´í„°ì…‹ì˜ ë¶€ë¶„ì§‘í•© ë¡œë” ë°˜í™˜"""
        
        print(f"ğŸ¯ {dataset_name} ë¶€ë¶„ì§‘í•© ìƒì„± ì¤‘ (í¬ê¸°: {subset_size})")
        
        transform = self.get_transforms(dataset_name, is_training=is_training)
        
        try:
            if dataset_name == 'MNIST':
                dataset = torchvision.datasets.MNIST(
                    root=self.data_dir, train=is_training, download=True, transform=transform)
            elif dataset_name == 'FashionMNIST':
                dataset = torchvision.datasets.FashionMNIST(
                    root=self.data_dir, train=is_training, download=True, transform=transform)
            elif dataset_name == 'SVHN':
                split = 'train' if is_training else 'test'
                dataset = torchvision.datasets.SVHN(
                    root=self.data_dir, split=split, download=True, transform=transform)
            elif dataset_name == 'CIFAR10':
                dataset = torchvision.datasets.CIFAR10(
                    root=self.data_dir, train=is_training, download=True, transform=transform)
            elif dataset_name == 'CIFAR100':
                dataset = torchvision.datasets.CIFAR100(
                    root=self.data_dir, train=is_training, download=True, transform=transform)
            elif dataset_name == 'STL10':
                split = 'train' if is_training else 'test'
                dataset = torchvision.datasets.STL10(
                    root=self.data_dir, split=split, download=True, transform=transform)
            elif dataset_name.startswith('Office31_'):
                from office31_loader import Office31Loader
                office31_loader = Office31Loader()
                domain = dataset_name.split('_')[1].lower()
                train_dataset, test_dataset = office31_loader.load_domain_data(domain)
                dataset = train_dataset if is_training else test_dataset
            elif dataset_name == 'OfficeHome':
                from officehome_loader import OfficeHomeLoader
                officehome_loader = OfficeHomeLoader(root=self.data_dir)
                domain = dataset_name.split('_')[1].lower()
                train_dataset, test_dataset = officehome_loader.load_domain_data(domain)
                dataset = train_dataset if is_training else test_dataset
            else:
                raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ë°ì´í„°ì…‹: {dataset_name}")
            
            # ëœë¤ ë¶€ë¶„ì§‘í•© ìƒì„±
            total_size = len(dataset)
            subset_size = min(subset_size, total_size)
            indices = np.random.choice(total_size, subset_size, replace=False)
            subset_dataset = Subset(dataset, indices)
            
            # DataLoader ìƒì„±
            subset_loader = DataLoader(
                subset_dataset, batch_size=batch_size, shuffle=True, 
                num_workers=2, pin_memory=True)
            
            print(f"âœ… {dataset_name} ë¶€ë¶„ì§‘í•© ìƒì„± ì™„ë£Œ! ({subset_size}/{total_size} ìƒ˜í”Œ)")
            
            return subset_loader
            
        except Exception as e:
            print(f"âŒ {dataset_name} ë¶€ë¶„ì§‘í•© ìƒì„± ì‹¤íŒ¨: {str(e)}")
            raise
    
    def print_dataset_summary(self):
        """ì§€ì› ë°ì´í„°ì…‹ ìš”ì•½ ì¶œë ¥"""
        
        print("\n" + "="*80)
        print("ğŸ“¦ ì§€ì› ë°ì´í„°ì…‹ ìš”ì•½")
        print("="*80)
        
        categories = {
            'ğŸ”¢ ìˆ«ì ì¸ì‹': ['MNIST', 'FashionMNIST', 'SVHN'],
            'ğŸ–¼ï¸ ìì—° ì´ë¯¸ì§€': ['CIFAR10', 'CIFAR100', 'STL10'],
            'ğŸ¢ Office-31': ['Office31_Amazon', 'Office31_Webcam', 'Office31_DSLR'],
            'ğŸ  Office-Home': ['OfficeHome']
        }
        
        for category, datasets in categories.items():
            print(f"\n{category}:")
            print("-" * 60)
            for dataset_name in datasets:
                if dataset_name in self.supported_datasets:
                    info = self.get_dataset_info(dataset_name)
                    print(f"  ğŸ“Š {dataset_name:<20} | "
                          f"í´ë˜ìŠ¤: {info['num_classes']:>3}ê°œ | "
                          f"í¬ê¸°: {info['image_size']:>3}x{info['image_size']} | "
                          f"ì±„ë„: {info['channels']} | "
                          f"{info['description']}")
        
        print("\n" + "="*80)

    def get_unified_transforms(self, source_dataset, target_dataset, is_training=True):
        """ë„ë©”ì¸ ì ì‘ì„ ìœ„í•œ í†µì¼ëœ ë³€í™˜ ë°˜í™˜ (ì±„ë„ ë° í¬ê¸° í†µì¼)"""
        
        source_info = self.get_dataset_info(source_dataset)
        target_info = self.get_dataset_info(target_dataset)
        
        # ğŸ”§ í†µì¼ëœ ì„¤ì • ê²°ì •
        # ë” í° ì´ë¯¸ì§€ í¬ê¸° ì‚¬ìš©
        unified_size = max(source_info['image_size'], target_info['image_size'])
        # ë” ë§ì€ ì±„ë„ ìˆ˜ ì‚¬ìš© (3ì±„ë„ë¡œ í†µì¼)
        unified_channels = max(source_info['channels'], target_info['channels'])
        
        print(f"ğŸ”§ ë„ë©”ì¸ ì ì‘ ë³€í™˜ ì„¤ì •:")
        print(f"   ğŸ“ í†µì¼ ì´ë¯¸ì§€ í¬ê¸°: {unified_size}x{unified_size}")
        print(f"   ğŸ¨ í†µì¼ ì±„ë„ ìˆ˜: {unified_channels}")
        
        # ImageNet ì •ê·œí™” ì‚¬ìš© (ì‚¬ì „ í›ˆë ¨ ëª¨ë¸ í˜¸í™˜)
        if unified_channels == 3:
            mean = [0.485, 0.456, 0.406]
            std = [0.229, 0.224, 0.225]
        else:
            mean = [0.5]
            std = [0.5]
        
        if is_training:
            transform = transforms.Compose([
                transforms.Resize((unified_size, unified_size)),
                transforms.RandomHorizontalFlip(0.5),
                transforms.RandomRotation(10),
                transforms.ToTensor(),
                # ğŸ¯ í•µì‹¬: 1ì±„ë„â†’3ì±„ë„ ë³€í™˜
                transforms.Lambda(lambda x: x.repeat(3, 1, 1) if x.size(0) == 1 else x),
                transforms.Normalize(mean=mean, std=std)
            ])
        else:
            transform = transforms.Compose([
                transforms.Resize((unified_size, unified_size)),
                transforms.ToTensor(),
                # ğŸ¯ í•µì‹¬: 1ì±„ë„â†’3ì±„ë„ ë³€í™˜
                transforms.Lambda(lambda x: x.repeat(3, 1, 1) if x.size(0) == 1 else x),
                transforms.Normalize(mean=mean, std=std)
            ])
        
        return transform
    
    def load_dataset_for_domain_adaptation(self, source_dataset, target_dataset, batch_size=32, shuffle=True, num_workers=2):
        """ë„ë©”ì¸ ì ì‘ìš© ë°ì´í„°ì…‹ ë¡œë” (ì±„ë„ ë° í¬ê¸° í†µì¼)"""
        
        print(f"ğŸ”„ ë„ë©”ì¸ ì ì‘ìš© ë°ì´í„°ì…‹ ë¡œë”©: {source_dataset} â†’ {target_dataset}")
        
        # í†µì¼ëœ ë³€í™˜ ìƒì„±
        train_transform = self.get_unified_transforms(source_dataset, target_dataset, is_training=True)
        test_transform = self.get_unified_transforms(source_dataset, target_dataset, is_training=False)
        
        try:
            # ì†ŒìŠ¤ ë°ì´í„°ì…‹ ë¡œë“œ
            source_train_dataset, source_test_dataset = self._load_single_dataset(
                source_dataset, train_transform, test_transform)
            
            # íƒ€ê²Ÿ ë°ì´í„°ì…‹ ë¡œë“œ
            target_train_dataset, target_test_dataset = self._load_single_dataset(
                target_dataset, train_transform, test_transform)
            
            # DataLoader ìƒì„±
            source_train_loader = DataLoader(
                source_train_dataset, batch_size=batch_size, shuffle=shuffle, 
                num_workers=num_workers, pin_memory=True)
            source_test_loader = DataLoader(
                source_test_dataset, batch_size=batch_size, shuffle=False, 
                num_workers=num_workers, pin_memory=True)
            target_train_loader = DataLoader(
                target_train_dataset, batch_size=batch_size, shuffle=shuffle, 
                num_workers=num_workers, pin_memory=True)
            target_test_loader = DataLoader(
                target_test_dataset, batch_size=batch_size, shuffle=False, 
                num_workers=num_workers, pin_memory=True)
            
            print(f"âœ… ë„ë©”ì¸ ì ì‘ìš© ë°ì´í„° ë¡œë” ìƒì„± ì™„ë£Œ!")
            print(f"   ğŸ“Š ì†ŒìŠ¤ í›ˆë ¨: {len(source_train_dataset):,}ê°œ")
            print(f"   ğŸ“Š íƒ€ê²Ÿ í›ˆë ¨: {len(target_train_dataset):,}ê°œ")
            
            return source_train_loader, target_train_loader, source_test_loader, target_test_loader
            
        except Exception as e:
            print(f"âŒ ë„ë©”ì¸ ì ì‘ìš© ë°ì´í„° ë¡œë” ìƒì„± ì‹¤íŒ¨: {str(e)}")
            raise
    
    def _load_single_dataset(self, dataset_name, train_transform, test_transform):
        """ë‹¨ì¼ ë°ì´í„°ì…‹ ë¡œë“œ (ë‚´ë¶€ í•¨ìˆ˜)"""
        
        if dataset_name == 'MNIST':
            train_dataset = torchvision.datasets.MNIST(
                root=self.data_dir, train=True, download=True, transform=train_transform)
            test_dataset = torchvision.datasets.MNIST(
                root=self.data_dir, train=False, download=True, transform=test_transform)
                
        elif dataset_name == 'FashionMNIST':
            train_dataset = torchvision.datasets.FashionMNIST(
                root=self.data_dir, train=True, download=True, transform=train_transform)
            test_dataset = torchvision.datasets.FashionMNIST(
                root=self.data_dir, train=False, download=True, transform=test_transform)
                
        elif dataset_name == 'SVHN':
            train_dataset = torchvision.datasets.SVHN(
                root=self.data_dir, split='train', download=True, transform=train_transform)
            test_dataset = torchvision.datasets.SVHN(
                root=self.data_dir, split='test', download=True, transform=test_transform)
                
        elif dataset_name == 'CIFAR10':
            train_dataset = torchvision.datasets.CIFAR10(
                root=self.data_dir, train=True, download=True, transform=train_transform)
            test_dataset = torchvision.datasets.CIFAR10(
                root=self.data_dir, train=False, download=True, transform=test_transform)
                
        elif dataset_name == 'CIFAR100':
            train_dataset = torchvision.datasets.CIFAR100(
                root=self.data_dir, train=True, download=True, transform=train_transform)
            test_dataset = torchvision.datasets.CIFAR100(
                root=self.data_dir, train=False, download=True, transform=test_transform)
                
        elif dataset_name == 'STL10':
            train_dataset = torchvision.datasets.STL10(
                root=self.data_dir, split='train', download=True, transform=train_transform)
            test_dataset = torchvision.datasets.STL10(
                root=self.data_dir, split='test', download=True, transform=test_transform)
                
        elif dataset_name.startswith('Office31_'):
            from office31_loader import Office31Loader
            office31_loader = Office31Loader()
            domain = dataset_name.split('_')[1].lower()
            train_dataset, test_dataset = office31_loader.load_domain_data(domain)
            
        elif dataset_name == 'OfficeHome':
            from officehome_loader import OfficeHomeLoader
            officehome_loader = OfficeHomeLoader(root=self.data_dir)
            domain = dataset_name.split('_')[1].lower()
            train_dataset, test_dataset = officehome_loader.load_domain_data(domain)
            
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ë°ì´í„°ì…‹: {dataset_name}")
        
        return train_dataset, test_dataset
    
    def get_subset_loader_unified(self, source_dataset, target_dataset, dataset_name, subset_size=500, batch_size=32, is_training=True):
        """ë„ë©”ì¸ ì ì‘ìš© í†µì¼ëœ ë¶€ë¶„ì§‘í•© ë¡œë” ë°˜í™˜"""
        
        print(f"ğŸ¯ {dataset_name} í†µì¼ ë¶€ë¶„ì§‘í•© ìƒì„± ì¤‘ (í¬ê¸°: {subset_size})")
        
        # í†µì¼ëœ ë³€í™˜ ì‚¬ìš©
        transform = self.get_unified_transforms(source_dataset, target_dataset, is_training=is_training)
        
        try:
            dataset, _ = self._load_single_dataset(dataset_name, transform, transform)
            if not is_training:
                _, dataset = self._load_single_dataset(dataset_name, transform, transform)
            
            # ëœë¤ ë¶€ë¶„ì§‘í•© ìƒì„±
            total_size = len(dataset)
            subset_size = min(subset_size, total_size)
            indices = np.random.choice(total_size, subset_size, replace=False)
            subset_dataset = Subset(dataset, indices)
            
            # DataLoader ìƒì„±
            subset_loader = DataLoader(
                subset_dataset, batch_size=batch_size, shuffle=True, 
                num_workers=2, pin_memory=True)
            
            print(f"âœ… {dataset_name} í†µì¼ ë¶€ë¶„ì§‘í•© ìƒì„± ì™„ë£Œ! ({subset_size}/{total_size} ìƒ˜í”Œ)")
            
            return subset_loader
            
        except Exception as e:
            print(f"âŒ {dataset_name} í†µì¼ ë¶€ë¶„ì§‘í•© ìƒì„± ì‹¤íŒ¨: {str(e)}")
            raise

# ì „ì—­ ë°ì´í„°ì…‹ ë§¤ë‹ˆì € ì¸ìŠ¤í„´ìŠ¤
dataset_manager = DatasetManager()

def get_data_loaders(source_dataset, target_dataset, batch_size=32):
    """ì†ŒìŠ¤ì™€ íƒ€ê²Ÿ ë°ì´í„°ì…‹ ë¡œë” ë°˜í™˜ (í˜¸í™˜ì„± í•¨ìˆ˜)"""
    
    print(f"ğŸ”„ ë°ì´í„° ë¡œë” ìƒì„±: {source_dataset} â†’ {target_dataset}")
    
    source_train_loader, source_test_loader = dataset_manager.load_dataset(
        source_dataset, batch_size=batch_size)
    target_train_loader, target_test_loader = dataset_manager.load_dataset(
        target_dataset, batch_size=batch_size)
    
    return source_train_loader, source_test_loader, target_train_loader, target_test_loader

def load_officehome_domains(source_domain, target_domain, data_root='./data', batch_size=32, num_workers=2):
    """Office-Home ë„ë©”ì¸ ë¡œë”©"""
    
    print(f"ğŸ  Office-Home ë„ë©”ì¸ ë¡œë”©: {source_domain} â†’ {target_domain}")
    
    try:
        from officehome_loader import OfficeHomeLoader
        
        # Office-Home ë¡œë” ìƒì„±
        loader = OfficeHomeLoader(root=data_root)
        
        # ë„ë©”ì¸ ì´ë¦„ ë§¤í•‘
        domain_mapping = {
            'Art': 'art',
            'Clipart': 'clipart', 
            'Product': 'product',
            'Real World': 'real_world',
            'RealWorld': 'real_world'  # ê³µë°± ì—†ëŠ” ë²„ì „ë„ ì§€ì›
        }
        
        # ì†ŒìŠ¤/íƒ€ê²Ÿ ë„ë©”ì¸ í‚¤ ë³€í™˜
        source_key = domain_mapping.get(source_domain, source_domain.lower())
        target_key = domain_mapping.get(target_domain, target_domain.lower())
        
        # ì†ŒìŠ¤ ë„ë©”ì¸ ë°ì´í„° ë¡œë“œ
        print(f"ğŸ“¤ ì†ŒìŠ¤ ë„ë©”ì¸ ë¡œë”©: {source_domain} ({source_key})")
        source_train_loader, source_test_loader = loader.load_domain_data(
            source_key, batch_size=batch_size, shuffle=True
        )
        
        # íƒ€ê²Ÿ ë„ë©”ì¸ ë°ì´í„° ë¡œë“œ
        print(f"ğŸ“¥ íƒ€ê²Ÿ ë„ë©”ì¸ ë¡œë”©: {target_domain} ({target_key})")
        target_train_loader, target_test_loader = loader.load_domain_data(
            target_key, batch_size=batch_size, shuffle=True
        )
        
        # í´ë˜ìŠ¤ ìˆ˜ í™•ì¸ (Office-Homeì€ 65ê°œ í´ë˜ìŠ¤)
        num_classes = 65
        
        print(f"âœ… Office-Home ë„ë©”ì¸ ë¡œë”© ì™„ë£Œ!")
        print(f"   ğŸ“Š í´ë˜ìŠ¤ ìˆ˜: {num_classes}")
        print(f"   ğŸ“¦ ì†ŒìŠ¤ ë°°ì¹˜: {len(source_train_loader)}ê°œ")
        print(f"   ğŸ¯ íƒ€ê²Ÿ ë°°ì¹˜: {len(target_train_loader)}ê°œ")
        
        return {
            'source_train_loader': source_train_loader,
            'source_test_loader': source_test_loader,
            'target_train_loader': target_train_loader,
            'target_test_loader': target_test_loader,
            'num_classes': num_classes,
            'source_domain': source_domain,
            'target_domain': target_domain
        }
        
    except ImportError as e:
        print(f"âŒ Office-Home ë¡œë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
        print("ğŸ’¡ officehome_loader.py íŒŒì¼ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        raise
    
    except Exception as e:
        print(f"âŒ Office-Home ë„ë©”ì¸ ë¡œë”© ì‹¤íŒ¨: {e}")
        raise

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    manager = DatasetManager()
    manager.print_dataset_summary()
    
    # ìƒ˜í”Œ ë°ì´í„° ë¡œë”© í…ŒìŠ¤íŠ¸
    print("\nğŸ§ª ìƒ˜í”Œ ë°ì´í„° ë¡œë”© í…ŒìŠ¤íŠ¸:")
    try:
        train_loader, test_loader = manager.load_dataset('SVHN', batch_size=64)
        print("âœ… SVHN ë°ì´í„°ì…‹ ë¡œë”© ì„±ê³µ!")
        
        subset_loader = manager.get_subset_loader('MNIST', subset_size=100)
        print("âœ… MNIST ë¶€ë¶„ì§‘í•© ìƒì„± ì„±ê³µ!")
        
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}") 