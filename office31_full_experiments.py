#!/usr/bin/env python3
# office31_full_experiments.py - Office31 ì¢…í•© ì‹¤í—˜ ë„êµ¬

"""
ğŸ¯ Office31 ì¢…í•© ì‹¤í—˜ ë„êµ¬

ê¸°ëŠ¥:
1. ëª¨ë“  ì†ŒìŠ¤-íƒ€ê²Ÿ ë„ë©”ì¸ ì¡°í•© ì‹¤í—˜ 
2. íƒ€ê²Ÿ ìƒ˜í”Œ ìˆ˜ë³„ ì„±ëŠ¥ ë¹„êµ (100~600ê°œ)
3. Î»_u, Î² í•˜ì´í¼íŒŒë¼ë¯¸í„° ê·¸ë¦¬ë“œ ì„œì¹˜
4. ì–¸ëŸ¬ë‹ íš¨ê³¼ ì‹œê°í™”

ì‚¬ìš©ë²•:
python office31_full_experiments.py --experiment [domain_pairs|sample_sizes|hyperparams|all]
"""

import os
import sys
import json
import copy
import time
import argparse
from pathlib import Path
from typing import Dict, List, Tuple, Any
import subprocess

# ì‹œê°í™”ë¥¼ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
try:
    import matplotlib.pyplot as plt
    import numpy as np
    import pandas as pd
    import seaborn as sns
    import matplotlib.font_manager as fm
    
    # í•œê¸€ í°íŠ¸ ì„¤ì • (ì—ëŸ¬ ë°©ì§€)
    try:
        # ì‚¬ìš© ê°€ëŠ¥í•œ í•œê¸€ í°íŠ¸ ì°¾ê¸°
        font_list = [f.name for f in fm.fontManager.ttflist]
        korean_fonts = ['Malgun Gothic', 'AppleGothic', 'NanumGothic', 'Arial Unicode MS']
        available_korean_font = None
        
        for font in korean_fonts:
            if font in font_list:
                available_korean_font = font
                break
        
        if available_korean_font:
            plt.rcParams['font.family'] = [available_korean_font, 'DejaVu Sans']
        else:
            # í•œê¸€ í°íŠ¸ê°€ ì—†ìœ¼ë©´ ì˜ì–´ë§Œ ì‚¬ìš©
            plt.rcParams['font.family'] = ['DejaVu Sans']
            
        plt.rcParams['axes.unicode_minus'] = False
    except:
        # í°íŠ¸ ì„¤ì • ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’ ìœ ì§€
        pass
    
    VISUALIZATION_AVAILABLE = True
except ImportError:
    VISUALIZATION_AVAILABLE = False
    print("âš ï¸ ì‹œê°í™” ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ìŒ. pip install matplotlib seaborn pandas ì‹¤í–‰í•˜ì„¸ìš”.")

# main.pyì˜ SDAUAlgorithm í´ë˜ìŠ¤ import
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
try:
    from main import SDAUAlgorithm, SDAUConfig
    MAIN_AVAILABLE = True
except ImportError:
    MAIN_AVAILABLE = False
    print("âš ï¸ main.py import ì‹¤íŒ¨. í™˜ê²½ ì„¤ì • ë¬¸ì œì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

class Office31ExperimentRunner:
    """Office31 ì¢…í•© ì‹¤í—˜ ì‹¤í–‰ê¸°"""
    
    def __init__(self, base_config_path: str = "config.json"):
        self.base_config_path = base_config_path
        self.results_dir = Path("results/office31_comprehensive")
        self.results_dir.mkdir(parents=True, exist_ok=True)
        
        # Office31 ë„ë©”ì¸ ì •ì˜
        self.domains = ["Amazon", "Webcam", "DSLR"]
        self.domain_pairs = [(s, t) for s in self.domains for t in self.domains if s != t]
        
        # ì‹¤í—˜ ì„¤ì •
        self.sample_sizes = [100, 200, 300, 400, 500, 600]
        self.lambda_values = [0.5, 0.6, 0.7, 0.8, 0.9]
        self.beta_values = [0.1, 0.2, 0.3, 0.4, 0.5]
        
        print(f"ğŸ¯ Office31 ì¢…í•© ì‹¤í—˜ ì´ˆê¸°í™” ì™„ë£Œ")
        print(f"ğŸ“‚ ê²°ê³¼ ì €ì¥: {self.results_dir}")
    
    def create_experiment_config(self, modifications: Dict[str, Any]) -> str:
        """ì‹¤í—˜ìš© ì„¤ì • íŒŒì¼ ìƒì„±"""
        # ê¸°ë³¸ ì„¤ì • ë¡œë”©
        with open(self.base_config_path, 'r', encoding='utf-8') as f:
            config = json.load(f)
        
        # ìˆ˜ì •ì‚¬í•­ ì ìš©
        for key_path, value in modifications.items():
            keys = key_path.split('.')
            current = config
            for key in keys[:-1]:
                current = current[key]
            current[keys[-1]] = value
        
        # ì„ì‹œ ì„¤ì • íŒŒì¼ ìƒì„±
        temp_config_path = self.results_dir / f"temp_config_{int(time.time())}.json"
        with open(temp_config_path, 'w', encoding='utf-8') as f:
            json.dump(config, f, indent=2, ensure_ascii=False)
        
        return str(temp_config_path)
    
    def run_single_experiment(self, source_domain: str, target_domain: str, 
                            config_path: str, experiment_name: str) -> Dict[str, Any]:
        """ë‹¨ì¼ ì‹¤í—˜ ì‹¤í–‰"""
        print(f"ğŸ”¬ ì‹¤í—˜ ì‹œì‘: {experiment_name} ({source_domain} â†’ {target_domain})")
        
        try:
            if MAIN_AVAILABLE:
                # ì§ì ‘ ì‹¤í–‰ ë°©ì‹
                sda_u = SDAUAlgorithm(config_path=config_path)
                results = sda_u.run_experiment("Office31", source_domain, target_domain)
                
                return {
                    'source_domain': source_domain,
                    'target_domain': target_domain,
                    'experiment_name': experiment_name,
                    'initial_target_acc': results.initial_target_acc,
                    'final_target_acc': results.final_target_acc,
                    'best_target_acc': results.best_target_acc,
                    'best_epoch': results.best_epoch,
                    'improvement': results.improvement,
                    'best_improvement': results.best_improvement,
                    'unlearning_count': results.unlearning_count,
                    'total_epochs': results.total_epochs,
                    'success': True,
                    'timestamp': time.time()
                }
            else:
                # subprocess ì‹¤í–‰ ë°©ì‹
                cmd = [
                    sys.executable, 'main.py',
                    '--dataset', 'Office31',
                    '--source_domain', source_domain,
                    '--target_domain', target_domain,
                    '--config', config_path,
                    '--results_file', f'{experiment_name}_results.json'
                ]
                
                result = subprocess.run(cmd, capture_output=True, text=True, timeout=3600)
                
                if result.returncode == 0:
                    print(f"âœ… ì„±ê³µ: {experiment_name}")
                    return {
                        'source_domain': source_domain,
                        'target_domain': target_domain,
                        'experiment_name': experiment_name,
                        'success': True,
                        'timestamp': time.time()
                    }
                else:
                    print(f"âŒ ì‹¤íŒ¨: {experiment_name}")
                    return {
                        'source_domain': source_domain,
                        'target_domain': target_domain,
                        'experiment_name': experiment_name,
                        'success': False,
                        'error': result.stderr,
                        'timestamp': time.time()
                    }
                    
        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜: {experiment_name} - {e}")
            return {
                'source_domain': source_domain,
                'target_domain': target_domain,
                'experiment_name': experiment_name,
                'success': False,
                'error': str(e),
                'timestamp': time.time()
            }
    
    def run_domain_pairs_experiment(self) -> Dict[str, Any]:
        """ëª¨ë“  ë„ë©”ì¸ ìŒ ì‹¤í—˜"""
        print(f"\nğŸ¯ ë„ë©”ì¸ ìŒ ì‹¤í—˜ ì‹œì‘ (ì´ {len(self.domain_pairs)}ê°œ)")
        
        results = []
        for i, (source, target) in enumerate(self.domain_pairs, 1):
            print(f"\nì§„í–‰ë¥ : {i}/{len(self.domain_pairs)}")
            
            # ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ ì‹¤í—˜
            config_path = self.create_experiment_config({})
            experiment_name = f"domain_pairs_{source}2{target}"
            
            result = self.run_single_experiment(source, target, config_path, experiment_name)
            results.append(result)
            
            # ì„ì‹œ ì„¤ì • íŒŒì¼ ì‚­ì œ
            os.remove(config_path)
        
        # ê²°ê³¼ ì €ì¥
        output_file = self.results_dir / "domain_pairs_results.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… ë„ë©”ì¸ ìŒ ì‹¤í—˜ ì™„ë£Œ! ê²°ê³¼: {output_file}")
        return {'results': results, 'output_file': str(output_file)}
    
    def run_sample_sizes_experiment(self) -> Dict[str, Any]:
        """íƒ€ê²Ÿ ìƒ˜í”Œ ìˆ˜ë³„ ì‹¤í—˜"""
        print(f"\nğŸ“Š íƒ€ê²Ÿ ìƒ˜í”Œ ìˆ˜ë³„ ì‹¤í—˜ ì‹œì‘")
        print(f"ìƒ˜í”Œ í¬ê¸°: {self.sample_sizes}")
        
        results = []
        # ëŒ€í‘œì ì¸ ë„ë©”ì¸ ìŒ ì„ íƒ (Amazon â†’ Webcam)
        source, target = "Amazon", "Webcam"
        
        for i, num_samples in enumerate(self.sample_sizes, 1):
            print(f"\nì§„í–‰ë¥ : {i}/{len(self.sample_sizes)} - ìƒ˜í”Œ {num_samples}ê°œ")
            
            # íƒ€ê²Ÿ ìƒ˜í”Œ ìˆ˜ ì„¤ì •
            config_path = self.create_experiment_config({
                'target_selection.num_samples': num_samples
            })
            
            experiment_name = f"sample_size_{num_samples}"
            result = self.run_single_experiment(source, target, config_path, experiment_name)
            result['num_samples'] = num_samples
            results.append(result)
            
            # ì„ì‹œ ì„¤ì • íŒŒì¼ ì‚­ì œ
            os.remove(config_path)
        
        # ê²°ê³¼ ì €ì¥
        output_file = self.results_dir / "sample_sizes_results.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… ìƒ˜í”Œ ìˆ˜ë³„ ì‹¤í—˜ ì™„ë£Œ! ê²°ê³¼: {output_file}")
        return {'results': results, 'output_file': str(output_file)}
    
    def run_hyperparameter_experiment(self) -> Dict[str, Any]:
        """í•˜ì´í¼íŒŒë¼ë¯¸í„° ê·¸ë¦¬ë“œ ì„œì¹˜"""
        print(f"\nğŸ” í•˜ì´í¼íŒŒë¼ë¯¸í„° ê·¸ë¦¬ë“œ ì„œì¹˜ ì‹œì‘")
        print(f"Î»_u ê°’: {self.lambda_values}")
        print(f"Î² ê°’: {self.beta_values}")
        print(f"ì´ ì¡°í•©: {len(self.lambda_values) * len(self.beta_values)}ê°œ")
        
        results = []
        # ëŒ€í‘œì ì¸ ë„ë©”ì¸ ìŒ ì„ íƒ (Amazon â†’ Webcam)
        source, target = "Amazon", "Webcam"
        
        total_combinations = len(self.lambda_values) * len(self.beta_values)
        current_combo = 0
        
        for lambda_u in self.lambda_values:
            for beta in self.beta_values:
                current_combo += 1
                print(f"\nì§„í–‰ë¥ : {current_combo}/{total_combinations} - Î»_u={lambda_u}, Î²={beta}")
                
                # í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •
                config_path = self.create_experiment_config({
                    'target_selection.lambda_utility': lambda_u,
                    'target_selection.beta_uncertainty': beta
                })
                
                experiment_name = f"hyperparam_lambda{lambda_u}_beta{beta}"
                result = self.run_single_experiment(source, target, config_path, experiment_name)
                result['lambda_utility'] = lambda_u
                result['beta_uncertainty'] = beta
                results.append(result)
                
                # ì„ì‹œ ì„¤ì • íŒŒì¼ ì‚­ì œ
                os.remove(config_path)
        
        # ê²°ê³¼ ì €ì¥
        output_file = self.results_dir / "hyperparameter_results.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… í•˜ì´í¼íŒŒë¼ë¯¸í„° ì‹¤í—˜ ì™„ë£Œ! ê²°ê³¼: {output_file}")
        return {'results': results, 'output_file': str(output_file)}
    
    def create_visualizations(self) -> None:
        """ì‹¤í—˜ ê²°ê³¼ ì‹œê°í™”"""
        if not VISUALIZATION_AVAILABLE:
            print("âš ï¸ ì‹œê°í™” ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—†ì–´ ê±´ë„ˆë›°ê¸°")
            return
        
        print(f"\nğŸ“ˆ ì‹œê°í™” ìƒì„± ì¤‘...")
        
        # ì‹œê°í™” ë””ë ‰í† ë¦¬ ìƒì„±
        viz_dir = self.results_dir / "visualizations"
        viz_dir.mkdir(exist_ok=True)
        
        # 1. ë„ë©”ì¸ ìŒ ê²°ê³¼ ì‹œê°í™”
        self._plot_domain_pairs_results(viz_dir)
        
        # 2. ìƒ˜í”Œ ìˆ˜ë³„ ê²°ê³¼ ì‹œê°í™”
        self._plot_sample_sizes_results(viz_dir)
        
        # 3. í•˜ì´í¼íŒŒë¼ë¯¸í„° íˆíŠ¸ë§µ
        self._plot_hyperparameter_heatmap(viz_dir)
        
        # 4. ì–¸ëŸ¬ë‹ íš¨ê³¼ ì‹œê°í™”
        self._plot_unlearning_effects(viz_dir)
        
        print(f"âœ… ì‹œê°í™” ì™„ë£Œ! ì €ì¥ ìœ„ì¹˜: {viz_dir}")
    
    def _plot_domain_pairs_results(self, viz_dir: Path) -> None:
        """ë„ë©”ì¸ ìŒ ê²°ê³¼ ì‹œê°í™”"""
        try:
            with open(self.results_dir / "domain_pairs_results.json", 'r') as f:
                results = json.load(f)
            
            # ì„±ê³µí•œ ì‹¤í—˜ë§Œ í•„í„°ë§
            successful_results = [r for r in results if r.get('success', False)]
            
            if not successful_results:
                print("âš ï¸ ì„±ê³µí•œ ë„ë©”ì¸ ìŒ ì‹¤í—˜ ê²°ê³¼ê°€ ì—†ìŒ")
                return
            
            # ë°ì´í„° ì¤€ë¹„
            domain_pairs = [f"{r['source_domain']} â†’ {r['target_domain']}" for r in successful_results]
            final_accs = [r.get('final_target_acc', 0) for r in successful_results]
            improvements = [r.get('improvement', 0) for r in successful_results]
            
            # ì‹œê°í™”
            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
            
            # ìµœì¢… ì„±ëŠ¥
            ax1.bar(range(len(domain_pairs)), final_accs, color='skyblue')
            ax1.set_title('Office31 Domain Pairs Final Performance')
            ax1.set_xlabel('Domain Pairs')
            ax1.set_ylabel('Target Accuracy (%)')
            ax1.set_xticks(range(len(domain_pairs)))
            ax1.set_xticklabels(domain_pairs, rotation=45)
            
            # ì„±ëŠ¥ ê°œì„ 
            colors = ['green' if imp > 0 else 'red' for imp in improvements]
            ax2.bar(range(len(domain_pairs)), improvements, color=colors)
            ax2.set_title('Office31 Domain Pairs Performance Improvement')
            ax2.set_xlabel('Domain Pairs')
            ax2.set_ylabel('Performance Improvement (%)')
            ax2.set_xticks(range(len(domain_pairs)))
            ax2.set_xticklabels(domain_pairs, rotation=45)
            ax2.axhline(y=0, color='black', linestyle='--', alpha=0.3)
            
            plt.tight_layout()
            plt.savefig(viz_dir / "domain_pairs_performance.png", dpi=300, bbox_inches='tight')
            plt.close()
            
        except Exception as e:
            print(f"âš ï¸ ë„ë©”ì¸ ìŒ ì‹œê°í™” ì‹¤íŒ¨: {e}")
    
    def _plot_sample_sizes_results(self, viz_dir: Path) -> None:
        """ìƒ˜í”Œ ìˆ˜ë³„ ê²°ê³¼ ì‹œê°í™”"""
        try:
            with open(self.results_dir / "sample_sizes_results.json", 'r') as f:
                results = json.load(f)
            
            successful_results = [r for r in results if r.get('success', False)]
            
            if not successful_results:
                print("âš ï¸ ì„±ê³µí•œ ìƒ˜í”Œ ìˆ˜ ì‹¤í—˜ ê²°ê³¼ê°€ ì—†ìŒ")
                return
            
            # ë°ì´í„° ì¤€ë¹„
            sample_sizes = [r['num_samples'] for r in successful_results]
            final_accs = [r.get('final_target_acc', 0) for r in successful_results]
            improvements = [r.get('improvement', 0) for r in successful_results]
            unlearning_counts = [r.get('unlearning_count', 0) for r in successful_results]
            
            # ì‹œê°í™”
            fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))
            
            # ìµœì¢… ì„±ëŠ¥ vs ìƒ˜í”Œ ìˆ˜
            ax1.plot(sample_sizes, final_accs, 'o-', color='blue', linewidth=2, markersize=8)
            ax1.set_title('Final Performance vs Target Sample Size')
            ax1.set_xlabel('Target Sample Size')
            ax1.set_ylabel('Target Accuracy (%)')
            ax1.grid(True, alpha=0.3)
            
            # ì„±ëŠ¥ ê°œì„  vs ìƒ˜í”Œ ìˆ˜
            ax2.plot(sample_sizes, improvements, 'o-', color='green', linewidth=2, markersize=8)
            ax2.set_title('Performance Improvement vs Target Sample Size')
            ax2.set_xlabel('Target Sample Size')
            ax2.set_ylabel('Performance Improvement (%)')
            ax2.axhline(y=0, color='red', linestyle='--', alpha=0.5)
            ax2.grid(True, alpha=0.3)
            
            # ì–¸ëŸ¬ë‹ íšŸìˆ˜ vs ìƒ˜í”Œ ìˆ˜
            ax3.bar(sample_sizes, unlearning_counts, color='orange', alpha=0.7)
            ax3.set_title('Unlearning Count vs Target Sample Size')
            ax3.set_xlabel('Target Sample Size')
            ax3.set_ylabel('Unlearning Count')
            ax3.grid(True, alpha=0.3)
            
            # íš¨ìœ¨ì„± (ê°œì„ /ìƒ˜í”Œìˆ˜)
            efficiency = [imp/size*100 for imp, size in zip(improvements, sample_sizes)]
            ax4.plot(sample_sizes, efficiency, 'o-', color='purple', linewidth=2, markersize=8)
            ax4.set_title('Sample Efficiency (Improvement/Size Ã— 100)')
            ax4.set_xlabel('Target Sample Size')
            ax4.set_ylabel('Efficiency')
            ax4.grid(True, alpha=0.3)
            
            plt.tight_layout()
            plt.savefig(viz_dir / "sample_sizes_analysis.png", dpi=300, bbox_inches='tight')
            plt.close()
            
        except Exception as e:
            print(f"âš ï¸ ìƒ˜í”Œ ìˆ˜ ì‹œê°í™” ì‹¤íŒ¨: {e}")
    
    def _plot_hyperparameter_heatmap(self, viz_dir: Path) -> None:
        """í•˜ì´í¼íŒŒë¼ë¯¸í„° íˆíŠ¸ë§µ"""
        try:
            with open(self.results_dir / "hyperparameter_results.json", 'r') as f:
                results = json.load(f)
            
            successful_results = [r for r in results if r.get('success', False)]
            
            if not successful_results:
                print("âš ï¸ ì„±ê³µí•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì‹¤í—˜ ê²°ê³¼ê°€ ì—†ìŒ")
                return
            
            # ë°ì´í„° ì¤€ë¹„
            df = pd.DataFrame(successful_results)
            pivot_final = df.pivot(index='beta_uncertainty', columns='lambda_utility', values='final_target_acc')
            pivot_improvement = df.pivot(index='beta_uncertainty', columns='lambda_utility', values='improvement')
            
            # ì‹œê°í™”
            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
            
            # ìµœì¢… ì„±ëŠ¥ íˆíŠ¸ë§µ
            sns.heatmap(pivot_final, annot=True, cmap='Blues', ax=ax1, fmt='.2f')
            ax1.set_title('Hyperparameter Final Performance (%)')
            ax1.set_xlabel('Î»_u (Utility Weight)')
            ax1.set_ylabel('Î² (Uncertainty Weight)')
            
            # ì„±ëŠ¥ ê°œì„  íˆíŠ¸ë§µ
            sns.heatmap(pivot_improvement, annot=True, cmap='RdYlGn', center=0, ax=ax2, fmt='.2f')
            ax2.set_title('Hyperparameter Performance Improvement (%)')
            ax2.set_xlabel('Î»_u (Utility Weight)')
            ax2.set_ylabel('Î² (Uncertainty Weight)')
            
            plt.tight_layout()
            plt.savefig(viz_dir / "hyperparameter_heatmap.png", dpi=300, bbox_inches='tight')
            plt.close()
            
        except Exception as e:
            print(f"âš ï¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì‹œê°í™” ì‹¤íŒ¨: {e}")
    
    def _plot_unlearning_effects(self, viz_dir: Path) -> None:
        """ì–¸ëŸ¬ë‹ íš¨ê³¼ ì‹œê°í™”"""
        try:
            # ëª¨ë“  ì‹¤í—˜ ê²°ê³¼ì—ì„œ ì–¸ëŸ¬ë‹ íš¨ê³¼ ë¶„ì„
            all_results = []
            
            for file_name in ["domain_pairs_results.json", "sample_sizes_results.json", "hyperparameter_results.json"]:
                file_path = self.results_dir / file_name
                if file_path.exists():
                    with open(file_path, 'r') as f:
                        results = json.load(f)
                        all_results.extend([r for r in results if r.get('success', False)])
            
            if not all_results:
                print("âš ï¸ ì–¸ëŸ¬ë‹ íš¨ê³¼ ë¶„ì„í•  ë°ì´í„° ì—†ìŒ")
                return
            
            # ë°ì´í„° ì¤€ë¹„
            unlearning_counts = [r.get('unlearning_count', 0) for r in all_results]
            improvements = [r.get('improvement', 0) for r in all_results]
            
            # ì–¸ëŸ¬ë‹ íšŸìˆ˜ë³„ ê·¸ë£¹í™”
            unlearning_groups = {}
            for count, improvement in zip(unlearning_counts, improvements):
                if count not in unlearning_groups:
                    unlearning_groups[count] = []
                unlearning_groups[count].append(improvement)
            
            # ì‹œê°í™”
            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
            
            # ì–¸ëŸ¬ë‹ íšŸìˆ˜ vs ì„±ëŠ¥ ê°œì„  ì‚°ì ë„
            ax1.scatter(unlearning_counts, improvements, alpha=0.6, s=50)
            ax1.set_title('Unlearning Count vs Performance Improvement')
            ax1.set_xlabel('Unlearning Count')
            ax1.set_ylabel('Performance Improvement (%)')
            ax1.axhline(y=0, color='red', linestyle='--', alpha=0.5)
            ax1.grid(True, alpha=0.3)
            
            # ì–¸ëŸ¬ë‹ íšŸìˆ˜ë³„ í‰ê·  ê°œì„ ëŸ‰
            counts = sorted(unlearning_groups.keys())
            avg_improvements = [np.mean(unlearning_groups[count]) for count in counts]
            
            ax2.bar(counts, avg_improvements, color='lightgreen', alpha=0.7)
            ax2.set_title('Average Performance Improvement by Unlearning Count')
            ax2.set_xlabel('Unlearning Count')
            ax2.set_ylabel('Average Performance Improvement (%)')
            ax2.axhline(y=0, color='red', linestyle='--', alpha=0.5)
            ax2.grid(True, alpha=0.3)
            
            plt.tight_layout()
            plt.savefig(viz_dir / "unlearning_effects.png", dpi=300, bbox_inches='tight')
            plt.close()
            
        except Exception as e:
            print(f"âš ï¸ ì–¸ëŸ¬ë‹ íš¨ê³¼ ì‹œê°í™” ì‹¤íŒ¨: {e}")
    
    def generate_summary_report(self) -> None:
        """ì¢…í•© ê²°ê³¼ ë¦¬í¬íŠ¸ ìƒì„±"""
        print(f"\nğŸ“‹ ì¢…í•© ê²°ê³¼ ë¦¬í¬íŠ¸ ìƒì„± ì¤‘...")
        
        report = []
        report.append("# Office31 SDA-U ì¢…í•© ì‹¤í—˜ ë¦¬í¬íŠ¸\n")
        report.append(f"ìƒì„± ì‹œê°„: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
        
        # ê° ì‹¤í—˜ë³„ ìš”ì•½
        for experiment_name, file_name in [
            ("ë„ë©”ì¸ ìŒ ì‹¤í—˜", "domain_pairs_results.json"),
            ("ìƒ˜í”Œ ìˆ˜ ì‹¤í—˜", "sample_sizes_results.json"),
            ("í•˜ì´í¼íŒŒë¼ë¯¸í„° ì‹¤í—˜", "hyperparameter_results.json")
        ]:
            file_path = self.results_dir / file_name
            if file_path.exists():
                with open(file_path, 'r') as f:
                    results = json.load(f)
                
                successful = [r for r in results if r.get('success', False)]
                total = len(results)
                success_rate = len(successful) / total * 100 if total > 0 else 0
                
                report.append(f"## {experiment_name}\n")
                report.append(f"- ì´ ì‹¤í—˜: {total}ê°œ\n")
                report.append(f"- ì„±ê³µ: {len(successful)}ê°œ ({success_rate:.1f}%)\n")
                
                if successful:
                    avg_improvement = np.mean([r.get('improvement', 0) for r in successful])
                    max_improvement = max([r.get('improvement', 0) for r in successful])
                    avg_unlearning = np.mean([r.get('unlearning_count', 0) for r in successful])
                    
                    report.append(f"- í‰ê·  ì„±ëŠ¥ ê°œì„ : {avg_improvement:.2f}%\n")
                    report.append(f"- ìµœëŒ€ ì„±ëŠ¥ ê°œì„ : {max_improvement:.2f}%\n")
                    report.append(f"- í‰ê·  ì–¸ëŸ¬ë‹ íšŸìˆ˜: {avg_unlearning:.1f}íšŒ\n")
                
                report.append("\n")
        
        # ë¦¬í¬íŠ¸ ì €ì¥
        report_file = self.results_dir / "comprehensive_report.md"
        with open(report_file, 'w', encoding='utf-8') as f:
            f.writelines(report)
        
        print(f"âœ… ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ: {report_file}")

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description='Office31 ì¢…í•© ì‹¤í—˜ ë„êµ¬')
    parser.add_argument('--experiment', type=str, 
                       choices=['domain_pairs', 'sample_sizes', 'hyperparams', 'all'],
                       default='all', help='ì‹¤í–‰í•  ì‹¤í—˜ ì¢…ë¥˜')
    parser.add_argument('--config', type=str, default='config.json', help='ê¸°ë³¸ ì„¤ì • íŒŒì¼')
    parser.add_argument('--visualize', action='store_true', help='ì‹œê°í™” ìƒì„±')
    
    args = parser.parse_args()
    
    print("ğŸ¯ Office31 ì¢…í•© ì‹¤í—˜ ë„êµ¬ ì‹œì‘!")
    print("="*60)
    
    # ì‹¤í—˜ ëŸ¬ë„ˆ ì´ˆê¸°í™”
    runner = Office31ExperimentRunner(args.config)
    
    # ì‹¤í—˜ ì‹¤í–‰
    if args.experiment == 'domain_pairs' or args.experiment == 'all':
        runner.run_domain_pairs_experiment()
    
    if args.experiment == 'sample_sizes' or args.experiment == 'all':
        runner.run_sample_sizes_experiment()
    
    if args.experiment == 'hyperparams' or args.experiment == 'all':
        runner.run_hyperparameter_experiment()
    
    # ì‹œê°í™” ë° ë¦¬í¬íŠ¸ ìƒì„±
    if args.visualize or args.experiment == 'all':
        runner.create_visualizations()
    
    runner.generate_summary_report()
    
    print("\nğŸ‰ Office31 ì¢…í•© ì‹¤í—˜ ì™„ë£Œ!")
    print(f"ğŸ“‚ ê²°ê³¼ í™•ì¸: {runner.results_dir}")

if __name__ == "__main__":
    main() 