# extract_officehome_performance.py
"""
ğŸ  Office-Home ì‹¤í—˜ ê²°ê³¼ ì„±ëŠ¥ ì¶”ì¶œ ë° ë¶„ì„ ë„êµ¬
- ê°œë³„ ì‹¤í—˜ ê²°ê³¼ ë¶„ì„
- ì „ì²´ ì‹¤í—˜ ì„±ëŠ¥ ìš”ì•½
- ë„ë©”ì¸ë³„ ì„±ëŠ¥ ë¹„êµ
- ì‹œê°í™” ë° í†µê³„ ë¶„ì„
"""

import json
import os
import sys
from pathlib import Path
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from datetime import datetime

class OfficeHomePerformanceExtractor:
    """Office-Home ì„±ëŠ¥ ì¶”ì¶œê¸°"""
    
    def __init__(self, results_dir='results/officehome'):
        """
        Args:
            results_dir (str): ê²°ê³¼ ë””ë ‰í† ë¦¬ ê²½ë¡œ
        """
        self.results_dir = Path(results_dir)
        self.domains = ['art', 'clipart', 'product', 'real_world']
        self.domain_names = {
            'art': 'Art',
            'clipart': 'Clipart',
            'product': 'Product', 
            'real_world': 'Real World'
        }
        
        print(f"ğŸ  Office-Home ì„±ëŠ¥ ì¶”ì¶œê¸° ì´ˆê¸°í™”")
        print(f"ğŸ“ ê²°ê³¼ ë””ë ‰í† ë¦¬: {self.results_dir}")
        
        if not self.results_dir.exists():
            print(f"âš ï¸ ê²°ê³¼ ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {self.results_dir}")
            self.results_dir.mkdir(parents=True, exist_ok=True)
    
    def load_single_experiment_result(self, source_domain, target_domain):
        """ë‹¨ì¼ ì‹¤í—˜ ê²°ê³¼ ë¡œë“œ"""
        
        source_name = self.domain_names.get(source_domain, source_domain)
        target_name = self.domain_names.get(target_domain, target_domain)
        experiment_name = f"{source_name}2{target_name}"
        
        result_file = self.results_dir / f"{experiment_name}_results.json"
        
        if not result_file.exists():
            print(f"âŒ ê²°ê³¼ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {result_file}")
            return None
        
        try:
            with open(result_file, 'r', encoding='utf-8') as f:
                result = json.load(f)
            
            print(f"âœ… {experiment_name} ê²°ê³¼ ë¡œë“œ ì„±ê³µ")
            return result
            
        except Exception as e:
            print(f"âŒ {experiment_name} ê²°ê³¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None
    
    def load_all_experiment_results(self):
        """ëª¨ë“  ì‹¤í—˜ ê²°ê³¼ ë¡œë“œ"""
        
        print(f"ğŸ“Š ëª¨ë“  Office-Home ì‹¤í—˜ ê²°ê³¼ ë¡œë“œ ì¤‘...")
        
        all_results = {}
        loaded_count = 0
        
        # ê°œë³„ ê²°ê³¼ íŒŒì¼ ë¡œë“œ
        for source in self.domains:
            for target in self.domains:
                if source != target:
                    result = self.load_single_experiment_result(source, target)
                    if result:
                        experiment_name = f"{self.domain_names[source]}2{self.domain_names[target]}"
                        all_results[experiment_name] = result
                        loaded_count += 1
        
        # ì „ì²´ ê²°ê³¼ íŒŒì¼ í™•ì¸
        full_results_file = self.results_dir / 'officehome_full_results.json'
        if full_results_file.exists():
            try:
                with open(full_results_file, 'r', encoding='utf-8') as f:
                    full_results = json.load(f)
                
                # ì „ì²´ ê²°ê³¼ì—ì„œ ê°œë³„ ì‹¤í—˜ ì¶”ì¶œ
                if 'experiments' in full_results:
                    for exp_name, exp_result in full_results['experiments'].items():
                        if exp_name not in all_results:
                            all_results[exp_name] = exp_result
                            loaded_count += 1
                
                print(f"âœ… ì „ì²´ ê²°ê³¼ íŒŒì¼ë„ ë¡œë“œ: {full_results_file}")
                
            except Exception as e:
                print(f"âš ï¸ ì „ì²´ ê²°ê³¼ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        print(f"ğŸ“ˆ ì´ {loaded_count}ê°œ ì‹¤í—˜ ê²°ê³¼ ë¡œë“œ ì™„ë£Œ")
        return all_results
    
    def extract_performance_metrics(self, results):
        """ì„±ëŠ¥ ì§€í‘œ ì¶”ì¶œ"""
        
        print(f"ğŸ“Š ì„±ëŠ¥ ì§€í‘œ ì¶”ì¶œ ì¤‘...")
        
        performance_data = []
        
        for experiment_name, result in results.items():
            # ì‹¤í—˜ ì´ë¦„ íŒŒì‹±
            if '2' in experiment_name:
                source_domain, target_domain = experiment_name.split('2')
            else:
                continue
            
            # ì˜¤ë¥˜ í™•ì¸
            if 'error' in result:
                performance_data.append({
                    'experiment': experiment_name,
                    'source_domain': source_domain,
                    'target_domain': target_domain,
                    'final_target_accuracy': 0.0,
                    'overall_target_accuracy': 0.0,
                    'source_accuracy': 0.0,
                    'unlearned_samples': 0,
                    'selected_target_samples': 0,
                    'training_time_minutes': result.get('elapsed_time', 0) / 60,
                    'status': 'FAILED',
                    'error': result.get('error', 'Unknown error')
                })
                continue
            
            # ì„±ê³µí•œ ì‹¤í—˜ ë°ì´í„° ì¶”ì¶œ
            performance_data.append({
                'experiment': experiment_name,
                'source_domain': source_domain,
                'target_domain': target_domain,
                'final_target_accuracy': result.get('final_target_accuracy', 0.0),
                'overall_target_accuracy': result.get('overall_target_accuracy', 0.0),
                'source_accuracy': result.get('source_accuracy', 0.0),
                'unlearned_samples': result.get('unlearned_samples', 0),
                'selected_target_samples': result.get('selected_target_samples', 0),
                'training_time_minutes': result.get('elapsed_time', 0) / 60,
                'status': 'SUCCESS',
                'error': None
            })
        
        print(f"âœ… {len(performance_data)}ê°œ ì‹¤í—˜ ì„±ëŠ¥ ì§€í‘œ ì¶”ì¶œ ì™„ë£Œ")
        return performance_data
    
    def create_performance_summary_table(self, performance_data):
        """ì„±ëŠ¥ ìš”ì•½ í…Œì´ë¸” ìƒì„±"""
        
        print(f"\nğŸ“Š Office-Home ì„±ëŠ¥ ìš”ì•½ í…Œì´ë¸”")
        print("="*120)
        
        # í—¤ë”
        header = f"{'ì‹¤í—˜':<20} {'ì†ŒìŠ¤':<12} {'íƒ€ê²Ÿ':<12} {'ìµœì¢…ì •í™•ë„':<12} {'ì „ì²´ì •í™•ë„':<12} {'ì†ŒìŠ¤ì •í™•ë„':<12} {'ì–¸ëŸ¬ë‹':<8} {'íƒ€ê²Ÿì„ íƒ':<8} {'ì‹œê°„(ë¶„)':<8} {'ìƒíƒœ':<8}"
        print(header)
        print("-"*120)
        
        # ì„±ê³µí•œ ì‹¤í—˜ë“¤
        successful_experiments = [exp for exp in performance_data if exp['status'] == 'SUCCESS']
        failed_experiments = [exp for exp in performance_data if exp['status'] == 'FAILED']
        
        # ì„±ê³µí•œ ì‹¤í—˜ ì¶œë ¥
        for exp in successful_experiments:
            row = f"{exp['experiment']:<20} {exp['source_domain']:<12} {exp['target_domain']:<12} {exp['final_target_accuracy']:<12.2f} {exp['overall_target_accuracy']:<12.2f} {exp['source_accuracy']:<12.2f} {exp['unlearned_samples']:<8} {exp['selected_target_samples']:<8} {exp['training_time_minutes']:<8.1f} {exp['status']:<8}"
            print(row)
        
        # ì‹¤íŒ¨í•œ ì‹¤í—˜ ì¶œë ¥
        if failed_experiments:
            print("\nâŒ ì‹¤íŒ¨í•œ ì‹¤í—˜ë“¤:")
            for exp in failed_experiments:
                row = f"{exp['experiment']:<20} {exp['source_domain']:<12} {exp['target_domain']:<12} {'FAILED':<12} {'FAILED':<12} {'FAILED':<12} {'FAILED':<8} {'FAILED':<8} {exp['training_time_minutes']:<8.1f} {exp['status']:<8}"
                print(row)
        
        print("="*120)
        
        # í†µê³„ ìš”ì•½
        if successful_experiments:
            final_accuracies = [exp['final_target_accuracy'] for exp in successful_experiments]
            overall_accuracies = [exp['overall_target_accuracy'] for exp in successful_experiments]
            source_accuracies = [exp['source_accuracy'] for exp in successful_experiments]
            training_times = [exp['training_time_minutes'] for exp in successful_experiments]
            
            print(f"\nğŸ“ˆ ì„±ëŠ¥ í†µê³„ ìš”ì•½:")
            print(f"   ğŸ¯ ìµœì¢… íƒ€ê²Ÿ ì •í™•ë„: í‰ê·  {np.mean(final_accuracies):.2f}%, ìµœê³  {np.max(final_accuracies):.2f}%, ìµœì € {np.min(final_accuracies):.2f}%, í‘œì¤€í¸ì°¨ {np.std(final_accuracies):.2f}")
            print(f"   ğŸ“Š ì „ì²´ íƒ€ê²Ÿ ì •í™•ë„: í‰ê·  {np.mean(overall_accuracies):.2f}%, ìµœê³  {np.max(overall_accuracies):.2f}%, ìµœì € {np.min(overall_accuracies):.2f}%, í‘œì¤€í¸ì°¨ {np.std(overall_accuracies):.2f}")
            print(f"   ğŸ”„ ì†ŒìŠ¤ ë„ë©”ì¸ ì •í™•ë„: í‰ê·  {np.mean(source_accuracies):.2f}%, ìµœê³  {np.max(source_accuracies):.2f}%, ìµœì € {np.min(source_accuracies):.2f}%, í‘œì¤€í¸ì°¨ {np.std(source_accuracies):.2f}")
            print(f"   â±ï¸ í›ˆë ¨ ì‹œê°„: í‰ê·  {np.mean(training_times):.1f}ë¶„, ìµœëŒ€ {np.max(training_times):.1f}ë¶„, ìµœì†Œ {np.min(training_times):.1f}ë¶„")
            print(f"   âœ… ì„±ê³µë¥ : {len(successful_experiments)}/{len(performance_data)} ({len(successful_experiments)/len(performance_data)*100:.1f}%)")
    
    def create_domain_analysis(self, performance_data):
        """ë„ë©”ì¸ë³„ ì„±ëŠ¥ ë¶„ì„"""
        
        print(f"\nğŸ·ï¸ ë„ë©”ì¸ë³„ ì„±ëŠ¥ ë¶„ì„")
        print("="*80)
        
        successful_experiments = [exp for exp in performance_data if exp['status'] == 'SUCCESS']
        
        if not successful_experiments:
            print("âŒ ì„±ê³µí•œ ì‹¤í—˜ì´ ì—†ì–´ ë„ë©”ì¸ ë¶„ì„ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # ì†ŒìŠ¤ ë„ë©”ì¸ë³„ ë¶„ì„
        print("\nğŸ“¤ ì†ŒìŠ¤ ë„ë©”ì¸ë³„ ì„±ëŠ¥ (íƒ€ê²Ÿìœ¼ë¡œ ì „ì´í•  ë•Œ):")
        source_performance = {}
        for domain in self.domains:
            domain_name = self.domain_names[domain]
            domain_experiments = [exp for exp in successful_experiments if exp['source_domain'] == domain_name]
            
            if domain_experiments:
                final_accs = [exp['final_target_accuracy'] for exp in domain_experiments]
                overall_accs = [exp['overall_target_accuracy'] for exp in domain_experiments]
                
                source_performance[domain_name] = {
                    'count': len(domain_experiments),
                    'final_acc_mean': np.mean(final_accs),
                    'final_acc_std': np.std(final_accs),
                    'overall_acc_mean': np.mean(overall_accs),
                    'overall_acc_std': np.std(overall_accs)
                }
                
                print(f"   {domain_name:>12}: {len(domain_experiments)}ê°œ ì‹¤í—˜, ìµœì¢…ì •í™•ë„ {np.mean(final_accs):.2f}Â±{np.std(final_accs):.2f}%, ì „ì²´ì •í™•ë„ {np.mean(overall_accs):.2f}Â±{np.std(overall_accs):.2f}%")
        
        # íƒ€ê²Ÿ ë„ë©”ì¸ë³„ ë¶„ì„
        print("\nğŸ“¥ íƒ€ê²Ÿ ë„ë©”ì¸ë³„ ì„±ëŠ¥ (ì†ŒìŠ¤ì—ì„œ ì ì‘ë°›ì„ ë•Œ):")
        target_performance = {}
        for domain in self.domains:
            domain_name = self.domain_names[domain]
            domain_experiments = [exp for exp in successful_experiments if exp['target_domain'] == domain_name]
            
            if domain_experiments:
                final_accs = [exp['final_target_accuracy'] for exp in domain_experiments]
                overall_accs = [exp['overall_target_accuracy'] for exp in domain_experiments]
                
                target_performance[domain_name] = {
                    'count': len(domain_experiments),
                    'final_acc_mean': np.mean(final_accs),
                    'final_acc_std': np.std(final_accs),
                    'overall_acc_mean': np.mean(overall_accs),
                    'overall_acc_std': np.std(overall_accs)
                }
                
                print(f"   {domain_name:>12}: {len(domain_experiments)}ê°œ ì‹¤í—˜, ìµœì¢…ì •í™•ë„ {np.mean(final_accs):.2f}Â±{np.std(final_accs):.2f}%, ì „ì²´ì •í™•ë„ {np.mean(overall_accs):.2f}Â±{np.std(overall_accs):.2f}%")
        
        return source_performance, target_performance
    
    def find_best_and_worst_experiments(self, performance_data):
        """ìµœê³ /ìµœì•… ì„±ëŠ¥ ì‹¤í—˜ ì°¾ê¸°"""
        
        successful_experiments = [exp for exp in performance_data if exp['status'] == 'SUCCESS']
        
        if not successful_experiments:
            print("âŒ ì„±ê³µí•œ ì‹¤í—˜ì´ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        print(f"\nğŸ† ìµœê³ /ìµœì•… ì„±ëŠ¥ ì‹¤í—˜")
        print("="*80)
        
        # ìµœì¢… íƒ€ê²Ÿ ì •í™•ë„ ê¸°ì¤€
        best_final = max(successful_experiments, key=lambda x: x['final_target_accuracy'])
        worst_final = min(successful_experiments, key=lambda x: x['final_target_accuracy'])
        
        print(f"ğŸ¥‡ ìµœê³  ìµœì¢… íƒ€ê²Ÿ ì •í™•ë„: {best_final['experiment']} ({best_final['final_target_accuracy']:.2f}%)")
        print(f"   ğŸ“Š ì „ì²´ íƒ€ê²Ÿ ì •í™•ë„: {best_final['overall_target_accuracy']:.2f}%")
        print(f"   ğŸ”„ ì†ŒìŠ¤ ì •í™•ë„: {best_final['source_accuracy']:.2f}%")
        print(f"   â±ï¸ í›ˆë ¨ ì‹œê°„: {best_final['training_time_minutes']:.1f}ë¶„")
        
        print(f"\nğŸ¥‰ ìµœì € ìµœì¢… íƒ€ê²Ÿ ì •í™•ë„: {worst_final['experiment']} ({worst_final['final_target_accuracy']:.2f}%)")
        print(f"   ğŸ“Š ì „ì²´ íƒ€ê²Ÿ ì •í™•ë„: {worst_final['overall_target_accuracy']:.2f}%")
        print(f"   ğŸ”„ ì†ŒìŠ¤ ì •í™•ë„: {worst_final['source_accuracy']:.2f}%")
        print(f"   â±ï¸ í›ˆë ¨ ì‹œê°„: {worst_final['training_time_minutes']:.1f}ë¶„")
        
        # ì „ì²´ íƒ€ê²Ÿ ì •í™•ë„ ê¸°ì¤€
        best_overall = max(successful_experiments, key=lambda x: x['overall_target_accuracy'])
        worst_overall = min(successful_experiments, key=lambda x: x['overall_target_accuracy'])
        
        print(f"\nğŸ¥‡ ìµœê³  ì „ì²´ íƒ€ê²Ÿ ì •í™•ë„: {best_overall['experiment']} ({best_overall['overall_target_accuracy']:.2f}%)")
        print(f"ğŸ¥‰ ìµœì € ì „ì²´ íƒ€ê²Ÿ ì •í™•ë„: {worst_overall['experiment']} ({worst_overall['overall_target_accuracy']:.2f}%)")
    
    def save_performance_csv(self, performance_data, filename=None):
        """ì„±ëŠ¥ ë°ì´í„°ë¥¼ CSVë¡œ ì €ì¥"""
        
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"officehome_performance_{timestamp}.csv"
        
        csv_path = self.results_dir / filename
        
        # CSV í—¤ë”
        headers = [
            'Experiment', 'Source Domain', 'Target Domain',
            'Final Target Accuracy (%)', 'Overall Target Accuracy (%)', 'Source Accuracy (%)',
            'Unlearned Samples', 'Selected Target Samples', 'Training Time (min)',
            'Status', 'Error'
        ]
        
        try:
            with open(csv_path, 'w', encoding='utf-8-sig') as f:
                # í—¤ë” ì‘ì„±
                f.write(','.join(headers) + '\n')
                
                # ë°ì´í„° ì‘ì„±
                for exp in performance_data:
                    row = [
                        exp['experiment'],
                        exp['source_domain'],
                        exp['target_domain'],
                        f"{exp['final_target_accuracy']:.2f}" if exp['status'] == 'SUCCESS' else 'FAILED',
                        f"{exp['overall_target_accuracy']:.2f}" if exp['status'] == 'SUCCESS' else 'FAILED',
                        f"{exp['source_accuracy']:.2f}" if exp['status'] == 'SUCCESS' else 'FAILED',
                        str(exp['unlearned_samples']) if exp['status'] == 'SUCCESS' else 'FAILED',
                        str(exp['selected_target_samples']) if exp['status'] == 'SUCCESS' else 'FAILED',
                        f"{exp['training_time_minutes']:.1f}",
                        exp['status'],
                        exp['error'] or ''
                    ]
                    f.write(','.join(row) + '\n')
            
            print(f"ğŸ’¾ ì„±ëŠ¥ ë°ì´í„° CSV ì €ì¥: {csv_path}")
            return csv_path
            
        except Exception as e:
            print(f"âŒ CSV ì €ì¥ ì‹¤íŒ¨: {e}")
            return None
    
    def run_full_analysis(self):
        """ì „ì²´ ì„±ëŠ¥ ë¶„ì„ ì‹¤í–‰"""
        
        print(f"\nğŸ  Office-Home ì „ì²´ ì„±ëŠ¥ ë¶„ì„ ì‹œì‘")
        print("="*80)
        
        # 1. ëª¨ë“  ê²°ê³¼ ë¡œë“œ
        all_results = self.load_all_experiment_results()
        
        if not all_results:
            print("âŒ ë¶„ì„í•  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # 2. ì„±ëŠ¥ ì§€í‘œ ì¶”ì¶œ
        performance_data = self.extract_performance_metrics(all_results)
        
        # 3. ì„±ëŠ¥ ìš”ì•½ í…Œì´ë¸”
        self.create_performance_summary_table(performance_data)
        
        # 4. ë„ë©”ì¸ë³„ ë¶„ì„
        self.create_domain_analysis(performance_data)
        
        # 5. ìµœê³ /ìµœì•… ì‹¤í—˜ ì°¾ê¸°
        self.find_best_and_worst_experiments(performance_data)
        
        # 6. CSV ì €ì¥
        csv_path = self.save_performance_csv(performance_data)
        
        print(f"\nğŸ‰ Office-Home ì„±ëŠ¥ ë¶„ì„ ì™„ë£Œ!")
        print(f"ğŸ“ ê²°ê³¼ ë””ë ‰í† ë¦¬: {self.results_dir}")
        if csv_path:
            print(f"ğŸ’¾ CSV íŒŒì¼: {csv_path}")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    import argparse
    
    parser = argparse.ArgumentParser(description='Office-Home ì‹¤í—˜ ì„±ëŠ¥ ì¶”ì¶œ ë° ë¶„ì„')
    parser.add_argument('--results_dir', type=str, default='results/officehome',
                       help='ê²°ê³¼ ë””ë ‰í† ë¦¬ ê²½ë¡œ (ê¸°ë³¸: results/officehome)')
    parser.add_argument('--experiment', type=str, default=None,
                       help='íŠ¹ì • ì‹¤í—˜ë§Œ ë¶„ì„ (ì˜ˆ: Art2Clipart)')
    parser.add_argument('--save_csv', action='store_true',
                       help='ê²°ê³¼ë¥¼ CSVë¡œ ì €ì¥')
    
    args = parser.parse_args()
    
    # ì„±ëŠ¥ ì¶”ì¶œê¸° ìƒì„±
    extractor = OfficeHomePerformanceExtractor(results_dir=args.results_dir)
    
    if args.experiment:
        # íŠ¹ì • ì‹¤í—˜ ë¶„ì„
        print(f"ğŸ” íŠ¹ì • ì‹¤í—˜ ë¶„ì„: {args.experiment}")
        
        if '2' in args.experiment:
            source_name, target_name = args.experiment.split('2')
            
            # ë„ë©”ì¸ ì´ë¦„ì„ í‚¤ë¡œ ë³€í™˜
            source_key = None
            target_key = None
            for key, name in extractor.domain_names.items():
                if name == source_name:
                    source_key = key
                if name == target_name:
                    target_key = key
            
            if source_key and target_key:
                result = extractor.load_single_experiment_result(source_key, target_key)
                if result:
                    performance_data = extractor.extract_performance_metrics({args.experiment: result})
                    extractor.create_performance_summary_table(performance_data)
                    
                    if args.save_csv:
                        extractor.save_performance_csv(performance_data, f"{args.experiment}_performance.csv")
            else:
                print(f"âŒ ìœ íš¨í•˜ì§€ ì•Šì€ ì‹¤í—˜ ì´ë¦„: {args.experiment}")
        else:
            print(f"âŒ ì‹¤í—˜ ì´ë¦„ í˜•ì‹ì´ ì˜ëª»ë˜ì—ˆìŠµë‹ˆë‹¤. ì˜ˆ: Art2Clipart")
    else:
        # ì „ì²´ ë¶„ì„
        extractor.run_full_analysis()

if __name__ == "__main__":
    main() 